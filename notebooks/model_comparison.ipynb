{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from model import ImageMamba, ModelArgs  # For MAMBA\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Reshape and preprocess the CIFAR-10 dataset for PyTorch models\n",
    "X_train = X_train.transpose(0, 3, 1, 2)  # Shape: (batch_size, channels, height, width)\n",
    "X_test = X_test.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Convert data to float and normalize pixel values in the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the train/test data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Your existing train loader code\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Add test loader\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # Note: shuffle=False for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AttackModel, self).__init__()\n",
    "        self.input_size = num_classes * 2  # logits + probabilities\n",
    "        \n",
    "        # Deeper architecture with batch normalization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class MembershipInferenceAttack:\n",
    "    def __init__(self, target_model, device='cpu'):\n",
    "        self.target_model = target_model\n",
    "        self.attack_model = AttackModel().to(device)\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.attack_model.parameters(), lr=0.001)\n",
    "    \n",
    "    def prepare_attack_data(self, train_loader, test_loader):\n",
    "        attack_inputs = []\n",
    "        attack_labels = []\n",
    "        \n",
    "        self.target_model.eval()\n",
    "        with torch.no_grad():\n",
    "            print(\"Processing training data...\")\n",
    "            for data, labels in train_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.target_model(data)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits, probabilities = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                features = torch.cat([logits, probabilities], dim=1)\n",
    "                attack_inputs.append(features.cpu())\n",
    "                attack_labels.extend([1] * logits.size(0))\n",
    "            \n",
    "            print(\"\\nProcessing test data...\")\n",
    "            for data, labels in test_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.target_model(data)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits, probabilities = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                features = torch.cat([logits, probabilities], dim=1)\n",
    "                attack_inputs.append(features.cpu())\n",
    "                attack_labels.extend([0] * logits.size(0))\n",
    "        \n",
    "        attack_inputs = torch.cat(attack_inputs, dim=0)\n",
    "        attack_labels = torch.tensor(attack_labels, dtype=torch.long)\n",
    "        return attack_inputs, attack_labels\n",
    "\n",
    "    def train_attack(self, train_loader, test_loader, epochs=10):\n",
    "        print(\"Preparing attack data...\")\n",
    "        X, y = self.prepare_attack_data(train_loader, test_loader)\n",
    "        \n",
    "        # Create balanced dataset\n",
    "        member_mask = y == 1\n",
    "        non_member_mask = y == 0\n",
    "        \n",
    "        member_samples = X[member_mask]\n",
    "        non_member_samples = X[non_member_mask]\n",
    "        \n",
    "        min_samples = min(len(member_samples), len(non_member_samples))\n",
    "        \n",
    "        if len(member_samples) > min_samples:\n",
    "            indices = torch.randperm(len(member_samples))[:min_samples]\n",
    "            member_samples = member_samples[indices]\n",
    "        if len(non_member_samples) > min_samples:\n",
    "            indices = torch.randperm(len(non_member_samples))[:min_samples]\n",
    "            non_member_samples = non_member_samples[indices]\n",
    "        \n",
    "        X_balanced = torch.cat([member_samples, non_member_samples])\n",
    "        y_balanced = torch.cat([torch.ones(min_samples), torch.zeros(min_samples)]).long()\n",
    "        \n",
    "        # Split into training and validation\n",
    "        indices = torch.randperm(len(X_balanced))\n",
    "        split = int(0.8 * len(indices))\n",
    "        train_indices = indices[:split]\n",
    "        val_indices = indices[split:]\n",
    "        \n",
    "        train_data = torch.utils.data.TensorDataset(\n",
    "            X_balanced[train_indices],\n",
    "            y_balanced[train_indices]\n",
    "        )\n",
    "        val_data = torch.utils.data.TensorDataset(\n",
    "            X_balanced[val_indices],\n",
    "            y_balanced[val_indices]\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=64)\n",
    "        \n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        best_val_acc = 0\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        print(\"\\nTraining attack model with extended epochs...\")\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.attack_model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.attack_model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.attack_model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.attack_model(inputs)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100. * val_correct / val_total\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch+1:3d} | Train Loss: {train_loss/len(train_loader):.4f} | '\n",
    "                  f'Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            if epochs_without_improvement >= 5:\n",
    "                print(f'\\nEarly stopping at epoch {epoch+1} due to no improvement in validation accuracy')\n",
    "                break\n",
    "        \n",
    "        return train_accs, val_accs\n",
    "\n",
    "def evaluate_attack(attack_model, target_model, data_loader, is_member, device, num_samples=1000):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    total = 0\n",
    "    \n",
    "    target_model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            if total >= num_samples:\n",
    "                break\n",
    "            \n",
    "            data = data.to(device)\n",
    "            outputs = target_model(data)\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits, probabilities = outputs\n",
    "            else:\n",
    "                logits = outputs\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            features = torch.cat([logits, probabilities], dim=1)\n",
    "            attack_outputs = attack_model(features)\n",
    "            predictions = (torch.softmax(attack_outputs, dim=1)[:, 1] > 0.5).long()\n",
    "            \n",
    "            # Calculate metrics based on membership status\n",
    "            if is_member:\n",
    "                true_positives += (predictions == 1).sum().item()\n",
    "                false_negatives += (predictions == 0).sum().item()\n",
    "            else:\n",
    "                true_negatives += (predictions == 0).sum().item()\n",
    "                false_positives += (predictions == 1).sum().item()\n",
    "            \n",
    "            total += data.size(0)\n",
    "            if total >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Calculate precision and recall for member identification\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'true_negatives': true_negatives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'accuracy': (true_positives + true_negatives) / total if total > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def run_extended_attack(model, train_loader, test_loader):\n",
    "    device = torch.device('cpu')\n",
    "    attack = MembershipInferenceAttack(model, device)\n",
    "    train_accs, val_accs = attack.train_attack(train_loader, test_loader)\n",
    "    \n",
    "    print(\"\\nEvaluating final attack performance...\")\n",
    "    train_metrics = evaluate_attack(attack.attack_model, model, train_loader, True, device)\n",
    "    test_metrics = evaluate_attack(attack.attack_model, model, test_loader, False, device)\n",
    "    \n",
    "    print(f\"\\nFinal Attack Results:\")\n",
    "    print(f\"Training Set Metrics:\")\n",
    "    print(f\"- Accuracy: {train_metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"- Precision: {train_metrics['precision']*100:.2f}%\")\n",
    "    print(f\"- Recall: {train_metrics['recall']*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTest Set Metrics:\")\n",
    "    print(f\"- Accuracy: {test_metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"- Precision: {test_metrics['precision']*100:.2f}%\")\n",
    "    print(f\"- Recall: {test_metrics['recall']*100:.2f}%\")\n",
    "    \n",
    "    overall_acc = (train_metrics['accuracy'] + test_metrics['accuracy']) / 2\n",
    "    print(f\"\\nOverall attack accuracy: {overall_acc*100:.2f}%\")\n",
    "    \n",
    "    return attack, train_accs, val_accs, train_metrics['accuracy'], test_metrics['accuracy']\n",
    "\n",
    "def attack_checkpoint(epoch_num, model, train_loader, test_loader):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing model checkpoint from epoch {epoch_num}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    model_path = os.path.join('model_checkpoints_extended', f'model_epoch_{epoch_num}.pt')\n",
    "    saved_model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(saved_model['model_state_dict'])\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    attack, train_accs, val_accs, final_train_acc, final_test_acc = run_extended_attack(\n",
    "        model, train_loader, test_loader)\n",
    "    \n",
    "    return {\n",
    "        'epoch': epoch_num,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'final_train_acc': final_train_acc * 100,  # Convert to percentage\n",
    "        'final_val_acc': final_test_acc * 100  # Convert to percentage\n",
    "    }\n",
    "\n",
    "def analyze_multiple_checkpoints(model, train_loader, test_loader, epochs=[100, 600, 1200, 1500]):\n",
    "    results = []\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        result = attack_checkpoint(epoch, model, train_loader, test_loader)\n",
    "        results.append(result)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Final accuracies for each checkpoint\n",
    "    plt.subplot(2, 1, 1)\n",
    "    epochs = [r['epoch'] for r in results]\n",
    "    train_accs = [r['final_train_acc'] for r in results]\n",
    "    val_accs = [r['final_val_acc'] for r in results]\n",
    "    \n",
    "    plt.plot(epochs, train_accs, 'b-o', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accs, 'r-o', label='Validation Accuracy')\n",
    "    plt.axhline(y=50, color='gray', linestyle='--', label='Random Guess (50%)')\n",
    "    plt.xlabel('Training Epoch of Target Model')\n",
    "    plt.ylabel('Attack Success Rate (%)')\n",
    "    plt.title('Membership Inference Attack Success Rate vs Model Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Attack training curves for each checkpoint\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for result in results:\n",
    "        epoch_num = result['epoch']\n",
    "        epochs_x = range(1, len(result['train_accs']) + 1)\n",
    "        plt.plot(epochs_x, result['train_accs'],\n",
    "                label=f'Train (Epoch {epoch_num})',\n",
    "                linestyle='-', alpha=0.7)\n",
    "        plt.plot(epochs_x, result['val_accs'],\n",
    "                label=f'Val (Epoch {epoch_num})',\n",
    "                linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.axhline(y=50, color='gray', linestyle='--', label='Random Guess (50%)')\n",
    "    plt.xlabel('Attack Model Training Epoch')\n",
    "    plt.ylabel('Attack Accuracy (%)')\n",
    "    plt.title('Attack Model Training Progress for Different Checkpoints')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Checkpoint':^15} | {'Train Acc':^15} | {'Val Acc':^15} | {'Vulnerability':^25}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for result in results:\n",
    "        vulnerability = \"Low\"\n",
    "        acc_diff = result['final_train_acc'] - result['final_val_acc']\n",
    "        if acc_diff > 5 or result['final_train_acc'] > 55:\n",
    "            vulnerability = \"Moderate\"\n",
    "        if acc_diff > 10 or result['final_train_acc'] > 60:\n",
    "            vulnerability = \"High\"\n",
    "        \n",
    "        print(f\"Epoch {result['epoch']:^8} | \"\n",
    "              f\"{result['final_train_acc']:^13.2f}% | \"\n",
    "              f\"{result['final_val_acc']:^13.2f}% | \"\n",
    "              f\"{vulnerability:^25}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(model_type):\n",
    "    \"\"\"Load training metrics from JSON file for specified model type\"\"\"\n",
    "    if model_type.lower() == 'mamba':\n",
    "        metrics_path = os.path.join('model_checkpoints_extended', 'training_metrics.json')\n",
    "    else:  # CNN\n",
    "        metrics_path = os.path.join('cnn_checkpoints', 'training_metrics.json')\n",
    "    \n",
    "    if not os.path.exists(metrics_path):\n",
    "        raise FileNotFoundError(f\"Metrics file not found for {model_type} at {metrics_path}\")\n",
    "    \n",
    "    with open(metrics_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_models(mamba_model, cnn_model, train_loader, test_loader, save_dir='comparison_plots'):\n",
    "    \"\"\"Analyze and compare MAMBA and CNN models with extended attack analysis\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training metrics\n",
    "    mamba_metrics = load_metrics('mamba')\n",
    "    cnn_metrics = load_metrics('cnn')\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # 1. Model Performance Plot\n",
    "    epochs = range(1, min(len(mamba_metrics['test_accuracies']), \n",
    "                         len(cnn_metrics['test_accuracies'])) + 1)\n",
    "    \n",
    "    ax1.plot(epochs, mamba_metrics['test_accuracies'], 'b-', label='MAMBA')\n",
    "    ax1.plot(epochs, cnn_metrics['test_accuracies'], 'r-', label='CNN')\n",
    "    ax1.set_title('Model Performance on CIFAR-10')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Test Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Overfitting Analysis Plot\n",
    "    mamba_overfit = [train - test for train, test in \n",
    "                     zip(mamba_metrics['train_accuracies'], mamba_metrics['test_accuracies'])]\n",
    "    cnn_overfit = [train - test for train, test in \n",
    "                   zip(cnn_metrics['train_accuracies'], cnn_metrics['test_accuracies'])]\n",
    "    \n",
    "    ax2.plot(epochs, mamba_overfit, 'b-', label='MAMBA')\n",
    "    ax2.plot(epochs, cnn_overfit, 'r-', label='CNN')\n",
    "    ax2.set_title('Overfitting Analysis (Train-Test Accuracy Gap)')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy Gap (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Inference Attack Susceptibility\n",
    "    # Run extended attacks at different checkpoints\n",
    "    mamba_attack_results = []\n",
    "    cnn_attack_results = []\n",
    "    checkpoint_epochs = [100, 300, 600, 900]  # Adjust based on available checkpoints\n",
    "    \n",
    "    print(\"\\nAnalyzing MAMBA checkpoints...\")\n",
    "    mamba_results = analyze_multiple_checkpoints(mamba_model, train_loader, test_loader, \n",
    "                                               epochs=checkpoint_epochs)\n",
    "    \n",
    "    print(\"\\nAnalyzing CNN checkpoints...\")\n",
    "    cnn_results = analyze_multiple_checkpoints(cnn_model, train_loader, test_loader, \n",
    "                                             epochs=checkpoint_epochs)\n",
    "    \n",
    "    # Plot attack success rates\n",
    "    mamba_epochs = [r['epoch'] for r in mamba_results]\n",
    "    mamba_attack_acc = [(r['final_train_acc'] + r['final_val_acc'])/2 for r in mamba_results]\n",
    "    \n",
    "    cnn_epochs = [r['epoch'] for r in cnn_results]\n",
    "    cnn_attack_acc = [(r['final_train_acc'] + r['final_val_acc'])/2 for r in cnn_results]\n",
    "    \n",
    "    ax3.plot(mamba_epochs, mamba_attack_acc, 'b-o', label='MAMBA')\n",
    "    ax3.plot(cnn_epochs, cnn_attack_acc, 'r-o', label='CNN')\n",
    "    ax3.axhline(y=50, color='gray', linestyle='--', label='Random Guess')\n",
    "    ax3.set_title('Membership Inference Attack Success Rate')\n",
    "    ax3.set_xlabel('Training Epochs')\n",
    "    ax3.set_ylabel('Attack Success Rate (%)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'model_analysis.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nModel Analysis Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Final Test Accuracy:\")\n",
    "    print(f\"MAMBA: {mamba_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Overfitting Gap:\")\n",
    "    print(f\"MAMBA: {mamba_overfit[-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_overfit[-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Attack Success Rates:\")\n",
    "    print(f\"MAMBA: {mamba_attack_acc[-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_attack_acc[-1]:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'mamba_metrics': {\n",
    "            'final_accuracy': mamba_metrics['test_accuracies'][-1],\n",
    "            'overfitting_gap': mamba_overfit[-1],\n",
    "            'attack_success': mamba_attack_acc[-1],\n",
    "            'attack_results': mamba_results\n",
    "        },\n",
    "        'cnn_metrics': {\n",
    "            'final_accuracy': cnn_metrics['test_accuracies'][-1],\n",
    "            'overfitting_gap': cnn_overfit[-1],\n",
    "            'attack_success': cnn_attack_acc[-1],\n",
    "            'attack_results': cnn_results\n",
    "        }\n",
    "    }\n",
    "\n",
    "def plot_model_comparisons(save_dir='comparison_plots'):\n",
    "    \"\"\"Plot comparison metrics between MAMBA and CNN models from saved metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load metrics for both models\n",
    "    mamba_metrics = load_metrics('mamba')\n",
    "    cnn_metrics = load_metrics('cnn')\n",
    "    \n",
    "    # Create epochs array\n",
    "    epochs = range(1, min(len(mamba_metrics['train_losses']), \n",
    "                         len(cnn_metrics['train_losses'])) + 1)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot Loss Comparison\n",
    "    ax1.plot(epochs, mamba_metrics['train_losses'], 'b-', label='MAMBA Train')\n",
    "    ax1.plot(epochs, mamba_metrics['test_losses'], 'b--', label='MAMBA Test')\n",
    "    ax1.plot(epochs, cnn_metrics['train_losses'], 'r-', label='CNN Train')\n",
    "    ax1.plot(epochs, cnn_metrics['test_losses'], 'r--', label='CNN Test')\n",
    "    ax1.set_title('Loss Comparison')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot Accuracy Comparison\n",
    "    ax2.plot(epochs, mamba_metrics['train_accuracies'], 'b-', label='MAMBA Train')\n",
    "    ax2.plot(epochs, mamba_metrics['test_accuracies'], 'b--', label='MAMBA Test')\n",
    "    ax2.plot(epochs, cnn_metrics['train_accuracies'], 'r-', label='CNN Train')\n",
    "    ax2.plot(epochs, cnn_metrics['test_accuracies'], 'r--', label='CNN Test')\n",
    "    ax2.set_title('Accuracy Comparison')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Plot Average Confidence Comparison\n",
    "    ax3.plot(epochs, mamba_metrics['train_confidences'], 'b-', label='MAMBA Train')\n",
    "    ax3.plot(epochs, mamba_metrics['test_confidences'], 'b--', label='MAMBA Test')\n",
    "    ax3.plot(epochs, cnn_metrics['train_confidences'], 'r-', label='CNN Train')\n",
    "    ax3.plot(epochs, cnn_metrics['test_confidences'], 'r--', label='CNN Test')\n",
    "    ax3.set_title('Average Confidence Comparison')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Confidence')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Plot Final Confidence Distributions\n",
    "    mamba_train_conf = mamba_metrics['epoch_train_confidences'][-1]\n",
    "    mamba_test_conf = mamba_metrics['epoch_test_confidences'][-1]\n",
    "    cnn_train_conf = cnn_metrics['epoch_train_confidences'][-1]\n",
    "    cnn_test_conf = cnn_metrics['epoch_test_confidences'][-1]\n",
    "    \n",
    "    sns.kdeplot(data=mamba_train_conf, ax=ax4, label='MAMBA Train', color='blue')\n",
    "    sns.kdeplot(data=mamba_test_conf, ax=ax4, label='MAMBA Test', color='blue', linestyle='--')\n",
    "    sns.kdeplot(data=cnn_train_conf, ax=ax4, label='CNN Train', color='red')\n",
    "    sns.kdeplot(data=cnn_test_conf, ax=ax4, label='CNN Test', color='red', linestyle='--')\n",
    "    ax4.set_title('Final Epoch Confidence Distributions')\n",
    "    ax4.set_xlabel('Confidence')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'model_comparisons.png'))\n",
    "    plt.show()  # Display the plot\n",
    "    plt.close()\n",
    "\n",
    "def plot_inference_attack_susceptibility(save_dir='comparison_plots'):\n",
    "    \"\"\"Plot inference attack susceptibility comparison between models\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load metrics\n",
    "    mamba_metrics = load_metrics('mamba')\n",
    "    cnn_metrics = load_metrics('cnn')\n",
    "    \n",
    "    # Calculate confidence gaps for both models\n",
    "    mamba_gaps = []\n",
    "    cnn_gaps = []\n",
    "    epochs = []\n",
    "    \n",
    "    min_epochs = min(len(mamba_metrics['train_confidences']), \n",
    "                    len(cnn_metrics['train_confidences']))\n",
    "    \n",
    "    for epoch in range(min_epochs):\n",
    "        # MAMBA confidence gap\n",
    "        mamba_train = np.mean(mamba_metrics['epoch_train_confidences'][epoch])\n",
    "        mamba_test = np.mean(mamba_metrics['epoch_test_confidences'][epoch])\n",
    "        mamba_gaps.append(mamba_train - mamba_test)\n",
    "        \n",
    "        # CNN confidence gap\n",
    "        cnn_train = np.mean(cnn_metrics['epoch_train_confidences'][epoch])\n",
    "        cnn_test = np.mean(cnn_metrics['epoch_test_confidences'][epoch])\n",
    "        cnn_gaps.append(cnn_train - cnn_test)\n",
    "        \n",
    "        epochs.append(epoch + 1)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot confidence gaps over time\n",
    "    ax1.plot(epochs, mamba_gaps, 'b-', label='MAMBA')\n",
    "    ax1.plot(epochs, cnn_gaps, 'r-', label='CNN')\n",
    "    ax1.set_title('Confidence Gap Evolution')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Confidence Gap (Train - Test)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot statistical analysis\n",
    "    # Calculate final statistics\n",
    "    final_stats = {\n",
    "        'mamba': {\n",
    "            'mean_gap': np.mean(mamba_gaps[-10:]),  # Average of last 10 epochs\n",
    "            'std_gap': np.std(mamba_gaps[-10:])\n",
    "        },\n",
    "        'cnn': {\n",
    "            'mean_gap': np.mean(cnn_gaps[-10:]),\n",
    "            'std_gap': np.std(cnn_gaps[-10:])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Plot final gap distributions\n",
    "    sns.kdeplot(data=mamba_gaps[-100:], ax=ax2, label=f\"MAMBA (μ={final_stats['mamba']['mean_gap']:.4f})\", color='blue')\n",
    "    sns.kdeplot(data=cnn_gaps[-100:], ax=ax2, label=f\"CNN (μ={final_stats['cnn']['mean_gap']:.4f})\", color='red')\n",
    "    ax2.set_title('Confidence Gap Distribution (Last 100 Epochs)')\n",
    "    ax2.set_xlabel('Confidence Gap')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'inference_attack_susceptibility.png'))\n",
    "    plt.show()  # Display the plot\n",
    "    plt.close()\n",
    "    \n",
    "    # Return statistics for analysis\n",
    "    return final_stats\n",
    "\n",
    "def generate_comparison_report(save_dir='comparison_plots'):\n",
    "    \"\"\"Generate comprehensive comparison report with plots and statistics\"\"\"\n",
    "    # Create plots\n",
    "    plot_model_comparisons(save_dir)\n",
    "    final_stats = plot_inference_attack_susceptibility(save_dir)\n",
    "    \n",
    "    # Create report\n",
    "    report = {\n",
    "        'inference_attack_susceptibility': final_stats,\n",
    "        'plots_generated': [\n",
    "            'model_comparisons.png',\n",
    "            'inference_attack_susceptibility.png'\n",
    "        ],\n",
    "        'timestamp': str(datetime.datetime.now())\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    with open(os.path.join(save_dir, 'comparison_report.json'), 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model class\n",
    "class ComparableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComparableCNN, self).__init__()\n",
    "        # First conv block - similar to MAMBA's first conv\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Second conv block - similar to MAMBA's second conv\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Additional conv layers to match MAMBA's 4 layers\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Global average pooling and final dense layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Fourth block\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return logits, probabilities\n",
    "\n",
    "# Initialize models\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10\n",
    "\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)\n",
    "mamba_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "cnn_model = ComparableCNN()\n",
    "\n",
    "# Load last checkpoints\n",
    "mamba_checkpoint = os.path.join('model_checkpoints_extended', 'mamba_model_epoch_400.pt')\n",
    "cnn_checkpoint = os.path.join('cnn_checkpoints', 'cnn_model_epoch_400.pt')\n",
    "\n",
    "print(f\"Loading MAMBA from {mamba_checkpoint}\")\n",
    "mamba_state = torch.load(mamba_checkpoint, map_location='cpu')\n",
    "mamba_model.load_state_dict(mamba_state['model_state_dict'])\n",
    "\n",
    "print(f\"Loading CNN from {cnn_checkpoint}\")\n",
    "cnn_state = torch.load(cnn_checkpoint, map_location='cpu')\n",
    "cnn_model.load_state_dict(cnn_state['model_state_dict'])\n",
    "\n",
    "# Set models to evaluation mode\n",
    "mamba_model.eval()\n",
    "cnn_model.eval()\n",
    "\n",
    "# Prepare data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Now ready to run the analysis\n",
    "print(\"Models loaded successfully. Ready for analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "results = analyze_models(mamba_model, cnn_model, train_loader, test_loader)\n",
    "\n",
    "# Generate the full report with all plots\n",
    "report = generate_comparison_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_metrics(model_type):\n",
    "    \"\"\"Load training metrics from JSON file\"\"\"\n",
    "    if model_type.lower() == 'mamba':\n",
    "        path = 'model_checkpoints_extended/training_metrics.json'\n",
    "    else:  # CNN\n",
    "        path = 'cnn_checkpoints/training_metrics.json'\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_comparison_plots():\n",
    "    # Load metrics for both models\n",
    "    mamba_metrics = load_metrics('mamba')\n",
    "    cnn_metrics = load_metrics('cnn')\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Get epochs range\n",
    "    epochs = range(1, min(len(mamba_metrics['test_accuracies']), \n",
    "                         len(cnn_metrics['test_accuracies'])) + 1)\n",
    "    \n",
    "    # 1. Model Performance Plot (Test Accuracy)\n",
    "    ax1.plot(epochs, mamba_metrics['test_accuracies'], 'b-', label='MAMBA')\n",
    "    ax1.plot(epochs, cnn_metrics['test_accuracies'], 'r-', label='CNN')\n",
    "    ax1.set_title('Model Performance Comparison')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Test Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Overfitting Plot (Train-Test Accuracy Gap)\n",
    "    mamba_overfit = [train - test for train, test in \n",
    "                     zip(mamba_metrics['train_accuracies'], mamba_metrics['test_accuracies'])]\n",
    "    cnn_overfit = [train - test for train, test in \n",
    "                   zip(cnn_metrics['train_accuracies'], cnn_metrics['test_accuracies'])]\n",
    "    \n",
    "    ax2.plot(epochs, mamba_overfit, 'b-', label='MAMBA')\n",
    "    ax2.plot(epochs, cnn_overfit, 'r-', label='CNN')\n",
    "    ax2.set_title('Overfitting Analysis (Train-Test Accuracy Gap)')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy Gap (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Inference Attack Susceptibility (based on confidence gap)\n",
    "    mamba_susceptibility = [train - test for train, test in \n",
    "                           zip(mamba_metrics['train_confidences'], mamba_metrics['test_confidences'])]\n",
    "    cnn_susceptibility = [train - test for train, test in \n",
    "                         zip(cnn_metrics['train_confidences'], cnn_metrics['test_confidences'])]\n",
    "    \n",
    "    ax3.plot(epochs, mamba_susceptibility, 'b-', label='MAMBA')\n",
    "    ax3.plot(epochs, cnn_susceptibility, 'r-', label='CNN')\n",
    "    ax3.set_title('Inference Attack Susceptibility (Confidence Gap)')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Train-Test Confidence Gap')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison_analysis.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Final Test Accuracy:\")\n",
    "    print(f\"MAMBA: {mamba_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Overfitting Gap:\")\n",
    "    print(f\"MAMBA: {mamba_overfit[-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_overfit[-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Inference Attack Susceptibility (Confidence Gap):\")\n",
    "    print(f\"MAMBA: {mamba_susceptibility[-1]:.4f}\")\n",
    "    print(f\"CNN: {cnn_susceptibility[-1]:.4f}\")\n",
    "\n",
    "# Create the plots\n",
    "create_comparison_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
