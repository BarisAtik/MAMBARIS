{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "from torch.utils.data import DataLoader\n",
    "from model import ImageMamba, ModelArgs  # For MAMBA\n",
    "from data_loader import load_cifar10, get_class_names # For consistent data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, X_train, X_test, Y_train, Y_test = load_cifar10(batch_size=64, seed=42)\n",
    "class_names = get_class_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AttackModel, self).__init__()\n",
    "        self.input_size = num_classes * 2  # logits + probabilities\n",
    "        \n",
    "        # Deeper architecture with batch normalization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class MembershipInferenceAttack:\n",
    "    def __init__(self, target_model, device='cpu'):\n",
    "        self.target_model = target_model\n",
    "        self.attack_model = AttackModel().to(device)\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.attack_model.parameters(), lr=0.001)\n",
    "    \n",
    "    def prepare_attack_data(self, train_loader, test_loader):\n",
    "        attack_inputs = []\n",
    "        attack_labels = []\n",
    "        \n",
    "        self.target_model.eval()\n",
    "        with torch.no_grad():\n",
    "            print(\"Processing training data...\")\n",
    "            for data, labels in train_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.target_model(data)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits, probabilities = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                features = torch.cat([logits, probabilities], dim=1)\n",
    "                attack_inputs.append(features.cpu())\n",
    "                attack_labels.extend([1] * logits.size(0))\n",
    "            \n",
    "            print(\"\\nProcessing test data...\")\n",
    "            for data, labels in test_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.target_model(data)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits, probabilities = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                features = torch.cat([logits, probabilities], dim=1)\n",
    "                attack_inputs.append(features.cpu())\n",
    "                attack_labels.extend([0] * logits.size(0))\n",
    "        \n",
    "        attack_inputs = torch.cat(attack_inputs, dim=0)\n",
    "        attack_labels = torch.tensor(attack_labels, dtype=torch.long)\n",
    "        return attack_inputs, attack_labels\n",
    "\n",
    "    def train_attack(self, train_loader, test_loader, epochs=10):\n",
    "        print(\"Preparing attack data...\")\n",
    "        X, y = self.prepare_attack_data(train_loader, test_loader)\n",
    "        \n",
    "        # Create balanced dataset\n",
    "        member_mask = y == 1\n",
    "        non_member_mask = y == 0\n",
    "        \n",
    "        member_samples = X[member_mask]\n",
    "        non_member_samples = X[non_member_mask]\n",
    "        \n",
    "        min_samples = min(len(member_samples), len(non_member_samples))\n",
    "        \n",
    "        if len(member_samples) > min_samples:\n",
    "            indices = torch.randperm(len(member_samples))[:min_samples]\n",
    "            member_samples = member_samples[indices]\n",
    "        if len(non_member_samples) > min_samples:\n",
    "            indices = torch.randperm(len(non_member_samples))[:min_samples]\n",
    "            non_member_samples = non_member_samples[indices]\n",
    "        \n",
    "        X_balanced = torch.cat([member_samples, non_member_samples])\n",
    "        y_balanced = torch.cat([torch.ones(min_samples), torch.zeros(min_samples)]).long()\n",
    "        \n",
    "        # Split into training and validation\n",
    "        indices = torch.randperm(len(X_balanced))\n",
    "        split = int(0.8 * len(indices))\n",
    "        train_indices = indices[:split]\n",
    "        val_indices = indices[split:]\n",
    "        \n",
    "        train_data = torch.utils.data.TensorDataset(\n",
    "            X_balanced[train_indices],\n",
    "            y_balanced[train_indices]\n",
    "        )\n",
    "        val_data = torch.utils.data.TensorDataset(\n",
    "            X_balanced[val_indices],\n",
    "            y_balanced[val_indices]\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=64)\n",
    "        \n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        best_val_acc = 0\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        print(\"\\nTraining attack model with extended epochs...\")\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.attack_model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.attack_model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.attack_model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.attack_model(inputs)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100. * val_correct / val_total\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch+1:3d} | Train Loss: {train_loss/len(train_loader):.4f} | '\n",
    "                  f'Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            if epochs_without_improvement >= 5:\n",
    "                print(f'\\nEarly stopping at epoch {epoch+1} due to no improvement in validation accuracy')\n",
    "                break\n",
    "        \n",
    "        return train_accs, val_accs\n",
    "\n",
    "def evaluate_attack(attack_model, target_model, data_loader, is_member, device, num_samples=1000):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    total = 0\n",
    "    \n",
    "    target_model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            if total >= num_samples:\n",
    "                break\n",
    "            \n",
    "            data = data.to(device)\n",
    "            outputs = target_model(data)\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits, probabilities = outputs\n",
    "            else:\n",
    "                logits = outputs\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            features = torch.cat([logits, probabilities], dim=1)\n",
    "            attack_outputs = attack_model(features)\n",
    "            predictions = (torch.softmax(attack_outputs, dim=1)[:, 1] > 0.5).long()\n",
    "            \n",
    "            # Calculate metrics based on membership status\n",
    "            if is_member:\n",
    "                true_positives += (predictions == 1).sum().item()\n",
    "                false_negatives += (predictions == 0).sum().item()\n",
    "            else:\n",
    "                true_negatives += (predictions == 0).sum().item()\n",
    "                false_positives += (predictions == 1).sum().item()\n",
    "            \n",
    "            total += data.size(0)\n",
    "            if total >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Calculate precision and recall for member identification\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'true_negatives': true_negatives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'accuracy': (true_positives + true_negatives) / total if total > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def run_extended_attack(model, train_loader, test_loader):\n",
    "    device = torch.device('cpu')\n",
    "    attack = MembershipInferenceAttack(model, device)\n",
    "    train_accs, val_accs = attack.train_attack(train_loader, test_loader)\n",
    "    \n",
    "    print(\"\\nEvaluating final attack performance...\")\n",
    "    train_metrics = evaluate_attack(attack.attack_model, model, train_loader, True, device)\n",
    "    test_metrics = evaluate_attack(attack.attack_model, model, test_loader, False, device)\n",
    "    \n",
    "    print(f\"\\nFinal Attack Results:\")\n",
    "    print(f\"Training Set Metrics:\")\n",
    "    print(f\"- Accuracy: {train_metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"- Precision: {train_metrics['precision']*100:.2f}%\")\n",
    "    print(f\"- Recall: {train_metrics['recall']*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTest Set Metrics:\")\n",
    "    print(f\"- Accuracy: {test_metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"- Precision: {test_metrics['precision']*100:.2f}%\")\n",
    "    print(f\"- Recall: {test_metrics['recall']*100:.2f}%\")\n",
    "    \n",
    "    overall_acc = (train_metrics['accuracy'] + test_metrics['accuracy']) / 2\n",
    "    print(f\"\\nOverall attack accuracy: {overall_acc*100:.2f}%\")\n",
    "    \n",
    "    return attack, train_accs, val_accs, train_metrics['accuracy'], test_metrics['accuracy']\n",
    "\n",
    "def attack_checkpoint(epoch_num, model, train_loader, test_loader):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing model checkpoint from epoch {epoch_num}\")\n",
    "    print('='*50)\n",
    "\n",
    "    if isinstance(model, ImageMamba):\n",
    "        model_path = os.path.join('model_checkpoints_ext_old', f'model_epoch_{epoch_num}.pt')\n",
    "    else:\n",
    "        model_path = os.path.join('smaller_cnn_checkpoints', f'cnn_model_epoch_{epoch_num}.pt')\n",
    "    saved_model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(saved_model['model_state_dict'])\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    attack, train_accs, val_accs, final_train_acc, final_test_acc = run_extended_attack(\n",
    "        model, train_loader, test_loader)\n",
    "    \n",
    "    return {\n",
    "        'epoch': epoch_num,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'final_train_acc': final_train_acc * 100,  # Convert to percentage\n",
    "        'final_val_acc': final_test_acc * 100  # Convert to percentage\n",
    "    }\n",
    "\n",
    "def analyze_multiple_checkpoints(model, train_loader, test_loader, epochs=[50, 100, 150, 200, 250, 300, 350, 400]): \n",
    "    results = []\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        result = attack_checkpoint(epoch, model, train_loader, test_loader)\n",
    "        results.append(result)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Final accuracies for each checkpoint\n",
    "    plt.subplot(2, 1, 1)\n",
    "    epochs = [r['epoch'] for r in results]\n",
    "    train_accs = [r['final_train_acc'] for r in results]\n",
    "    val_accs = [r['final_val_acc'] for r in results]\n",
    "    \n",
    "    plt.plot(epochs, train_accs, 'b-o', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accs, 'r-o', label='Validation Accuracy')\n",
    "    plt.axhline(y=50, color='gray', linestyle='--', label='Random Guess (50%)')\n",
    "    plt.xlabel('Training Epoch of Target Model')\n",
    "    plt.ylabel('Attack Success Rate (%)')\n",
    "    plt.title('Membership Inference Attack Success Rate vs Model Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Attack training curves for each checkpoint\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for result in results:\n",
    "        epoch_num = result['epoch']\n",
    "        epochs_x = range(1, len(result['train_accs']) + 1)\n",
    "        plt.plot(epochs_x, result['train_accs'],\n",
    "                label=f'Train (Epoch {epoch_num})',\n",
    "                linestyle='-', alpha=0.7)\n",
    "        plt.plot(epochs_x, result['val_accs'],\n",
    "                label=f'Val (Epoch {epoch_num})',\n",
    "                linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.axhline(y=50, color='gray', linestyle='--', label='Random Guess (50%)')\n",
    "    plt.xlabel('Attack Model Training Epoch')\n",
    "    plt.ylabel('Attack Accuracy (%)')\n",
    "    plt.title('Attack Model Training Progress for Different Checkpoints')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Checkpoint':^15} | {'Train Acc':^15} | {'Val Acc':^15} | {'Vulnerability':^25}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for result in results:\n",
    "        vulnerability = \"Low\"\n",
    "        acc_diff = result['final_train_acc'] - result['final_val_acc']\n",
    "        if acc_diff > 5 or result['final_train_acc'] > 55:\n",
    "            vulnerability = \"Moderate\"\n",
    "        if acc_diff > 10 or result['final_train_acc'] > 60:\n",
    "            vulnerability = \"High\"\n",
    "        \n",
    "        print(f\"Epoch {result['epoch']:^8} | \"\n",
    "              f\"{result['final_train_acc']:^13.2f}% | \"\n",
    "              f\"{result['final_val_acc']:^13.2f}% | \"\n",
    "              f\"{vulnerability:^25}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(model_type):\n",
    "    \"\"\"Load training metrics from JSON file for specified model type\"\"\"\n",
    "    if model_type.lower() == 'mamba':\n",
    "        metrics_path = os.path.join('model_checkpoints_extended', 'training_metrics.json')\n",
    "    else:  # CNN\n",
    "        metrics_path = os.path.join('cnn_checkpoints', 'training_metrics.json')\n",
    "    \n",
    "    if not os.path.exists(metrics_path):\n",
    "        raise FileNotFoundError(f\"Metrics file not found for {model_type} at {metrics_path}\")\n",
    "    \n",
    "    with open(metrics_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def analyze_models(mamba_model, cnn_model, train_loader, test_loader, save_dir='comparison_plots'):\n",
    "    \"\"\"Analyze and compare MAMBA and CNN models with extended attack analysis\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load training metrics\n",
    "    mamba_metrics = load_metrics('mamba')\n",
    "    cnn_metrics = load_metrics('cnn')\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # 1. Model Performance Plot\n",
    "    epochs = range(1, min(len(mamba_metrics['test_accuracies']), \n",
    "                         len(cnn_metrics['test_accuracies'])) + 1)\n",
    "    \n",
    "    ax1.plot(epochs, mamba_metrics['test_accuracies'], 'b-', label='MAMBA')\n",
    "    ax1.plot(epochs, cnn_metrics['test_accuracies'], 'r-', label='CNN')\n",
    "    ax1.set_title('Model Performance on CIFAR-10')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Test Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Overfitting Analysis Plot\n",
    "    mamba_overfit = [train - test for train, test in \n",
    "                     zip(mamba_metrics['train_accuracies'], mamba_metrics['test_accuracies'])]\n",
    "    cnn_overfit = [train - test for train, test in \n",
    "                   zip(cnn_metrics['train_accuracies'], cnn_metrics['test_accuracies'])]\n",
    "    \n",
    "    ax2.plot(epochs, mamba_overfit, 'b-', label='MAMBA')\n",
    "    ax2.plot(epochs, cnn_overfit, 'r-', label='CNN')\n",
    "    ax2.set_title('Overfitting Analysis (Train-Test Accuracy Gap)')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy Gap (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Inference Attack Susceptibility\n",
    "    # Run extended attacks at different checkpoints\n",
    "    mamba_attack_results = []\n",
    "    cnn_attack_results = []\n",
    "     # 100, until 1500 epochs\n",
    "    checkpoint_epochs = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500]\n",
    "    \n",
    "    print(\"\\nAnalyzing MAMBA checkpoints...\")\n",
    "    mamba_results = analyze_multiple_checkpoints(mamba_model, train_loader, test_loader, \n",
    "                                               epochs=checkpoint_epochs)\n",
    "    \n",
    "    print(\"\\nAnalyzing CNN checkpoints...\")\n",
    "    cnn_results = analyze_multiple_checkpoints(cnn_model, train_loader, test_loader, \n",
    "                                             epochs=checkpoint_epochs)\n",
    "    \n",
    "    # Plot attack success rates\n",
    "    mamba_epochs = [r['epoch'] for r in mamba_results]\n",
    "    mamba_attack_acc = [(r['final_train_acc'] + r['final_val_acc'])/2 for r in mamba_results]\n",
    "    \n",
    "    cnn_epochs = [r['epoch'] for r in cnn_results]\n",
    "    cnn_attack_acc = [(r['final_train_acc'] + r['final_val_acc'])/2 for r in cnn_results]\n",
    "    \n",
    "    ax3.plot(mamba_epochs, mamba_attack_acc, 'b-o', label='MAMBA')\n",
    "    ax3.plot(cnn_epochs, cnn_attack_acc, 'r-o', label='CNN')\n",
    "    ax3.axhline(y=50, color='gray', linestyle='--', label='Random Guess')\n",
    "    ax3.set_title('Membership Inference Attack Success Rate')\n",
    "    ax3.set_xlabel('Training Epochs')\n",
    "    ax3.set_ylabel('Attack Success Rate (%)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'model_analysis.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nModel Analysis Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Final Test Accuracy:\")\n",
    "    print(f\"MAMBA: {mamba_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Overfitting Gap:\")\n",
    "    print(f\"MAMBA: {mamba_overfit[-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_overfit[-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Attack Success Rates:\")\n",
    "    print(f\"MAMBA: {mamba_attack_acc[-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_attack_acc[-1]:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'mamba_metrics': {\n",
    "            'final_accuracy': mamba_metrics['test_accuracies'][-1],\n",
    "            'overfitting_gap': mamba_overfit[-1],\n",
    "            'attack_success': mamba_attack_acc[-1],\n",
    "            'attack_results': mamba_results\n",
    "        },\n",
    "        'cnn_metrics': {\n",
    "            'final_accuracy': cnn_metrics['test_accuracies'][-1],\n",
    "            'overfitting_gap': cnn_overfit[-1],\n",
    "            'attack_success': cnn_attack_acc[-1],\n",
    "            'attack_results': cnn_results\n",
    "        }\n",
    "    }\n",
    "\n",
    "def plot_model_comparisons(save_dir='comparison_plots'):\n",
    "    \"\"\"Plot comparison metrics between MAMBA and CNN models from saved metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load metrics for both models\n",
    "    mamba_metrics = load_metrics('mamba')\n",
    "    cnn_metrics = load_metrics('cnn')\n",
    "    \n",
    "    # Create epochs array\n",
    "    epochs = range(1, min(len(mamba_metrics['train_losses']), \n",
    "                         len(cnn_metrics['train_losses'])) + 1)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot Loss Comparison\n",
    "    ax1.plot(epochs, mamba_metrics['train_losses'], 'b-', label='MAMBA Train')\n",
    "    ax1.plot(epochs, mamba_metrics['test_losses'], 'b--', label='MAMBA Test')\n",
    "    ax1.plot(epochs, cnn_metrics['train_losses'], 'r-', label='CNN Train')\n",
    "    ax1.plot(epochs, cnn_metrics['test_losses'], 'r--', label='CNN Test')\n",
    "    ax1.set_title('Loss Comparison')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot Accuracy Comparison\n",
    "    ax2.plot(epochs, mamba_metrics['train_accuracies'], 'b-', label='MAMBA Train')\n",
    "    ax2.plot(epochs, mamba_metrics['test_accuracies'], 'b--', label='MAMBA Test')\n",
    "    ax2.plot(epochs, cnn_metrics['train_accuracies'], 'r-', label='CNN Train')\n",
    "    ax2.plot(epochs, cnn_metrics['test_accuracies'], 'r--', label='CNN Test')\n",
    "    ax2.set_title('Accuracy Comparison')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Plot Average Confidence Comparison\n",
    "    ax3.plot(epochs, mamba_metrics['train_confidences'], 'b-', label='MAMBA Train')\n",
    "    ax3.plot(epochs, mamba_metrics['test_confidences'], 'b--', label='MAMBA Test')\n",
    "    ax3.plot(epochs, cnn_metrics['train_confidences'], 'r-', label='CNN Train')\n",
    "    ax3.plot(epochs, cnn_metrics['test_confidences'], 'r--', label='CNN Test')\n",
    "    ax3.set_title('Average Confidence Comparison')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Confidence')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Plot Final Confidence Distributions\n",
    "    mamba_train_conf = mamba_metrics['epoch_train_confidences'][-1]\n",
    "    mamba_test_conf = mamba_metrics['epoch_test_confidences'][-1]\n",
    "    cnn_train_conf = cnn_metrics['epoch_train_confidences'][-1]\n",
    "    cnn_test_conf = cnn_metrics['epoch_test_confidences'][-1]\n",
    "    \n",
    "    sns.kdeplot(data=mamba_train_conf, ax=ax4, label='MAMBA Train', color='blue')\n",
    "    sns.kdeplot(data=mamba_test_conf, ax=ax4, label='MAMBA Test', color='blue', linestyle='--')\n",
    "    sns.kdeplot(data=cnn_train_conf, ax=ax4, label='CNN Train', color='red')\n",
    "    sns.kdeplot(data=cnn_test_conf, ax=ax4, label='CNN Test', color='red', linestyle='--')\n",
    "    ax4.set_title('Final Epoch Confidence Distributions')\n",
    "    ax4.set_xlabel('Confidence')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'model_comparisons.png'))\n",
    "    plt.show()  # Display the plot\n",
    "    plt.close()\n",
    "\n",
    "def plot_inference_attack_susceptibility(save_dir='comparison_plots'):\n",
    "    \"\"\"Plot inference attack susceptibility comparison between models\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Load metrics\n",
    "    mamba_metrics = load_metrics('mamba')\n",
    "    cnn_metrics = load_metrics('cnn')\n",
    "    \n",
    "    # Calculate confidence gaps for both models\n",
    "    mamba_gaps = []\n",
    "    cnn_gaps = []\n",
    "    epochs = []\n",
    "    \n",
    "    min_epochs = min(len(mamba_metrics['train_confidences']), \n",
    "                    len(cnn_metrics['train_confidences']))\n",
    "    \n",
    "    for epoch in range(min_epochs):\n",
    "        # MAMBA confidence gap\n",
    "        mamba_train = np.mean(mamba_metrics['epoch_train_confidences'][epoch])\n",
    "        mamba_test = np.mean(mamba_metrics['epoch_test_confidences'][epoch])\n",
    "        mamba_gaps.append(mamba_train - mamba_test)\n",
    "        \n",
    "        # CNN confidence gap\n",
    "        cnn_train = np.mean(cnn_metrics['epoch_train_confidences'][epoch])\n",
    "        cnn_test = np.mean(cnn_metrics['epoch_test_confidences'][epoch])\n",
    "        cnn_gaps.append(cnn_train - cnn_test)\n",
    "        \n",
    "        epochs.append(epoch + 1)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot confidence gaps over time\n",
    "    ax1.plot(epochs, mamba_gaps, 'b-', label='MAMBA')\n",
    "    ax1.plot(epochs, cnn_gaps, 'r-', label='CNN')\n",
    "    ax1.set_title('Confidence Gap Evolution')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Confidence Gap (Train - Test)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot statistical analysis\n",
    "    # Calculate final statistics\n",
    "    final_stats = {\n",
    "        'mamba': {\n",
    "            'mean_gap': np.mean(mamba_gaps[-10:]),  # Average of last 10 epochs\n",
    "            'std_gap': np.std(mamba_gaps[-10:])\n",
    "        },\n",
    "        'cnn': {\n",
    "            'mean_gap': np.mean(cnn_gaps[-10:]),\n",
    "            'std_gap': np.std(cnn_gaps[-10:])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Plot final gap distributions\n",
    "    sns.kdeplot(data=mamba_gaps[-100:], ax=ax2, label=f\"MAMBA (μ={final_stats['mamba']['mean_gap']:.4f})\", color='blue')\n",
    "    sns.kdeplot(data=cnn_gaps[-100:], ax=ax2, label=f\"CNN (μ={final_stats['cnn']['mean_gap']:.4f})\", color='red')\n",
    "    ax2.set_title('Confidence Gap Distribution (Last 100 Epochs)')\n",
    "    ax2.set_xlabel('Confidence Gap')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'inference_attack_susceptibility.png'))\n",
    "    plt.show()  # Display the plot\n",
    "    plt.close()\n",
    "    \n",
    "    # Return statistics for analysis\n",
    "    return final_stats\n",
    "\n",
    "def generate_comparison_report(save_dir='comparison_plots'):\n",
    "    \"\"\"Generate comprehensive comparison report with plots and statistics\"\"\"\n",
    "    # Create plots\n",
    "    plot_model_comparisons(save_dir)\n",
    "    final_stats = plot_inference_attack_susceptibility(save_dir)\n",
    "    \n",
    "    # Create report\n",
    "    report = {\n",
    "        'inference_attack_susceptibility': final_stats,\n",
    "        'plots_generated': [\n",
    "            'model_comparisons.png',\n",
    "            'inference_attack_susceptibility.png'\n",
    "        ],\n",
    "        'timestamp': str(datetime.datetime.now())\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    with open(os.path.join(save_dir, 'comparison_report.json'), 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MAMBA from model_checkpoints_ext_old\\model_epoch_1500.pt\n",
      "Loading CNN from smaller_cnn_checkpoints\\cnn_model_epoch_1500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baris\\AppData\\Local\\Temp\\ipykernel_19000\\3426694417.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mamba_state = torch.load(mamba_checkpoint, map_location='cpu')\n",
      "C:\\Users\\baris\\AppData\\Local\\Temp\\ipykernel_19000\\3426694417.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  smaller_cnn_state = torch.load(smaller_cnn_checkpoint, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Models loaded successfully. Ready for analysis...\n"
     ]
    }
   ],
   "source": [
    "class SmallerComparableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallerComparableCNN, self).__init__()\n",
    "        # Reduced initial channels and total layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Reduced from 64 to 32\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Reduced from 128 to 64\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Removed two conv layers to reduce parameters\n",
    "        \n",
    "        # Global average pooling and final dense layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 10)  # Changed input features to match last conv layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return logits, probabilities\n",
    "\n",
    "# Initialize models\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10\n",
    "\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)\n",
    "mamba_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "smaller_cnn_model = SmallerComparableCNN()\n",
    "\n",
    "# Load last checkpoints\n",
    "mamba_checkpoint = os.path.join('model_checkpoints_ext_old', 'model_epoch_1500.pt')\n",
    "smaller_cnn_checkpoint = os.path.join('smaller_cnn_checkpoints', 'cnn_model_epoch_1500.pt')\n",
    "\n",
    "print(f\"Loading MAMBA from {mamba_checkpoint}\")\n",
    "mamba_state = torch.load(mamba_checkpoint, map_location='cpu')\n",
    "mamba_model.load_state_dict(mamba_state['model_state_dict'])\n",
    "\n",
    "print(f\"Loading CNN from {smaller_cnn_checkpoint}\")\n",
    "smaller_cnn_state = torch.load(smaller_cnn_checkpoint, map_location='cpu')\n",
    "smaller_cnn_model.load_state_dict(smaller_cnn_state['model_state_dict'])\n",
    "\n",
    "# Set models to evaluation mode\n",
    "mamba_model.eval()\n",
    "smaller_cnn_model.eval()\n",
    "\n",
    "# Prepare data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "##############WHY IS THIS DATA LOADED IN A DIFFERENT MANNER??????? THEN THE MODEL TRAINING NOTEBOOKS\n",
    "\n",
    "# Now ready to run the analysis\n",
    "print(\"Models loaded successfully. Ready for analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the analysis\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmamba_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmaller_cnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Generate the full report with all plots\u001b[39;00m\n\u001b[0;32m      5\u001b[0m report \u001b[38;5;241m=\u001b[39m generate_comparison_report()\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36manalyze_models\u001b[1;34m(mamba_model, cnn_model, train_loader, test_loader, save_dir)\u001b[0m\n\u001b[0;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load training metrics\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m mamba_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mload_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmamba\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m cnn_metrics \u001b[38;5;241m=\u001b[39m load_metrics(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create figure with three subplots\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mload_metrics\u001b[1;34m(model_type)\u001b[0m\n\u001b[0;32m      6\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmaller_cnn_checkpoints/training_metrics.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "results = analyze_models(mamba_model, smaller_cnn_model, train_loader, test_loader)\n",
    "\n",
    "# Generate the full report with all plots\n",
    "report = generate_comparison_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (9,) and (400,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnn_susceptibility[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Create the plots\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[43mcreate_comparison_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m, in \u001b[0;36mcreate_comparison_plots\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(mamba_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracies\u001b[39m\u001b[38;5;124m'\u001b[39m]), \n\u001b[0;32m     21\u001b[0m                      \u001b[38;5;28mlen\u001b[39m(cnn_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracies\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 1. Model Performance Plot (Test Accuracy)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43max1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmamba_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_accuracies\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAMBA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(epochs, cnn_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracies\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Performance Comparison\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (9,) and (400,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_metrics(model_type):\n",
    "    \"\"\"Load training metrics from JSON file\"\"\"\n",
    "    if model_type.lower() == 'mamba':\n",
    "        path = 'model_checkpoints_extended/training_metrics.json'\n",
    "    else:  # CNN\n",
    "        path = 'smaller_cnn_checkpoints/training_metrics.json'\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_comparison_plots():\n",
    "    # Load metrics for both models\n",
    "    mamba_metrics = load_metrics('mamba')\n",
    "    cnn_metrics = load_metrics('smaller_cnn')\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Get epochs range\n",
    "    epochs = range(1, min(len(mamba_metrics['test_accuracies']), \n",
    "                         len(cnn_metrics['test_accuracies'])) + 1)\n",
    "    \n",
    "    # 1. Model Performance Plot (Test Accuracy)\n",
    "    ax1.plot(epochs, mamba_metrics['test_accuracies'], 'b-', label='MAMBA')\n",
    "    ax1.plot(epochs, cnn_metrics['test_accuracies'], 'r-', label='CNN')\n",
    "    ax1.set_title('Model Performance Comparison')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Test Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Overfitting Plot (Train-Test Accuracy Gap)\n",
    "    mamba_overfit = [train - test for train, test in \n",
    "                     zip(mamba_metrics['train_accuracies'], mamba_metrics['test_accuracies'])]\n",
    "    cnn_overfit = [train - test for train, test in \n",
    "                   zip(cnn_metrics['train_accuracies'], cnn_metrics['test_accuracies'])]\n",
    "    \n",
    "    ax2.plot(epochs, mamba_overfit, 'b-', label='MAMBA')\n",
    "    ax2.plot(epochs, cnn_overfit, 'r-', label='CNN')\n",
    "    ax2.set_title('Overfitting Analysis (Train-Test Accuracy Gap)')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy Gap (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Inference Attack Susceptibility (based on confidence gap)\n",
    "    mamba_susceptibility = [train - test for train, test in \n",
    "                           zip(mamba_metrics['train_confidences'], mamba_metrics['test_confidences'])]\n",
    "    cnn_susceptibility = [train - test for train, test in \n",
    "                         zip(cnn_metrics['train_confidences'], cnn_metrics['test_confidences'])]\n",
    "    \n",
    "    ax3.plot(epochs, mamba_susceptibility, 'b-', label='MAMBA')\n",
    "    ax3.plot(epochs, cnn_susceptibility, 'r-', label='CNN')\n",
    "    ax3.set_title('Inference Attack Susceptibility (Confidence Gap)')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Train-Test Confidence Gap')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison_analysis.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Final Test Accuracy:\")\n",
    "    print(f\"MAMBA: {mamba_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Overfitting Gap:\")\n",
    "    print(f\"MAMBA: {mamba_overfit[-1]:.2f}%\")\n",
    "    print(f\"CNN: {cnn_overfit[-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Inference Attack Susceptibility (Confidence Gap):\")\n",
    "    print(f\"MAMBA: {mamba_susceptibility[-1]:.4f}\")\n",
    "    print(f\"CNN: {cnn_susceptibility[-1]:.4f}\")\n",
    "\n",
    "# Create the plots\n",
    "create_comparison_plots()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
