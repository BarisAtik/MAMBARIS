{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from model import ImageMamba, ModelArgs, SmallerComparableCNN\n",
    "from data_loader import load_cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_loader import load_cifar10, get_class_names\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_checkpoints(checkpoint_dir, model, device='cuda'):\n",
    "    \"\"\"Load all available checkpoints for a model.\"\"\"\n",
    "    checkpoints = []\n",
    "    epochs = []\n",
    "    \n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        if filename.endswith('.pt'):\n",
    "            epoch = int(filename.split('_')[-1].split('.pt')[0])\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            checkpoints.append({\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'metrics': checkpoint['metrics'] if 'metrics' in checkpoint else None\n",
    "            })\n",
    "            epochs.append(epoch)\n",
    "    \n",
    "    return sorted(checkpoints, key=lambda x: x['epoch']), sorted(epochs)\n",
    "\n",
    "def evaluate_model_vulnerability(model, train_loader, test_loader, device='cuda'):\n",
    "    \"\"\"Evaluate model's vulnerability to inference attacks.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    def get_confidence_stats(loader):\n",
    "        confidences = []\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for data, labels in loader:\n",
    "                data = data.to(device)\n",
    "                _, probs = model(data)\n",
    "                max_probs, preds = torch.max(probs, dim=1)\n",
    "                confidences.extend(max_probs.cpu().numpy())\n",
    "                predictions.extend((preds == labels.to(device)).cpu().numpy())\n",
    "        return np.mean(confidences), np.std(confidences), np.mean(predictions)\n",
    "\n",
    "    train_conf_mean, train_conf_std, train_acc = get_confidence_stats(train_loader)\n",
    "    test_conf_mean, test_conf_std, test_acc = get_confidence_stats(test_loader)\n",
    "    \n",
    "    # Calculate vulnerability metrics\n",
    "    confidence_gap = train_conf_mean - test_conf_mean\n",
    "    acc_gap = train_acc - test_acc\n",
    "    \n",
    "    # Calculate confidence distribution overlap\n",
    "    confidence_overlap = min(train_conf_mean + train_conf_std, test_conf_mean + test_conf_std) - \\\n",
    "                        max(train_conf_mean - train_conf_std, test_conf_mean - test_conf_std)\n",
    "    \n",
    "    return {\n",
    "        'train_confidence': train_conf_mean,\n",
    "        'test_confidence': test_conf_mean,\n",
    "        'train_accuracy': train_acc * 100,\n",
    "        'test_accuracy': test_acc * 100,\n",
    "        'confidence_gap': confidence_gap,\n",
    "        'accuracy_gap': acc_gap * 100,\n",
    "        'confidence_overlap': confidence_overlap\n",
    "    }\n",
    "\n",
    "def analyze_models(mamba_model, cnn_model, train_loader, test_loader, mamba_dir, cnn_dir, device='cuda'):\n",
    "    \"\"\"Analyze both models across their checkpoints.\"\"\"\n",
    "    \n",
    "    # Load checkpoints\n",
    "    print(\"Loading MAMBA checkpoints...\")\n",
    "    mamba_checkpoints, mamba_epochs = load_all_checkpoints(mamba_dir, mamba_model, device)\n",
    "    print(\"Loading CNN checkpoints...\")\n",
    "    cnn_checkpoints, cnn_epochs = load_all_checkpoints(cnn_dir, cnn_model, device)\n",
    "    \n",
    "    results = {\n",
    "        'mamba': {'vulnerabilities': [], 'epochs': mamba_epochs},\n",
    "        'cnn': {'vulnerabilities': [], 'epochs': cnn_epochs}\n",
    "    }\n",
    "    \n",
    "    # Analyze MAMBA checkpoints\n",
    "    print(\"\\nAnalyzing MAMBA checkpoints...\")\n",
    "    for checkpoint in tqdm(mamba_checkpoints):\n",
    "        mamba_model.load_state_dict(checkpoint['model_state'])\n",
    "        vuln = evaluate_model_vulnerability(mamba_model, train_loader, test_loader, device)\n",
    "        results['mamba']['vulnerabilities'].append(vuln)\n",
    "    \n",
    "    # Analyze CNN checkpoints\n",
    "    print(\"\\nAnalyzing CNN checkpoints...\")\n",
    "    for checkpoint in tqdm(cnn_checkpoints):\n",
    "        cnn_model.load_state_dict(checkpoint['model_state'])\n",
    "        vuln = evaluate_model_vulnerability(cnn_model, train_loader, test_loader, device)\n",
    "        results['cnn']['vulnerabilities'].append(vuln)\n",
    "    \n",
    "    return results\n",
    "    \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_models_memory_efficient(mamba_model, cnn_model, train_loader, test_loader, \n",
    "                                 mamba_dir, cnn_dir, device='cuda'):\n",
    "    \"\"\"Memory efficient version of model analysis.\"\"\"\n",
    "    \n",
    "    def get_checkpoint_epochs(checkpoint_dir):\n",
    "        \"\"\"Get sorted list of epoch numbers from checkpoint directory.\"\"\"\n",
    "        epochs = []\n",
    "        for filename in os.listdir(checkpoint_dir):\n",
    "            if filename.endswith('.pt'):\n",
    "                epoch = int(filename.split('_')[-1].split('.pt')[0])\n",
    "                epochs.append(epoch)\n",
    "        return sorted(epochs)\n",
    "    \n",
    "    def evaluate_single_checkpoint(model, checkpoint_path, train_loader, test_loader, device):\n",
    "        \"\"\"Evaluate a single checkpoint and free memory.\"\"\"\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Evaluate\n",
    "        vuln_metrics = evaluate_model_vulnerability(model, train_loader, test_loader, device)\n",
    "        \n",
    "        # Clear memory\n",
    "        del checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return vuln_metrics\n",
    "    \n",
    "    # Get epochs for each model\n",
    "    print(\"Finding checkpoints...\")\n",
    "    mamba_epochs = get_checkpoint_epochs(mamba_dir)\n",
    "    cnn_epochs = get_checkpoint_epochs(cnn_dir)\n",
    "    \n",
    "    results = {\n",
    "        'mamba': {'vulnerabilities': [], 'epochs': mamba_epochs},\n",
    "        'cnn': {'vulnerabilities': [], 'epochs': cnn_epochs}\n",
    "    }\n",
    "    \n",
    "    # Analyze MAMBA checkpoints\n",
    "    print(\"\\nAnalyzing MAMBA checkpoints...\")\n",
    "    for epoch in tqdm(mamba_epochs):\n",
    "        checkpoint_path = os.path.join(mamba_dir, f'model_epoch_{epoch}.pt')\n",
    "        vuln = evaluate_single_checkpoint(mamba_model, checkpoint_path, \n",
    "                                        train_loader, test_loader, device)\n",
    "        results['mamba']['vulnerabilities'].append(vuln)\n",
    "        gc.collect()\n",
    "    \n",
    "    # Analyze CNN checkpoints\n",
    "    print(\"\\nAnalyzing CNN checkpoints...\")\n",
    "    for epoch in tqdm(cnn_epochs):\n",
    "        checkpoint_path = os.path.join(cnn_dir, f'cnn_model_epoch_{epoch}.pt')\n",
    "        vuln = evaluate_single_checkpoint(cnn_model, checkpoint_path, \n",
    "                                        train_loader, test_loader, device)\n",
    "        results['cnn']['vulnerabilities'].append(vuln)\n",
    "        gc.collect()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_model_vulnerability(model, train_loader, test_loader, device='cuda'):\n",
    "    \"\"\"Evaluate model's vulnerability to inference attacks with memory efficiency.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "def plot_vulnerability_comparison(results, save_dir='comparison_plots'):\n",
    "    \"\"\"Plot comparison of model vulnerabilities with proper line connectivity.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Sort results by epoch to ensure proper line connectivity\n",
    "    mamba_data = sorted(zip(results['mamba']['epochs'], \n",
    "                           results['mamba']['vulnerabilities']), \n",
    "                       key=lambda x: x[0])\n",
    "    cnn_data = sorted(zip(results['cnn']['epochs'], \n",
    "                         results['cnn']['vulnerabilities']), \n",
    "                     key=lambda x: x[0])\n",
    "    \n",
    "    # Unzip the sorted data\n",
    "    mamba_epochs, mamba_vulns = zip(*mamba_data)\n",
    "    cnn_epochs, cnn_vulns = zip(*cnn_data)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = plt.GridSpec(3, 2)\n",
    "    \n",
    "    # 1. Accuracy Gaps\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(mamba_epochs, \n",
    "             [v['accuracy_gap'] for v in mamba_vulns], \n",
    "             'b-', label='MAMBA', linewidth=2)\n",
    "    ax1.plot(cnn_epochs, \n",
    "             [v['accuracy_gap'] for v in cnn_vulns], \n",
    "             'r-', label='CNN', linewidth=2)\n",
    "    ax1.set_title('Train-Test Accuracy Gap Evolution')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy Gap (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Confidence Gaps\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(mamba_epochs, \n",
    "             [v['confidence_gap'] for v in mamba_vulns], \n",
    "             'b-', label='MAMBA', linewidth=2)\n",
    "    ax2.plot(cnn_epochs, \n",
    "             [v['confidence_gap'] for v in cnn_vulns], \n",
    "             'r-', label='CNN', linewidth=2)\n",
    "    ax2.set_title('Confidence Gap Evolution')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Train-Test Confidence Gap')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Train vs Test Accuracies\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.plot(mamba_epochs, \n",
    "             [v['train_accuracy'] for v in mamba_vulns], \n",
    "             'b-', label='MAMBA Train', linewidth=2)\n",
    "    ax3.plot(mamba_epochs, \n",
    "             [v['test_accuracy'] for v in mamba_vulns], \n",
    "             'b--', label='MAMBA Test', linewidth=2)\n",
    "    ax3.plot(cnn_epochs, \n",
    "             [v['train_accuracy'] for v in cnn_vulns], \n",
    "             'r-', label='CNN Train', linewidth=2)\n",
    "    ax3.plot(cnn_epochs, \n",
    "             [v['test_accuracy'] for v in cnn_vulns], \n",
    "             'r--', label='CNN Test', linewidth=2)\n",
    "    ax3.set_title('Accuracy Comparison')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # 4. Vulnerability Score Evolution\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    mamba_scores = [v['confidence_gap'] * (1 - v['confidence_overlap']) \n",
    "                   for v in mamba_vulns]\n",
    "    cnn_scores = [v['confidence_gap'] * (1 - v['confidence_overlap']) \n",
    "                  for v in cnn_vulns]\n",
    "    \n",
    "    ax4.plot(mamba_epochs, mamba_scores, 'b-', label='MAMBA', linewidth=2)\n",
    "    ax4.plot(cnn_epochs, cnn_scores, 'r-', label='CNN', linewidth=2)\n",
    "    ax4.set_title('Overall Vulnerability Score Evolution')\n",
    "    ax4.set_xlabel('Epochs')\n",
    "    ax4.set_ylabel('Vulnerability Score')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'vulnerability_analysis.png'))\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nVulnerability Analysis Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    final_mamba = mamba_vulns[-1]\n",
    "    final_cnn = cnn_vulns[-1]\n",
    "    \n",
    "    print(f\"\\nFinal Checkpoint Statistics:\")\n",
    "    print(f\"MAMBA (Epoch {mamba_epochs[-1]}):\")\n",
    "    print(f\"  - Accuracy Gap: {final_mamba['accuracy_gap']:.2f}%\")\n",
    "    print(f\"  - Confidence Gap: {final_mamba['confidence_gap']:.4f}\")\n",
    "    print(f\"  - Train Accuracy: {final_mamba['train_accuracy']:.2f}%\")\n",
    "    print(f\"  - Test Accuracy: {final_mamba['test_accuracy']:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nCNN (Epoch {cnn_epochs[-1]}):\")\n",
    "    print(f\"  - Accuracy Gap: {final_cnn['accuracy_gap']:.2f}%\")\n",
    "    print(f\"  - Confidence Gap: {final_cnn['confidence_gap']:.4f}\")\n",
    "    print(f\"  - Train Accuracy: {final_cnn['train_accuracy']:.2f}%\")\n",
    "    print(f\"  - Test Accuracy: {final_cnn['test_accuracy']:.2f}%\")\n",
    "    \n",
    "    # Calculate overall vulnerability scores\n",
    "    mamba_vuln_score = mamba_scores[-1]\n",
    "    cnn_vuln_score = cnn_scores[-1]\n",
    "    \n",
    "    print(f\"\\nOverall Vulnerability Score (higher = more vulnerable):\")\n",
    "    print(f\"MAMBA: {mamba_vuln_score:.4f}\")\n",
    "    print(f\"CNN: {cnn_vuln_score:.4f}\")\n",
    "    \n",
    "    conclusion = \"MAMBA\" if mamba_vuln_score > cnn_vuln_score else \"CNN\"\n",
    "    print(f\"\\nConclusion: {conclusion} appears more vulnerable to inference attacks.\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "d_model = 128  # As used in mamba training\n",
    "n_layer = 8    # As used in mamba training\n",
    "num_classes = 10\n",
    "\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)\n",
    "mamba_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "cnn_model = SmallerComparableCNN()  # Uses fixed architecture\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamba_model = mamba_model.to(device)\n",
    "cnn_model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, _, _, _, _ = load_cifar10(batch_size=64, seed=42)\n",
    "class_names = get_class_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis...\n",
      "Initializing models...\n",
      "\n",
      "Analyzing MAMBA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing MAMBA:   0%|          | 0/12 [00:00<?, ?it/s]C:\\Users\\baris\\AppData\\Local\\Temp\\ipykernel_12032\\2657652263.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "Analyzing MAMBA:  33%|███▎      | 4/12 [19:20<35:30, 266.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing checkpoint model_epoch_1200.pt: PytorchStreamReader failed locating file data.pkl: file not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing MAMBA: 100%|██████████| 12/12 [49:54<00:00, 249.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing CNN: 100%|██████████| 9/9 [23:43<00:00, 158.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing checkpoint model_epoch_900.pt: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "\n",
      "Creating plots...\n",
      "Error during execution: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_single_model(model, checkpoint_dir, train_loader, test_loader, device='cuda', model_name=''):\n",
    "    \"\"\"Analyze a single model's checkpoints one at a time with memory safeguards.\"\"\"\n",
    "    epochs = []\n",
    "    vulnerabilities = []\n",
    "    \n",
    "    # Get sorted list of checkpoints\n",
    "    checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')])\n",
    "    \n",
    "    for checkpoint_file in tqdm(checkpoints, desc=f\"Analyzing {model_name}\"):\n",
    "        try:\n",
    "            # Clear memory before loading new checkpoint\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Get epoch number\n",
    "            epoch = int(checkpoint_file.split('_')[-1].split('.pt')[0])\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "            \n",
    "            # Load and evaluate checkpoint\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            # Free memory\n",
    "            del checkpoint\n",
    "            gc.collect()\n",
    "            \n",
    "            # Evaluate model\n",
    "            model.eval()\n",
    "            vuln = evaluate_model_vulnerability(model, train_loader, test_loader, device)\n",
    "            \n",
    "            epochs.append(epoch)\n",
    "            vulnerabilities.append(vuln)\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error processing checkpoint {checkpoint_file}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        # Clear memory after processing checkpoint\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return epochs, vulnerabilities\n",
    "\n",
    "# Main analysis function\n",
    "def run_analysis():\n",
    "    try:\n",
    "        # Initialize models with correct architecture\n",
    "        d_model = 128\n",
    "        n_layer = 8\n",
    "        num_classes = 10\n",
    "        \n",
    "        print(\"Initializing models...\")\n",
    "        model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)\n",
    "        mamba_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "        cnn_model = SmallerComparableCNN()\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        mamba_model = mamba_model.to(device)\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        \n",
    "        # Analyze models separately\n",
    "        print(\"\\nAnalyzing MAMBA model...\")\n",
    "        mamba_epochs, mamba_vulns = analyze_single_model(\n",
    "            mamba_model, 'mamba_checkpoints', train_loader, test_loader, \n",
    "            device, 'MAMBA'\n",
    "        )\n",
    "        \n",
    "        print(\"\\nAnalyzing CNN model...\")\n",
    "        cnn_epochs, cnn_vulns = analyze_single_model(\n",
    "            cnn_model, 'cnn_checkpoints', train_loader, test_loader, \n",
    "            device, 'CNN'\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        results = {\n",
    "            'mamba': {'vulnerabilities': mamba_vulns, 'epochs': mamba_epochs},\n",
    "            'cnn': {'vulnerabilities': cnn_vulns, 'epochs': cnn_epochs}\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "try:\n",
    "    print(\"Starting analysis...\")\n",
    "    results = run_analysis()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nCreating plots...\")\n",
    "        plot_vulnerability_comparison(results)\n",
    "    else:\n",
    "        print(\"Analysis failed to produce results.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during execution: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoints in mamba_checkpoints:\n",
      "model_epoch_100.pt\n",
      "model_epoch_1000.pt\n",
      "model_epoch_1100.pt\n",
      "model_epoch_1200.pt\n",
      "model_epoch_200.pt\n",
      "model_epoch_300.pt\n",
      "model_epoch_400.pt\n",
      "model_epoch_500.pt\n",
      "model_epoch_600.pt\n",
      "model_epoch_700.pt\n",
      "model_epoch_800.pt\n",
      "model_epoch_900.pt\n",
      "\n",
      "Checkpoints in cnn_checkpoints:\n",
      "model_epoch_100.pt\n",
      "model_epoch_200.pt\n",
      "model_epoch_300.pt\n",
      "model_epoch_400.pt\n",
      "model_epoch_500.pt\n",
      "model_epoch_600.pt\n",
      "model_epoch_700.pt\n",
      "model_epoch_800.pt\n",
      "model_epoch_900.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_checkpoints(directory):\n",
    "    checkpoints = sorted([f for f in os.listdir(directory) if f.endswith('.pt')])\n",
    "    print(f\"\\nCheckpoints in {directory}:\")\n",
    "    for cp in checkpoints:\n",
    "        print(cp)\n",
    "\n",
    "check_checkpoints('mamba_checkpoints')\n",
    "check_checkpoints('cnn_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying checkpoints:\n",
      "\n",
      "Mamba checkpoints (12 files):\n",
      "- model_epoch_100.pt\n",
      "- model_epoch_1000.pt\n",
      "- model_epoch_1100.pt\n",
      "- model_epoch_1200.pt\n",
      "- model_epoch_200.pt\n",
      "- model_epoch_300.pt\n",
      "- model_epoch_400.pt\n",
      "- model_epoch_500.pt\n",
      "- model_epoch_600.pt\n",
      "- model_epoch_700.pt\n",
      "- model_epoch_800.pt\n",
      "- model_epoch_900.pt\n",
      "\n",
      "CNN checkpoints (9 files):\n",
      "- model_epoch_100.pt\n",
      "- model_epoch_200.pt\n",
      "- model_epoch_300.pt\n",
      "- model_epoch_400.pt\n",
      "- model_epoch_500.pt\n",
      "- model_epoch_600.pt\n",
      "- model_epoch_700.pt\n",
      "- model_epoch_800.pt\n",
      "- model_epoch_900.pt\n"
     ]
    }
   ],
   "source": [
    "# Add this debugging code before running the analysis\n",
    "def verify_checkpoints():\n",
    "    print(\"\\nVerifying checkpoints:\")\n",
    "    # Check Mamba checkpoints\n",
    "    mamba_files = sorted([f for f in os.listdir('mamba_checkpoints') if f.endswith('.pt')])\n",
    "    print(f\"\\nMamba checkpoints ({len(mamba_files)} files):\")\n",
    "    for f in mamba_files:\n",
    "        print(f\"- {f}\")\n",
    "    \n",
    "    # Check CNN checkpoints\n",
    "    cnn_files = sorted([f for f in os.listdir('cnn_checkpoints') if f.endswith('.pt')])\n",
    "    print(f\"\\nCNN checkpoints ({len(cnn_files)} files):\")\n",
    "    for f in cnn_files:\n",
    "        print(f\"- {f}\")\n",
    "\n",
    "verify_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verifying Setup ===\n",
      "\n",
      "Checking data loaders...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train loader size: 1563\n",
      "Test loader size: 313\n",
      "\n",
      "Checking a batch from train loader...\n",
      "Batch shape: torch.Size([32, 3, 32, 32])\n",
      "Labels shape: torch.Size([32])\n",
      "\n",
      "Initializing models...\n",
      "Models initialized successfully\n",
      "\n",
      "Testing forward pass...\n",
      "Testing Mamba model...\n",
      "Mamba output shapes: [torch.Size([32, 10]), torch.Size([32, 10])]\n",
      "\n",
      "Testing CNN model...\n",
      "CNN output shapes: [torch.Size([32, 10]), torch.Size([32, 10])]\n",
      "\n",
      "Setup verification completed successfully!\n",
      "\n",
      "Proceeding with analysis...\n",
      "\n",
      "Verifying checkpoints:\n",
      "\n",
      "Mamba checkpoints (12 files):\n",
      "- model_epoch_100.pt\n",
      "- model_epoch_1000.pt\n",
      "- model_epoch_1100.pt\n",
      "- model_epoch_1200.pt\n",
      "- model_epoch_200.pt\n",
      "- model_epoch_300.pt\n",
      "- model_epoch_400.pt\n",
      "- model_epoch_500.pt\n",
      "- model_epoch_600.pt\n",
      "- model_epoch_700.pt\n",
      "- model_epoch_800.pt\n",
      "- model_epoch_900.pt\n",
      "\n",
      "CNN checkpoints (9 files):\n",
      "- model_epoch_100.pt\n",
      "- model_epoch_200.pt\n",
      "- model_epoch_300.pt\n",
      "- model_epoch_400.pt\n",
      "- model_epoch_500.pt\n",
      "- model_epoch_600.pt\n",
      "- model_epoch_700.pt\n",
      "- model_epoch_800.pt\n",
      "- model_epoch_900.pt\n",
      "Initializing models...\n",
      "\n",
      "Analyzing MAMBA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing MAMBA:   0%|          | 0/12 [00:00<?, ?it/s]C:\\Users\\baris\\AppData\\Local\\Temp\\ipykernel_12032\\2657652263.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "Analyzing MAMBA:  33%|███▎      | 4/12 [19:28<35:43, 267.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing checkpoint model_epoch_1200.pt: PytorchStreamReader failed locating file data.pkl: file not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing MAMBA: 100%|██████████| 12/12 [53:05<00:00, 265.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing CNN:  67%|██████▋   | 6/9 [12:39<09:39, 193.11s/it]"
     ]
    }
   ],
   "source": [
    "# First, let's verify our data loaders and models are working\n",
    "def verify_setup():\n",
    "    print(\"\\n=== Verifying Setup ===\")\n",
    "    \n",
    "    # Check data loaders\n",
    "    print(\"\\nChecking data loaders...\")\n",
    "    try:\n",
    "        train_loader, test_loader, _, _, _, _ = load_cifar10(batch_size=32, seed=42)\n",
    "        print(f\"Train loader size: {len(train_loader)}\")\n",
    "        print(f\"Test loader size: {len(test_loader)}\")\n",
    "        \n",
    "        # Check a batch\n",
    "        print(\"\\nChecking a batch from train loader...\")\n",
    "        sample_batch, sample_labels = next(iter(train_loader))\n",
    "        print(f\"Batch shape: {sample_batch.shape}\")\n",
    "        print(f\"Labels shape: {sample_labels.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with data loaders: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    # Initialize and check models\n",
    "    print(\"\\nInitializing models...\")\n",
    "    try:\n",
    "        d_model = 128\n",
    "        n_layer = 8\n",
    "        num_classes = 10\n",
    "        \n",
    "        model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)\n",
    "        mamba_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "        cnn_model = SmallerComparableCNN()\n",
    "        \n",
    "        print(\"Models initialized successfully\")\n",
    "        \n",
    "        # Try a forward pass\n",
    "        print(\"\\nTesting forward pass...\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        mamba_model = mamba_model.to(device)\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sample_batch = sample_batch.to(device)\n",
    "            \n",
    "            print(\"Testing Mamba model...\")\n",
    "            mamba_output = mamba_model(sample_batch)\n",
    "            print(f\"Mamba output shapes: {[o.shape for o in mamba_output]}\")\n",
    "            \n",
    "            print(\"\\nTesting CNN model...\")\n",
    "            cnn_output = cnn_model(sample_batch)\n",
    "            print(f\"CNN output shapes: {[o.shape for o in cnn_output]}\")\n",
    "        \n",
    "        print(\"\\nSetup verification completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model verification: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run verification before analysis\n",
    "if verify_setup():\n",
    "    print(\"\\nProceeding with analysis...\")\n",
    "    verify_checkpoints()\n",
    "    results = run_analysis()\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\nCreating plots...\")\n",
    "        plot_vulnerability_comparison(results)\n",
    "    else:\n",
    "        print(\"Analysis failed to produce results.\")\n",
    "else:\n",
    "    print(\"\\nSetup verification failed - please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
