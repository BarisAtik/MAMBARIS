{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model import ImageMamba, ModelArgs\n",
    "from data_loader import load_cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this an import soon too\n",
    "class SmallerComparableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallerComparableCNN, self).__init__()\n",
    "        # Reduced initial channels and total layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Reduced from 64 to 32\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Reduced from 128 to 64\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Global average pooling and final dense layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 10)  # Changed input features to match last conv layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return logits, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_checkpoints(checkpoint_dir, model, device='cuda'):\n",
    "    \"\"\"Load all available checkpoints for a model.\"\"\"\n",
    "    checkpoints = []\n",
    "    epochs = []\n",
    "    \n",
    "    for filename in os.listdir(checkpoint_dir):\n",
    "        if filename.endswith('.pt'):\n",
    "            epoch = int(filename.split('_')[-1].split('.pt')[0])\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            checkpoints.append({\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'metrics': checkpoint['metrics'] if 'metrics' in checkpoint else None\n",
    "            })\n",
    "            epochs.append(epoch)\n",
    "    \n",
    "    return sorted(checkpoints, key=lambda x: x['epoch']), sorted(epochs)\n",
    "\n",
    "def evaluate_model_vulnerability(model, train_loader, test_loader, device='cuda'):\n",
    "    \"\"\"Evaluate model's vulnerability to inference attacks.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    def get_confidence_stats(loader):\n",
    "        confidences = []\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for data, labels in loader:\n",
    "                data = data.to(device)\n",
    "                _, probs = model(data)\n",
    "                max_probs, preds = torch.max(probs, dim=1)\n",
    "                confidences.extend(max_probs.cpu().numpy())\n",
    "                predictions.extend((preds == labels.to(device)).cpu().numpy())\n",
    "        return np.mean(confidences), np.std(confidences), np.mean(predictions)\n",
    "\n",
    "    train_conf_mean, train_conf_std, train_acc = get_confidence_stats(train_loader)\n",
    "    test_conf_mean, test_conf_std, test_acc = get_confidence_stats(test_loader)\n",
    "    \n",
    "    # Calculate vulnerability metrics\n",
    "    confidence_gap = train_conf_mean - test_conf_mean\n",
    "    acc_gap = train_acc - test_acc\n",
    "    \n",
    "    # Calculate confidence distribution overlap\n",
    "    confidence_overlap = min(train_conf_mean + train_conf_std, test_conf_mean + test_conf_std) - \\\n",
    "                        max(train_conf_mean - train_conf_std, test_conf_mean - test_conf_std)\n",
    "    \n",
    "    return {\n",
    "        'train_confidence': train_conf_mean,\n",
    "        'test_confidence': test_conf_mean,\n",
    "        'train_accuracy': train_acc * 100,\n",
    "        'test_accuracy': test_acc * 100,\n",
    "        'confidence_gap': confidence_gap,\n",
    "        'accuracy_gap': acc_gap * 100,\n",
    "        'confidence_overlap': confidence_overlap\n",
    "    }\n",
    "\n",
    "def analyze_models(mamba_model, cnn_model, train_loader, test_loader, mamba_dir, cnn_dir, device='cuda'):\n",
    "    \"\"\"Analyze both models across their checkpoints.\"\"\"\n",
    "    \n",
    "    # Load checkpoints\n",
    "    print(\"Loading MAMBA checkpoints...\")\n",
    "    mamba_checkpoints, mamba_epochs = load_all_checkpoints(mamba_dir, mamba_model, device)\n",
    "    print(\"Loading CNN checkpoints...\")\n",
    "    cnn_checkpoints, cnn_epochs = load_all_checkpoints(cnn_dir, cnn_model, device)\n",
    "    \n",
    "    results = {\n",
    "        'mamba': {'vulnerabilities': [], 'epochs': mamba_epochs},\n",
    "        'cnn': {'vulnerabilities': [], 'epochs': cnn_epochs}\n",
    "    }\n",
    "    \n",
    "    # Analyze MAMBA checkpoints\n",
    "    print(\"\\nAnalyzing MAMBA checkpoints...\")\n",
    "    for checkpoint in tqdm(mamba_checkpoints):\n",
    "        mamba_model.load_state_dict(checkpoint['model_state'])\n",
    "        vuln = evaluate_model_vulnerability(mamba_model, train_loader, test_loader, device)\n",
    "        results['mamba']['vulnerabilities'].append(vuln)\n",
    "    \n",
    "    # Analyze CNN checkpoints\n",
    "    print(\"\\nAnalyzing CNN checkpoints...\")\n",
    "    for checkpoint in tqdm(cnn_checkpoints):\n",
    "        cnn_model.load_state_dict(checkpoint['model_state'])\n",
    "        vuln = evaluate_model_vulnerability(cnn_model, train_loader, test_loader, device)\n",
    "        results['cnn']['vulnerabilities'].append(vuln)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_vulnerability_comparison(results, save_dir='comparison_plots'):\n",
    "    \"\"\"Plot comparison of model vulnerabilities.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = plt.GridSpec(3, 2)\n",
    "    \n",
    "    # 1. Accuracy Gaps\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(results['mamba']['epochs'], \n",
    "             [v['accuracy_gap'] for v in results['mamba']['vulnerabilities']], \n",
    "             'b-', label='MAMBA')\n",
    "    ax1.plot(results['cnn']['epochs'], \n",
    "             [v['accuracy_gap'] for v in results['cnn']['vulnerabilities']], \n",
    "             'r-', label='CNN')\n",
    "    ax1.set_title('Train-Test Accuracy Gap Evolution')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy Gap (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Confidence Gaps\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(results['mamba']['epochs'], \n",
    "             [v['confidence_gap'] for v in results['mamba']['vulnerabilities']], \n",
    "             'b-', label='MAMBA')\n",
    "    ax2.plot(results['cnn']['epochs'], \n",
    "             [v['confidence_gap'] for v in results['cnn']['vulnerabilities']], \n",
    "             'r-', label='CNN')\n",
    "    ax2.set_title('Confidence Gap Evolution')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Train-Test Confidence Gap')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Train vs Test Accuracies\n",
    "    ax3 = fig.add_subplot(gs[1, :])\n",
    "    ax3.plot(results['mamba']['epochs'], \n",
    "             [v['train_accuracy'] for v in results['mamba']['vulnerabilities']], \n",
    "             'b-', label='MAMBA Train')\n",
    "    ax3.plot(results['mamba']['epochs'], \n",
    "             [v['test_accuracy'] for v in results['mamba']['vulnerabilities']], \n",
    "             'b--', label='MAMBA Test')\n",
    "    ax3.plot(results['cnn']['epochs'], \n",
    "             [v['train_accuracy'] for v in results['cnn']['vulnerabilities']], \n",
    "             'r-', label='CNN Train')\n",
    "    ax3.plot(results['cnn']['epochs'], \n",
    "             [v['test_accuracy'] for v in results['cnn']['vulnerabilities']], \n",
    "             'r--', label='CNN Test')\n",
    "    ax3.set_title('Accuracy Comparison')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # 4. Vulnerability Score Evolution\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    mamba_scores = [v['confidence_gap'] * (1 - v['confidence_overlap']) for v in results['mamba']['vulnerabilities']]\n",
    "    cnn_scores = [v['confidence_gap'] * (1 - v['confidence_overlap']) for v in results['cnn']['vulnerabilities']]\n",
    "    \n",
    "    ax4.plot(results['mamba']['epochs'], mamba_scores, 'b-', label='MAMBA')\n",
    "    ax4.plot(results['cnn']['epochs'], cnn_scores, 'r-', label='CNN')\n",
    "    ax4.set_title('Overall Vulnerability Score Evolution')\n",
    "    ax4.set_xlabel('Epochs')\n",
    "    ax4.set_ylabel('Vulnerability Score')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'vulnerability_analysis.png'))\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nVulnerability Analysis Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    final_mamba = results['mamba']['vulnerabilities'][-1]\n",
    "    final_cnn = results['cnn']['vulnerabilities'][-1]\n",
    "    \n",
    "    print(\"\\nFinal Checkpoint Statistics:\")\n",
    "    print(f\"MAMBA:\")\n",
    "    print(f\"  - Accuracy Gap: {final_mamba['accuracy_gap']:.2f}%\")\n",
    "    print(f\"  - Confidence Gap: {final_mamba['confidence_gap']:.4f}\")\n",
    "    print(f\"  - Train Accuracy: {final_mamba['train_accuracy']:.2f}%\")\n",
    "    print(f\"  - Test Accuracy: {final_mamba['test_accuracy']:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nCNN:\")\n",
    "    print(f\"  - Accuracy Gap: {final_cnn['accuracy_gap']:.2f}%\")\n",
    "    print(f\"  - Confidence Gap: {final_cnn['confidence_gap']:.4f}\")\n",
    "    print(f\"  - Train Accuracy: {final_cnn['train_accuracy']:.2f}%\")\n",
    "    print(f\"  - Test Accuracy: {final_cnn['test_accuracy']:.2f}%\")\n",
    "    \n",
    "    # Calculate overall vulnerability scores\n",
    "    mamba_vuln_score = final_mamba['confidence_gap'] * (1 - final_mamba['confidence_overlap'])\n",
    "    cnn_vuln_score = final_cnn['confidence_gap'] * (1 - final_cnn['confidence_overlap'])\n",
    "    \n",
    "    print(f\"\\nOverall Vulnerability Score (higher = more vulnerable):\")\n",
    "    print(f\"MAMBA: {mamba_vuln_score:.4f}\")\n",
    "    print(f\"CNN: {cnn_vuln_score:.4f}\")\n",
    "    \n",
    "    conclusion = \"MAMBA\" if mamba_vuln_score > cnn_vuln_score else \"CNN\"\n",
    "    print(f\"\\nConclusion: {conclusion} appears more vulnerable to inference attacks.\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10\n",
    "\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)\n",
    "mamba_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "cnn_model = SmallerComparableCNN()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mamba_model = mamba_model.to(device)\n",
    "cnn_model = cnn_model.to(device)\n",
    "\n",
    "# Load data\n",
    "train_loader, test_loader, _, _, _, _ = load_cifar10(batch_size=64, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis with explicitly specified checkpoint directories\n",
    "results = analyze_models(\n",
    "    mamba_model=mamba_model,\n",
    "    cnn_model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    mamba_dir='mamba_checkpoints',\n",
    "    cnn_dir='cnn_checkpoints'\n",
    ")\n",
    "\n",
    "# Create plots\n",
    "plot_vulnerability_comparison(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
