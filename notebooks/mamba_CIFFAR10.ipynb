{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset\n",
    "Find new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-24.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 1)) (3.9.2)\n",
      "Requirement already satisfied: seaborn in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: scipy in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: ucimlrepo in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 4)) (0.0.7)\n",
      "Requirement already satisfied: transformers==4.30.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 5)) (4.30.0)\n",
      "Requirement already satisfied: einops==0.6.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: scikit-learn==1.4.1.post1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 7)) (1.4.1.post1)\n",
      "Requirement already satisfied: pandas==2.2.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: torch in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 9)) (2.1.2)\n",
      "Requirement already satisfied: numpy in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 10)) (1.24.3)\n",
      "Requirement already satisfied: keras==2.13.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 11)) (2.13.1)\n",
      "Requirement already satisfied: tensorflow in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 12)) (2.13.0)\n",
      "Requirement already satisfied: filelock in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 5)) (4.66.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from scikit-learn==1.4.1.post1->-r ../requirements.txt (line 7)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from scikit-learn==1.4.1.post1->-r ../requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from pandas==2.2.1->-r ../requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from pandas==2.2.1->-r ../requirements.txt (line 8)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from pandas==2.2.1->-r ../requirements.txt (line 8)) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from matplotlib->-r ../requirements.txt (line 1)) (6.4.5)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from ucimlrepo->-r ../requirements.txt (line 4)) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 9)) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 9)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 9)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 9)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 9)) (2024.9.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow->-r ../requirements.txt (line 12)) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (4.25.5)\n",
      "Requirement already satisfied: setuptools in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (2.13.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r ../requirements.txt (line 1)) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from jinja2->torch->-r ../requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from requests->transformers==4.30.0->-r ../requirements.txt (line 5)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from requests->transformers==4.30.0->-r ../requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from requests->transformers==4.30.0->-r ../requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from sympy->torch->-r ../requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (2.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (8.5.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 12)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model #save and load models\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Reshape and preprocess the CIFAR-10 dataset for PyTorch models\n",
    "X_train = X_train.transpose(0, 3, 1, 2)  # Shape: (batch_size, channels, height, width)\n",
    "X_test = X_test.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Convert data to float and normalize pixel values in the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the train/test data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Your existing train loader code\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Add test loader\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # Note: shuffle=False for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageMamba(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x ImageResidualBlock(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (norm_f): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model parameters\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create an instance of ModelArgs\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "\n",
    "# Instantiate the ImageMamba model\n",
    "model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MAMBA on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/MAMBARIS/.venv/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MAMBARIS/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model + testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 2, more epochs\n",
    "As we can see there is not much difference between the probabilities of true class predictions on the traindataset instances, and the testdataset instances.\n",
    "\n",
    "This might indicate that the dataset was hard easy for 10 epochs, lets add 15 more epochs, to get to a total of 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    " \n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see still no significant diffirence, lets make a stronger model, and train more epochs to achieve a loss close to 0, and then try inference again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 3, complex model\n",
    "To get a much lower loss, we will both increase the number of epochs and make the model more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "d_model = 128\n",
    "n_layer = 8\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create an instance of ModelArgs\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "\n",
    "# Instantiate the ImageMamba model\n",
    "complex_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "complex_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 25 \n",
    "for epoch in range(num_epochs):\n",
    "    complex_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = complex_model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will do 25 more, to have a total of 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(complex_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    complex_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = complex_model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "class TrainingMetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.test_accuracies = []\n",
    "        self.train_confidences = []\n",
    "        self.test_confidences = []\n",
    "        self.confidence_gaps = []\n",
    "        self.epoch_train_confidences = []  # Store all confidence values for each epoch\n",
    "        self.epoch_test_confidences = []\n",
    "\n",
    "    def update(self, train_loss, test_loss, train_acc, test_acc, \n",
    "              train_conf, test_conf, train_epoch_confs, test_epoch_confs):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        self.train_confidences.append(train_conf)\n",
    "        self.test_confidences.append(test_conf)\n",
    "        self.confidence_gaps.append(train_conf - test_conf)\n",
    "        self.epoch_train_confidences.append(train_epoch_confs)\n",
    "        self.epoch_test_confidences.append(test_epoch_confs)\n",
    "\n",
    "    def plot_metrics(self, save_path=None):\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        # Create a figure with subplots\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot Loss\n",
    "        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        ax1.plot(epochs, self.test_losses, 'r-', label='Test Loss')\n",
    "        ax1.set_title('Loss over Epochs')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Plot Accuracy\n",
    "        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')\n",
    "        ax2.plot(epochs, self.test_accuracies, 'r-', label='Test Accuracy')\n",
    "        ax2.set_title('Accuracy over Epochs')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        # Plot Confidence\n",
    "        ax3.plot(epochs, self.train_confidences, 'b-', label='Training Confidence')\n",
    "        ax3.plot(epochs, self.test_confidences, 'r-', label='Test Confidence')\n",
    "        ax3.set_title('Average Confidence over Epochs')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Confidence')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "\n",
    "        # Plot Confidence Gap\n",
    "        ax4.plot(epochs, self.confidence_gaps, 'g-', label='Confidence Gap')\n",
    "        ax4.set_title('Confidence Gap over Epochs')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Gap (Train - Test)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confidence_distributions(self, epoch):\n",
    "        \"\"\"Plot confidence distributions for a specific epoch\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create density plots\n",
    "        sns.kdeplot(data=self.epoch_train_confidences[epoch], label='Training', color='blue')\n",
    "        sns.kdeplot(data=self.epoch_test_confidences[epoch], label='Test', color='red')\n",
    "        \n",
    "        plt.title(f'Confidence Distributions (Epoch {epoch+1})')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def statistical_tests(self, epoch=-1):\n",
    "        \"\"\"Perform statistical tests on confidence distributions\"\"\"\n",
    "        if epoch == -1:\n",
    "            epoch = len(self.epoch_train_confidences) - 1\n",
    "\n",
    "        train_conf = self.epoch_train_confidences[epoch]\n",
    "        test_conf = self.epoch_test_confidences[epoch]\n",
    "\n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_stat, ks_pval = stats.ks_2samp(train_conf, test_conf)\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        mw_stat, mw_pval = stats.mannwhitneyu(train_conf, test_conf, alternative='two-sided')\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        cohens_d = (np.mean(train_conf) - np.mean(test_conf)) / np.sqrt(\n",
    "            (np.var(train_conf) + np.var(test_conf)) / 2)\n",
    "\n",
    "        return {\n",
    "            'ks_test': {'statistic': ks_stat, 'p_value': ks_pval},\n",
    "            'mw_test': {'statistic': mw_stat, 'p_value': mw_pval},\n",
    "            'cohens_d': cohens_d\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both the training loop and evaluation function, we need to detach tensors before converting to numpy\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confidence_sum = 0\n",
    "    all_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "            confidence_sum += confidence.sum().item()\n",
    "            all_confidences.extend(confidence.detach().cpu().numpy())  # Fixed here\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_confidence = confidence_sum / total\n",
    "    \n",
    "    return accuracy, avg_loss, avg_confidence, all_confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]\n",
      "  Training:\n",
      "    Loss: 1.4128\n",
      "    Accuracy: 50.22%\n",
      "    Average Confidence: 0.4167\n",
      "  Testing:\n",
      "    Loss: 1.3802\n",
      "    Accuracy: 51.31%\n",
      "    Average Confidence: 0.4751\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics tracker\n",
    "metrics_tracker = TrainingMetricsTracker()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "    train_confidence_sum = 0\n",
    "    train_epoch_confidences = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        logits, probabilities = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        confidence, _ = torch.max(probabilities, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        train_confidence_sum += confidence.sum().item()\n",
    "        train_epoch_confidences.extend(confidence.detach().cpu().numpy())  # Fixed here\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = (running_correct / total_samples) * 100\n",
    "    train_avg_confidence = train_confidence_sum / total_samples\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_accuracy, test_loss, test_avg_confidence, test_epoch_confidences = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(\n",
    "        train_loss, test_loss,\n",
    "        train_accuracy, test_accuracy,\n",
    "        train_avg_confidence, test_avg_confidence,\n",
    "        train_epoch_confidences, test_epoch_confidences\n",
    "    )\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    print(f'  Training:')\n",
    "    print(f'    Loss: {train_loss:.4f}')\n",
    "    print(f'    Accuracy: {train_accuracy:.2f}%')\n",
    "    print(f'    Average Confidence: {train_avg_confidence:.4f}')\n",
    "    print(f'  Testing:')\n",
    "    print(f'    Loss: {test_loss:.4f}')\n",
    "    print(f'    Accuracy: {test_accuracy:.2f}%')\n",
    "    print(f'    Average Confidence: {test_avg_confidence:.4f}')\n",
    "    \n",
    "    # Perform statistical tests every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"\\nStatistical Tests:\")\n",
    "        stats_results = metrics_tracker.statistical_tests(epoch)\n",
    "        print(f\"  Kolmogorov-Smirnov test:\")\n",
    "        print(f\"    Statistic: {stats_results['ks_test']['statistic']:.4f}\")\n",
    "        print(f\"    p-value: {stats_results['ks_test']['p_value']:.4f}\")\n",
    "        print(f\"  Mann-Whitney U test:\")\n",
    "        print(f\"    Statistic: {stats_results['mw_test']['statistic']:.4f}\")\n",
    "        print(f\"    p-value: {stats_results['mw_test']['p_value']:.4f}\")\n",
    "        print(f\"  Cohen's d: {stats_results['cohens_d']:.4f}\")\n",
    "        \n",
    "        # Plot confidence distributions\n",
    "        metrics_tracker.plot_confidence_distributions(epoch)\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "# After training, plot all metrics\n",
    "metrics_tracker.plot_metrics(save_path='training_metrics.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
