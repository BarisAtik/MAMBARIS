{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python --version\n",
    "#!pip install --upgrade pip\n",
    "#!pip uninstall keras tensorflow\n",
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from model import ModelArgs, ImageMamba\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from model import ImageMamba, ModelArgs\n",
    "from __future__ import print_function\n",
    "from data_loader import load_cifar10, get_class_names # For consistent data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, X_train, X_test, Y_train, Y_test = load_cifar10(batch_size=64, seed=42)\n",
    "class_names = get_class_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints & measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from check_and_measure import evaluate_model, save_checkpoint, load_last_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mamba(model, train_loader, test_loader, num_epochs=2000, device='cuda',\n",
    "               checkpoint_dir='mamba_checkpoints', checkpoint_freq=100):\n",
    "   \"\"\"Train model with comprehensive metrics tracking.\"\"\"\n",
    "   \n",
    "   # Check if directory exists and contains files\n",
    "   if os.path.exists(checkpoint_dir) and os.listdir(checkpoint_dir):\n",
    "       raise RuntimeError(\n",
    "           f\"Directory {checkpoint_dir} already contains files. Using this directory would overwrite \"\n",
    "           \"existing training data. To prevent data loss, please use an empty directory \"\n",
    "           \"or use continue_training() to resume from the last checkpoint.\"\n",
    "       )\n",
    "       \n",
    "   os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "   criterion = nn.CrossEntropyLoss()\n",
    "   optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "   \n",
    "   scheduler = OneCycleLR(\n",
    "       optimizer,\n",
    "       max_lr=1e-3,\n",
    "       epochs=num_epochs,\n",
    "       steps_per_epoch=len(train_loader),\n",
    "       pct_start=0.3,\n",
    "       anneal_strategy='cos'\n",
    "   )\n",
    "   \n",
    "   metrics = {\n",
    "       'train_losses': [], 'test_losses': [],\n",
    "       'train_accuracies': [], 'test_accuracies': [],\n",
    "       'train_confidences': [], 'test_confidences': [],\n",
    "       'epoch_train_confidences': [], 'epoch_test_confidences': []\n",
    "   }\n",
    "   \n",
    "   for epoch in range(num_epochs):\n",
    "       model.train()\n",
    "       running_loss = 0.0\n",
    "       running_correct = 0\n",
    "       total_samples = 0\n",
    "       train_confidences = []\n",
    "       \n",
    "       for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "           inputs, labels = inputs.to(device), labels.to(device)\n",
    "           optimizer.zero_grad()\n",
    "           \n",
    "           logits, probabilities = model(inputs)\n",
    "           loss = criterion(logits, labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           scheduler.step()\n",
    "           \n",
    "           _, predicted = torch.max(logits, 1)\n",
    "           confidence, _ = torch.max(probabilities, 1)\n",
    "           \n",
    "           running_loss += loss.item()\n",
    "           running_correct += (predicted == labels).sum().item()\n",
    "           total_samples += labels.size(0)\n",
    "           train_confidences.extend(confidence.detach().cpu().numpy())\n",
    "       \n",
    "       train_loss = running_loss / len(train_loader)\n",
    "       train_accuracy = 100 * running_correct / total_samples\n",
    "       train_avg_confidence = np.mean(train_confidences)\n",
    "       \n",
    "       test_loss, test_accuracy, test_avg_confidence, test_confidences = evaluate_model(\n",
    "           model, test_loader, criterion, device)\n",
    "       \n",
    "       metrics['train_losses'].append(train_loss)\n",
    "       metrics['test_losses'].append(test_loss)\n",
    "       metrics['train_accuracies'].append(train_accuracy)\n",
    "       metrics['test_accuracies'].append(test_accuracy)\n",
    "       metrics['train_confidences'].append(train_avg_confidence)\n",
    "       metrics['test_confidences'].append(test_avg_confidence)\n",
    "       metrics['epoch_train_confidences'].append(train_confidences)\n",
    "       metrics['epoch_test_confidences'].append(test_confidences)\n",
    "       \n",
    "       print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "       print(f'Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "       print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "       \n",
    "       if (epoch + 1) % checkpoint_freq == 0:\n",
    "           checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
    "           save_checkpoint(model, optimizer, scheduler, epoch, metrics, checkpoint_path)\n",
    "           \n",
    "           metrics_path = os.path.join(checkpoint_dir, 'training_metrics.json')\n",
    "           json_metrics = {\n",
    "               'train_losses': [float(x) for x in metrics['train_losses']],\n",
    "               'test_losses': [float(x) for x in metrics['test_losses']],\n",
    "               'train_accuracies': [float(x) for x in metrics['train_accuracies']],\n",
    "               'test_accuracies': [float(x) for x in metrics['test_accuracies']],\n",
    "               'train_confidences': [float(x) for x in metrics['train_confidences']],\n",
    "               'test_confidences': [float(x) for x in metrics['test_confidences']],\n",
    "               'current_epoch': epoch + 1\n",
    "           }\n",
    "           with open(metrics_path, 'w') as f:\n",
    "               json.dump(json_metrics, f, indent=4)\n",
    "   \n",
    "   return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters for overfitting\n",
    "d_model = 128\n",
    "n_layer = 8\n",
    "num_classes = 10\n",
    "\n",
    "# Initialize model\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)\n",
    "model = ImageMamba(model_args, num_classes=num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "metrics = train_mamba(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=2000,\n",
    "    device=device,\n",
    "    checkpoint_freq=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crash\n",
    "If there was a crash. Which can be when dealing with so many epochs, one can continue from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_training(model, train_loader, test_loader, checkpoint_dir, target_epochs=2000, device='cuda'):\n",
    "    \"\"\"Continue training from last checkpoint.\"\"\"\n",
    "    checkpoint, last_epoch = load_last_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        epochs=target_epochs - last_epoch,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    if checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Load existing metrics\n",
    "    with open(os.path.join(checkpoint_dir, 'training_metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    complete_metrics = {\n",
    "        'train_losses': metrics['train_losses'],\n",
    "        'test_losses': metrics['test_losses'],\n",
    "        'train_accuracies': metrics['train_accuracies'],\n",
    "        'test_accuracies': metrics['test_accuracies'],\n",
    "        'train_confidences': metrics['train_confidences'],\n",
    "        'test_confidences': metrics['test_confidences'],\n",
    "        'epoch_train_confidences': checkpoint['metrics']['epoch_train_confidences'],\n",
    "        'epoch_test_confidences': checkpoint['metrics']['epoch_test_confidences']\n",
    "    }\n",
    "    \n",
    "    print(f\"Continuing training from epoch {last_epoch} to {target_epochs}\")\n",
    "    \n",
    "    for epoch in range(last_epoch, target_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        train_confidences = []\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch [{epoch+1}/{target_epochs}]'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            train_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * running_correct / total_samples\n",
    "        train_avg_confidence = np.mean(train_confidences)\n",
    "        \n",
    "        test_loss, test_accuracy, test_avg_confidence, test_confidences = evaluate_model(\n",
    "            model, test_loader, criterion, device)\n",
    "        \n",
    "        complete_metrics['train_losses'].append(train_loss)\n",
    "        complete_metrics['test_losses'].append(test_loss)\n",
    "        complete_metrics['train_accuracies'].append(train_accuracy)\n",
    "        complete_metrics['test_accuracies'].append(test_accuracy)\n",
    "        complete_metrics['train_confidences'].append(train_avg_confidence)\n",
    "        complete_metrics['test_confidences'].append(test_avg_confidence)\n",
    "        complete_metrics['epoch_train_confidences'].append(train_confidences)\n",
    "        complete_metrics['epoch_test_confidences'].append(test_confidences)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, complete_metrics, checkpoint_path)\n",
    "            \n",
    "            json_metrics = {\n",
    "                'train_losses': [float(x) for x in complete_metrics['train_losses']],\n",
    "                'test_losses': [float(x) for x in complete_metrics['test_losses']],\n",
    "                'train_accuracies': [float(x) for x in complete_metrics['train_accuracies']],\n",
    "                'test_accuracies': [float(x) for x in complete_metrics['test_accuracies']],\n",
    "                'train_confidences': [float(x) for x in complete_metrics['train_confidences']],\n",
    "                'test_confidences': [float(x) for x in complete_metrics['test_confidences']],\n",
    "                'current_epoch': epoch + 1\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'training_metrics.json'), 'w') as f:\n",
    "                json.dump(json_metrics, f, indent=4)\n",
    "    \n",
    "    return complete_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mamba_checkpoints/training_metrics.json', 'r') as f:\n",
    "   metrics = json.load(f)\n",
    "print(f\"Last completed epoch: {metrics['current_epoch']}\")\n",
    "\n",
    "metrics = continue_training(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    checkpoint_dir='mamba_checkpoints',\n",
    "    target_epochs=2000,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
