{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python --version\n",
    "#!pip install --upgrade pip\n",
    "#!pip uninstall keras tensorflow\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Reshape and preprocess the CIFAR-10 dataset for PyTorch models\n",
    "X_train = X_train.transpose(0, 3, 1, 2)  # Shape: (batch_size, channels, height, width)\n",
    "X_test = X_test.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Convert data to float and normalize pixel values in the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the train/test data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Your existing train loader code\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Add test loader\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # Note: shuffle=False for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pytorch_model(model, filepath, epoch=None):\n",
    "    \"\"\"\n",
    "    Save a PyTorch model to disk, including both architecture and weights.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model (nn.Module)\n",
    "        filepath: Path to save the model\n",
    "        epoch: Optional epoch number to include in the save\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "    \n",
    "    # Prepare the save dictionary\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_args': model.args,  # Saving the model arguments\n",
    "        'epoch': epoch if epoch is not None else None\n",
    "    }\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(save_dict, filepath)\n",
    "    print(f\"Model saved successfully to {filepath}\")\n",
    "\n",
    "def load_pytorch_model(filepath):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model from disk.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded PyTorch model\n",
    "        epoch: Epoch number when the model was saved (if available)\n",
    "    \"\"\"\n",
    "    # Load the save dictionary\n",
    "    save_dict = torch.load(filepath)\n",
    "    \n",
    "    # Create a new model instance with the saved arguments\n",
    "    model = ImageMamba(args=save_dict['model_args'], num_classes=1000)  # Adjust num_classes as needed\n",
    "    \n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(save_dict['model_state_dict'])\n",
    "    \n",
    "    return model, save_dict.get('epoch')\n",
    "\n",
    "# Example usage:\n",
    "# Saving the model\n",
    "# save_pytorch_model(complex_model, '25epoch_complex_model.pt', epoch=25)\n",
    "\n",
    "# Loading the model\n",
    "# loaded_model, epoch = load_pytorch_model('25epoch_complex_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create an instance of ModelArgs\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "\n",
    "# Instantiate the ImageMamba model\n",
    "model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MAMBA on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model + testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 2, more epochs\n",
    "As we can see there is not much difference between the probabilities of true class predictions on the traindataset instances, and the testdataset instances.\n",
    "\n",
    "This might indicate that the dataset was hard easy for 10 epochs, lets add 15 more epochs, to get to a total of 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    " \n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see still no significant diffirence, lets make a stronger model, and train more epochs to achieve a loss close to 0, and then try inference again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 3, complex model\n",
    "To get a much lower loss, we will both increase the number of epochs and make the model more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "d_model = 128\n",
    "n_layer = 8\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create an instance of ModelArgs\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "\n",
    "# Instantiate the ImageMamba model\n",
    "complex_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "complex_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(complex_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25 \n",
    "for epoch in range(num_epochs):\n",
    "    complex_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = complex_model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after 25 epochs\n",
    "save_pytorch_model(complex_model, '../trained_models/25epoch_complex_model.pt', epoch=25)\n",
    "\n",
    "# Later, to load the model:\n",
    "# loaded_model, epoch = load_pytorch_model('25epoch_complex_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "complex_model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = complex_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "complex_model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = complex_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference in percentage is: \", (average_prob_correct_class_train - average_prob_correct_class_test)/average_prob_correct_class_train*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the model started to overfit which is why the loss stopped decreasing, now we will try a different optimiser and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer with AdamW\n",
    "optimizer = optim.AdamW(complex_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Cosine Annealing Scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# Training Loop with Scheduler\n",
    "num_epochs = 50  # More epochs to ensure convergence\n",
    "for epoch in range(num_epochs):\n",
    "    complex_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = complex_model(inputs)\n",
    "\n",
    "        # Flatten labels if necessary\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Step the scheduler at each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after 25 epochs\n",
    "save_pytorch_model(complex_model, '../trained_models/different_50epoch_complex_model.pt', epoch=25)\n",
    "\n",
    "# Later, to load the model:\n",
    "# loaded_model, epoch = load_pytorch_model('25epoch_complex_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "complex_model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = complex_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "complex_model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = complex_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference in percentage is: \", (average_prob_correct_class_train - average_prob_correct_class_test)/average_prob_correct_class_train*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "class TrainingMetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.test_accuracies = []\n",
    "        self.train_confidences = []\n",
    "        self.test_confidences = []\n",
    "        self.confidence_gaps = []\n",
    "        self.epoch_train_confidences = []  # Store all confidence values for each epoch\n",
    "        self.epoch_test_confidences = []\n",
    "\n",
    "    def update(self, train_loss, test_loss, train_acc, test_acc, \n",
    "              train_conf, test_conf, train_epoch_confs, test_epoch_confs):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        self.train_confidences.append(train_conf)\n",
    "        self.test_confidences.append(test_conf)\n",
    "        self.confidence_gaps.append(train_conf - test_conf)\n",
    "        self.epoch_train_confidences.append(train_epoch_confs)\n",
    "        self.epoch_test_confidences.append(test_epoch_confs)\n",
    "\n",
    "    def plot_metrics(self, save_path=None):\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        # Create a figure with subplots\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot Loss\n",
    "        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        ax1.plot(epochs, self.test_losses, 'r-', label='Test Loss')\n",
    "        ax1.set_title('Loss over Epochs')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Plot Accuracy\n",
    "        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')\n",
    "        ax2.plot(epochs, self.test_accuracies, 'r-', label='Test Accuracy')\n",
    "        ax2.set_title('Accuracy over Epochs')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        # Plot Confidence\n",
    "        ax3.plot(epochs, self.train_confidences, 'b-', label='Training Confidence')\n",
    "        ax3.plot(epochs, self.test_confidences, 'r-', label='Test Confidence')\n",
    "        ax3.set_title('Average Confidence over Epochs')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Confidence')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "\n",
    "        # Plot Confidence Gap\n",
    "        ax4.plot(epochs, self.confidence_gaps, 'g-', label='Confidence Gap')\n",
    "        ax4.set_title('Confidence Gap over Epochs')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Gap (Train - Test)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confidence_distributions(self, epoch):\n",
    "        \"\"\"Plot confidence distributions for a specific epoch\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create density plots\n",
    "        sns.kdeplot(data=self.epoch_train_confidences[epoch], label='Training', color='blue')\n",
    "        sns.kdeplot(data=self.epoch_test_confidences[epoch], label='Test', color='red')\n",
    "        \n",
    "        plt.title(f'Confidence Distributions (Epoch {epoch+1})')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def statistical_tests(self, epoch=-1):\n",
    "        \"\"\"Perform statistical tests on confidence distributions\"\"\"\n",
    "        if epoch == -1:\n",
    "            epoch = len(self.epoch_train_confidences) - 1\n",
    "\n",
    "        train_conf = self.epoch_train_confidences[epoch]\n",
    "        test_conf = self.epoch_test_confidences[epoch]\n",
    "\n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_stat, ks_pval = stats.ks_2samp(train_conf, test_conf)\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        mw_stat, mw_pval = stats.mannwhitneyu(train_conf, test_conf, alternative='two-sided')\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        cohens_d = (np.mean(train_conf) - np.mean(test_conf)) / np.sqrt(\n",
    "            (np.var(train_conf) + np.var(test_conf)) / 2)\n",
    "\n",
    "        return {\n",
    "            'ks_test': {'statistic': ks_stat, 'p_value': ks_pval},\n",
    "            'mw_test': {'statistic': mw_stat, 'p_value': mw_pval},\n",
    "            'cohens_d': cohens_d\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both the training loop and evaluation function, we need to detach tensors before converting to numpy\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confidence_sum = 0\n",
    "    all_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "            confidence_sum += confidence.sum().item()\n",
    "            all_confidences.extend(confidence.detach().cpu().numpy())  # Fixed here\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_confidence = confidence_sum / total\n",
    "    \n",
    "    return accuracy, avg_loss, avg_confidence, all_confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "    train_confidence_sum = 0\n",
    "    train_epoch_confidences = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        logits, probabilities = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        confidence, _ = torch.max(probabilities, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        train_confidence_sum += confidence.sum().item()\n",
    "        train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = (running_correct / total_samples) * 100\n",
    "    train_avg_confidence = train_confidence_sum / total_samples\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy, test_loss, test_avg_confidence, test_epoch_confidences = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(\n",
    "        train_loss, test_loss,\n",
    "        train_accuracy, test_accuracy,\n",
    "        train_avg_confidence, test_avg_confidence,\n",
    "        train_epoch_confidences, test_epoch_confidences\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    print(f'  Training:')\n",
    "    print(f'    Loss: {train_loss:.4f}')\n",
    "    print(f'    Accuracy: {train_accuracy:.2f}%')\n",
    "    print(f'    Average Confidence: {train_avg_confidence:.4f}')\n",
    "    print(f'  Testing:')\n",
    "    print(f'    Loss: {test_loss:.4f}')\n",
    "    print(f'    Accuracy: {test_accuracy:.2f}%')\n",
    "    print(f'    Average Confidence: {test_avg_confidence:.4f}')\n",
    "    \n",
    "    # Save model every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
    "        save_pytorch_model(model, checkpoint_path, epoch=epoch+1)\n",
    "        print(f'Saved model checkpoint at epoch {epoch+1}')\n",
    "    \n",
    "    # Perform statistical tests every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"\\nStatistical Tests:\")\n",
    "        stats_results = metrics_tracker.statistical_tests(epoch)\n",
    "        print(f\"  Kolmogorov-Smirnov test:\")\n",
    "        print(f\"    Statistic: {stats_results['ks_test']['statistic']:.4f}\")\n",
    "        print(f\"    p-value: {stats_results['ks_test']['p_value']:.4f}\")\n",
    "        print(f\"  Mann-Whitney U test:\")\n",
    "        print(f\"    Statistic: {stats_results['mw_test']['statistic']:.4f}\")\n",
    "        print(f\"    p-value: {stats_results['mw_test']['p_value']:.4f}\")\n",
    "        print(f\"  Cohen's d: {stats_results['cohens_d']:.4f}\")\n",
    "        \n",
    "        # Plot confidence distributions\n",
    "        metrics_tracker.plot_confidence_distributions(epoch)\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "# After training, plot all metrics\n",
    "metrics_tracker.plot_metrics(save_path='training_metrics.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the saved model from epoch 100\n",
    "checkpoint_path = os.path.join('model_checkpoints', 'model_epoch_100.pt')\n",
    "print(f\"Loading model from {checkpoint_path}\")\n",
    "saved_model = torch.load(checkpoint_path)\n",
    "model.load_state_dict(saved_model['model_state_dict'])\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics_tracker = TrainingMetricsTracker()\n",
    "\n",
    "# Create directories for model checkpoints and plots\n",
    "checkpoint_dir = 'model_checkpoints_extended'\n",
    "plots_dir = 'training_plots'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Training loop - start from epoch 101 to 400\n",
    "start_epoch = 101\n",
    "num_epochs = 400\n",
    "last_plot_epoch = start_epoch - 1\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "    train_confidence_sum = 0\n",
    "    train_epoch_confidences = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        logits, probabilities = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        confidence, _ = torch.max(probabilities, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        train_confidence_sum += confidence.sum().item()\n",
    "        train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = (running_correct / total_samples) * 100\n",
    "    train_avg_confidence = train_confidence_sum / total_samples\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy, test_loss, test_avg_confidence, test_epoch_confidences = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(\n",
    "        train_loss, test_loss,\n",
    "        train_accuracy, test_accuracy,\n",
    "        train_avg_confidence, test_avg_confidence,\n",
    "        train_epoch_confidences, test_epoch_confidences\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "    print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "    print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "    \n",
    "    # Save model every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt')\n",
    "        save_pytorch_model(model, checkpoint_path, epoch=epoch)\n",
    "        print(f'Saved checkpoint: epoch {epoch}')\n",
    "    \n",
    "    # Create summary plots every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        plot_filename = os.path.join(plots_dir, f'training_metrics_epoch_{epoch}.png')\n",
    "        metrics_tracker.plot_metrics(save_path=plot_filename)\n",
    "        print(f'Saved plots for epochs {last_plot_epoch+1}-{epoch}')\n",
    "        \n",
    "        if epoch < num_epochs:\n",
    "            last_plot_epoch = epoch\n",
    "            metrics_tracker = TrainingMetricsTracker()\n",
    "    \n",
    "    # Perform statistical tests every 5 epochs, but only after we have enough data\n",
    "    if epoch % 5 == 0 and (epoch - last_plot_epoch) > 0:\n",
    "        try:\n",
    "            stats_results = metrics_tracker.statistical_tests(epoch - last_plot_epoch - 1)  # Use -1 to get last complete epoch\n",
    "            print(f\"Stats: KS={stats_results['ks_test']['statistic']:.4f}(p={stats_results['ks_test']['p_value']:.4f}), \" \n",
    "                  f\"MW={stats_results['mw_test']['statistic']:.4f}(p={stats_results['mw_test']['p_value']:.4f}), \"\n",
    "                  f\"Cohen's d={stats_results['cohens_d']:.4f}\")\n",
    "            \n",
    "            metrics_tracker.plot_confidence_distributions(epoch - last_plot_epoch - 1)\n",
    "        except IndexError:\n",
    "            print(\"Skipping statistical tests - not enough data yet\")\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "# Final summary plot\n",
    "final_plot_filename = os.path.join(plots_dir, 'training_metrics_final.png')\n",
    "metrics_tracker.plot_metrics(save_path=final_plot_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test inference from loaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_saved_model(model_path, test_loader, train_loader):\n",
    "    global model  # Use the globally defined model\n",
    "    saved_model = torch.load(model_path, map_location=torch.device('cpu')) # Only if on machine without GPU\n",
    "    model.load_state_dict(saved_model['model_state_dict'])\n",
    "    model = model.to('cpu')\n",
    "    device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    metrics = {}\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0\n",
    "    test_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            test_loss += loss.item()\n",
    "            test_confidences.extend(confidence.detach().numpy())\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    train_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            train_loss += loss.item()\n",
    "            train_confidences.extend(confidence.detach().numpy())\n",
    "    \n",
    "    metrics['test_accuracy'] = 100 * test_correct / test_total\n",
    "    metrics['test_loss'] = test_loss / len(test_loader)\n",
    "    metrics['test_confidence'] = np.mean(test_confidences)\n",
    "    metrics['train_accuracy'] = 100 * train_correct / train_total\n",
    "    metrics['train_loss'] = train_loss / len(train_loader)\n",
    "    metrics['train_confidence'] = np.mean(train_confidences)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Collect epochs and metrics\n",
    "epochs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_confidences = []\n",
    "test_confidences = []\n",
    "\n",
    "# Load and evaluate checkpoints from both directories\n",
    "for directory in ['model_checkpoints', 'model_checkpoints_extended']:\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.startswith('model_epoch_'):\n",
    "            epoch = int(filename.split('_')[-1].split('.')[0])\n",
    "            print(f\"Evaluating epoch {epoch}...\")\n",
    "            model_path = os.path.join(directory, filename)\n",
    "            metrics = evaluate_saved_model(model_path, test_loader, train_loader)\n",
    "            # Rest of the code remains same\n",
    "\n",
    "            epochs.append(epoch)\n",
    "            train_losses.append(metrics['train_loss'])\n",
    "            test_losses.append(metrics['test_loss'])\n",
    "            train_accuracies.append(metrics['train_accuracy'])\n",
    "            test_accuracies.append(metrics['test_accuracy'])\n",
    "            train_confidences.append(metrics['train_confidence'])\n",
    "            test_confidences.append(metrics['test_confidence'])\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, test_accuracies, 'r-', label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_confidences, 'b-', label='Training Confidence')\n",
    "plt.plot(epochs, test_confidences, 'r-', label='Test Confidence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Average Confidence over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "confidence_gap = np.array(train_confidences) - np.array(test_confidences)\n",
    "plt.plot(epochs, confidence_gap, 'g-', label='Confidence Gap')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gap (Train - Test)')\n",
    "plt.title('Confidence Gap over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('complete_training_history.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference in percentage is: \", (average_prob_correct_class_train - average_prob_correct_class_test)/average_prob_correct_class_train*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training from epoch 400 to 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the saved model from epoch 100\n",
    "checkpoint_path = os.path.join('model_checkpoints_extended', 'model_epoch_400.pt')\n",
    "print(f\"Loading model from {checkpoint_path}\")\n",
    "saved_model = torch.load(checkpoint_path)\n",
    "model.load_state_dict(saved_model['model_state_dict'])\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics_tracker = TrainingMetricsTracker()\n",
    "\n",
    "# Initialize metrics dictionary for JSON saving\n",
    "metrics = {\n",
    "    'train_losses': [],\n",
    "    'test_losses': [],\n",
    "    'train_accuracies': [],\n",
    "    'test_accuracies': [],\n",
    "    'train_confidences': [],\n",
    "    'test_confidences': [],\n",
    "    'epoch_train_confidences': [],\n",
    "    'epoch_test_confidences': []\n",
    "}\n",
    "\n",
    "# Create directories for model checkpoints and plots\n",
    "checkpoint_dir = 'model_checkpoints_extended'\n",
    "plots_dir = 'training_plots'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Training loop - start from epoch 101 to 400\n",
    "start_epoch = 401\n",
    "num_epochs = 900\n",
    "last_plot_epoch = start_epoch - 1\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "    train_confidence_sum = 0\n",
    "    train_epoch_confidences = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        logits, probabilities = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        confidence, _ = torch.max(probabilities, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        train_confidence_sum += confidence.sum().item()\n",
    "        train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = (running_correct / total_samples) * 100\n",
    "    train_avg_confidence = train_confidence_sum / total_samples\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy, test_loss, test_avg_confidence, test_epoch_confidences = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(\n",
    "        train_loss, test_loss,\n",
    "        train_accuracy, test_accuracy,\n",
    "        train_avg_confidence, test_avg_confidence,\n",
    "        train_epoch_confidences, test_epoch_confidences\n",
    "    )\n",
    "    \n",
    "    # Update metrics dictionary for JSON saving\n",
    "    metrics['train_losses'].append(round(float(train_loss), 4))\n",
    "    metrics['test_losses'].append(round(float(test_loss), 4))\n",
    "    metrics['train_accuracies'].append(round(float(train_accuracy), 4))\n",
    "    metrics['test_accuracies'].append(round(float(test_accuracy), 4))\n",
    "    metrics['train_confidences'].append(round(float(train_avg_confidence), 4))\n",
    "    metrics['test_confidences'].append(round(float(test_avg_confidence), 4))\n",
    "    metrics['epoch_train_confidences'].append([round(float(x), 4) for x in train_epoch_confidences])\n",
    "    metrics['epoch_test_confidences'].append([round(float(x), 4) for x in test_epoch_confidences])\n",
    "    \n",
    "    # Save metrics to JSON file after each epoch\n",
    "    metrics_path = os.path.join(checkpoint_dir, 'training_metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "    print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "    print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "    \n",
    "    # Save model every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt')\n",
    "        save_pytorch_model(model, checkpoint_path, epoch=epoch)\n",
    "        print(f'Saved checkpoint: epoch {epoch}')\n",
    "    \n",
    "    # Create summary plots every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        plot_filename = os.path.join(plots_dir, f'training_metrics_epoch_{epoch}.png')\n",
    "        metrics_tracker.plot_metrics(save_path=plot_filename)\n",
    "        print(f'Saved plots for epochs {last_plot_epoch+1}-{epoch}')\n",
    "        \n",
    "        if epoch < num_epochs:\n",
    "            last_plot_epoch = epoch\n",
    "            metrics_tracker = TrainingMetricsTracker()\n",
    "    \n",
    "    # Perform statistical tests every 5 epochs, but only after we have enough data\n",
    "    if epoch % 5 == 0 and (epoch - last_plot_epoch) > 0:\n",
    "        try:\n",
    "            stats_results = metrics_tracker.statistical_tests(epoch - last_plot_epoch - 1)  # Use -1 to get last complete epoch\n",
    "            print(f\"Stats: KS={stats_results['ks_test']['statistic']:.4f}(p={stats_results['ks_test']['p_value']:.4f}), \" \n",
    "                  f\"MW={stats_results['mw_test']['statistic']:.4f}(p={stats_results['mw_test']['p_value']:.4f}), \"\n",
    "                  f\"Cohen's d={stats_results['cohens_d']:.4f}\")\n",
    "            \n",
    "            metrics_tracker.plot_confidence_distributions(epoch - last_plot_epoch - 1)\n",
    "        except IndexError:\n",
    "            print(\"Skipping statistical tests - not enough data yet\")\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "# Final summary plot\n",
    "final_plot_filename = os.path.join(plots_dir, 'training_metrics_final.png')\n",
    "metrics_tracker.plot_metrics(save_path=final_plot_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 800 epooch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_saved_model(model_path, test_loader, train_loader):\n",
    "    global model  # Use the globally defined model\n",
    "    saved_model = torch.load(model_path, map_location=torch.device('cpu')) # Only if on machine without GPU\n",
    "    model.load_state_dict(saved_model['model_state_dict'])\n",
    "    model = model.to('cpu')\n",
    "    device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    metrics = {}\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0\n",
    "    test_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            test_loss += loss.item()\n",
    "            test_confidences.extend(confidence.detach().numpy())\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    train_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            train_loss += loss.item()\n",
    "            train_confidences.extend(confidence.detach().numpy())\n",
    "    \n",
    "    metrics['test_accuracy'] = 100 * test_correct / test_total\n",
    "    metrics['test_loss'] = test_loss / len(test_loader)\n",
    "    metrics['test_confidence'] = np.mean(test_confidences)\n",
    "    metrics['train_accuracy'] = 100 * train_correct / train_total\n",
    "    metrics['train_loss'] = train_loss / len(train_loader)\n",
    "    metrics['train_confidence'] = np.mean(train_confidences)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Collect epochs and metrics\n",
    "epochs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_confidences = []\n",
    "test_confidences = []\n",
    "\n",
    "# Load and evaluate checkpoints from both directories\n",
    "for directory in ['model_checkpoints', 'model_checkpoints_extended']:\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.startswith('model_epoch_'):\n",
    "            epoch = int(filename.split('_')[-1].split('.')[0])\n",
    "            print(f\"Evaluating epoch {epoch}...\")\n",
    "            model_path = os.path.join(directory, filename)\n",
    "            metrics = evaluate_saved_model(model_path, test_loader, train_loader)\n",
    "            # Rest of the code remains same\n",
    "\n",
    "            epochs.append(epoch)\n",
    "            train_losses.append(metrics['train_loss'])\n",
    "            test_losses.append(metrics['test_loss'])\n",
    "            train_accuracies.append(metrics['train_accuracy'])\n",
    "            test_accuracies.append(metrics['test_accuracy'])\n",
    "            train_confidences.append(metrics['train_confidence'])\n",
    "            test_confidences.append(metrics['test_confidence'])\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, test_accuracies, 'r-', label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_confidences, 'b-', label='Training Confidence')\n",
    "plt.plot(epochs, test_confidences, 'r-', label='Test Confidence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Average Confidence over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "confidence_gap = np.array(train_confidences) - np.array(test_confidences)\n",
    "plt.plot(epochs, confidence_gap, 'g-', label='Confidence Gap')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gap (Train - Test)')\n",
    "plt.title('Confidence Gap over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('complete_training_history.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model output format\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(train_loader))\n",
    "    inputs, labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.view(-1)\n",
    "    logits, probabilities = model(inputs)\n",
    "    print(f\"Input shape: {inputs.shape}\")\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Probabilities shape: {probabilities.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AttackModel, self).__init__()\n",
    "        self.input_size = num_classes * 2  # logits + probabilities\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        if x.size(1) != self.input_size:\n",
    "            raise ValueError(f\"Expected input size {self.input_size}, got {x.size(1)}\")\n",
    "        return self.classifier(x)\n",
    "\n",
    "class MembershipInferenceAttack:\n",
    "    def __init__(self, target_model, device='cpu'):\n",
    "        self.target_model = target_model\n",
    "        self.attack_model = AttackModel().to(device)\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.attack_model.parameters(), lr=0.001)\n",
    "        \n",
    "    def prepare_attack_data(self, train_loader, test_loader):\n",
    "        \"\"\"Prepare data for training the attack model\"\"\"\n",
    "        attack_inputs = []\n",
    "        attack_labels = []\n",
    "        \n",
    "        self.target_model.eval()\n",
    "        with torch.no_grad():\n",
    "            print(\"Processing training data...\")\n",
    "            for data, labels in train_loader:\n",
    "                data = data.to(self.device)\n",
    "                labels = labels.view(-1)\n",
    "                outputs = self.target_model(data)\n",
    "                # Handle different model output formats\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits, probabilities = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                features = torch.cat([logits, probabilities], dim=1)\n",
    "                attack_inputs.append(features.cpu())\n",
    "                attack_labels.extend([1] * logits.size(0))\n",
    "                \n",
    "            print(\"\\nProcessing test data...\")\n",
    "            for data, labels in test_loader:\n",
    "                data = data.to(self.device)\n",
    "                labels = labels.view(-1)\n",
    "                outputs = self.target_model(data)\n",
    "                # Handle different model output formats\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits, probabilities = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                features = torch.cat([logits, probabilities], dim=1)\n",
    "                attack_inputs.append(features.cpu())\n",
    "                attack_labels.extend([0] * logits.size(0))\n",
    "        \n",
    "        attack_inputs = torch.cat(attack_inputs, dim=0)\n",
    "        attack_labels = torch.tensor(attack_labels, dtype=torch.long)\n",
    "        \n",
    "        return attack_inputs, attack_labels\n",
    "\n",
    "    def train_attack(self, train_loader, test_loader, epochs=10, batch_size=64):\n",
    "        \"\"\"Train the membership inference attack model\"\"\"\n",
    "        print(\"Preparing attack data...\")\n",
    "        X, y = self.prepare_attack_data(train_loader, test_loader)\n",
    "        \n",
    "        # Create balanced dataset\n",
    "        member_mask = y == 1\n",
    "        non_member_mask = y == 0\n",
    "        \n",
    "        member_samples = X[member_mask]\n",
    "        non_member_samples = X[non_member_mask]\n",
    "        \n",
    "        min_samples = min(len(member_samples), len(non_member_samples))\n",
    "        \n",
    "        if len(member_samples) > min_samples:\n",
    "            indices = torch.randperm(len(member_samples))[:min_samples]\n",
    "            member_samples = member_samples[indices]\n",
    "        if len(non_member_samples) > min_samples:\n",
    "            indices = torch.randperm(len(non_member_samples))[:min_samples]\n",
    "            non_member_samples = non_member_samples[indices]\n",
    "        \n",
    "        X_balanced = torch.cat([member_samples, non_member_samples])\n",
    "        y_balanced = torch.cat([torch.ones(min_samples), torch.zeros(min_samples)]).long()\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X_balanced, y_balanced)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(\"\\nTraining attack model...\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for inputs, labels in loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.attack_model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            accuracy = 100. * correct / total\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "def evaluate_attack(attack_model, target_model, data_loader, is_member, device, num_samples=1000):\n",
    "    \"\"\"Evaluate attack model on a subset of data\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    target_model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            if total >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch_size = data.size(0)\n",
    "            if total + batch_size > num_samples:\n",
    "                data = data[:num_samples-total]\n",
    "                batch_size = data.size(0)\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            outputs = target_model(data)\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits, probabilities = outputs\n",
    "            else:\n",
    "                logits = outputs\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            features = torch.cat([logits, probabilities], dim=1)\n",
    "            \n",
    "            outputs = attack_model(features)\n",
    "            predictions = (torch.softmax(outputs, dim=1)[:, 1] > 0.5).long()\n",
    "            \n",
    "            target = torch.full((predictions.size(0),), is_member, device=device).long()\n",
    "            correct += (predictions == target).sum().item()\n",
    "            total += batch_size\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "# Modified attack runner for specific model\n",
    "def run_attack_on_model(model, train_loader, test_loader):\n",
    "    \"\"\"Run the membership inference attack on a specific loaded model\"\"\"\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Initializing attack...\")\n",
    "    attack = MembershipInferenceAttack(model, device)\n",
    "    \n",
    "    # Train the attack model\n",
    "    attack.train_attack(train_loader, test_loader)\n",
    "    \n",
    "    # Evaluate attack performance\n",
    "    print(\"\\nEvaluating attack performance...\")\n",
    "    train_accuracy = evaluate_attack(attack.attack_model, model, train_loader, True, device)\n",
    "    test_accuracy = evaluate_attack(attack.attack_model, model, test_loader, False, device)\n",
    "    \n",
    "    print(f\"\\nAttack Results:\")\n",
    "    print(f\"Accuracy on training samples: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"Accuracy on test samples: {test_accuracy*100:.2f}%\")\n",
    "    print(f\"Overall attack accuracy: {((train_accuracy + test_accuracy)/2)*100:.2f}%\")\n",
    "    \n",
    "    return attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your specific model\n",
    "model_path = os.path.join('model_checkpoints_extended', 'model_epoch_1200.pt')\n",
    "saved_model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(saved_model['model_state_dict'])\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Run the attack\n",
    "attack = run_attack_on_model(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack(attack_model, target_model, data_loader, is_member, device, num_samples=1000):\n",
    "    \"\"\"Evaluate attack model on a subset of data\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    target_model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            if total >= num_samples:\n",
    "                break\n",
    "            \n",
    "            batch_size = data.size(0)\n",
    "            if total + batch_size > num_samples:\n",
    "                data = data[:num_samples-total]\n",
    "                batch_size = data.size(0)\n",
    "            \n",
    "            # Keep the original data format for the target model\n",
    "            data_orig = data.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            # Get model predictions while maintaining original input format\n",
    "            outputs = target_model(data_orig)\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits, probabilities = outputs\n",
    "            else:\n",
    "                logits = outputs\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            # Prepare features for attack model\n",
    "            features = torch.cat([logits, probabilities], dim=1)\n",
    "            \n",
    "            # Get attack model predictions\n",
    "            attack_outputs = attack_model(features)\n",
    "            predictions = (torch.softmax(attack_outputs, dim=1)[:, 1] > 0.5).long()\n",
    "            \n",
    "            target = torch.full((predictions.size(0),), is_member, device=device).long()\n",
    "            correct += (predictions == target).sum().item()\n",
    "            total += batch_size\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def run_extended_attack(model, train_loader, test_loader, epochs=30):\n",
    "    \"\"\"Run membership inference attack with extended epochs and detailed tracking\"\"\"\n",
    "    device = torch.device('cpu')\n",
    "    attack = MembershipInferenceAttack(model, device)\n",
    "    \n",
    "    # Prepare attack data\n",
    "    print(\"Preparing attack data...\")\n",
    "    X, y = attack.prepare_attack_data(train_loader, test_loader)\n",
    "    \n",
    "    # Create balanced dataset\n",
    "    member_mask = y == 1\n",
    "    non_member_mask = y == 0\n",
    "    \n",
    "    member_samples = X[member_mask]\n",
    "    non_member_samples = X[non_member_mask]\n",
    "    min_samples = min(len(member_samples), len(non_member_samples))\n",
    "    \n",
    "    # Balance the dataset\n",
    "    if len(member_samples) > min_samples:\n",
    "        indices = torch.randperm(len(member_samples))[:min_samples]\n",
    "        member_samples = member_samples[indices]\n",
    "    if len(non_member_samples) > min_samples:\n",
    "        indices = torch.randperm(len(non_member_samples))[:min_samples]\n",
    "        non_member_samples = non_member_samples[indices]\n",
    "    \n",
    "    X_balanced = torch.cat([member_samples, non_member_samples])\n",
    "    y_balanced = torch.cat([torch.ones(min_samples), torch.zeros(min_samples)]).long()\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    indices = torch.randperm(len(X_balanced))\n",
    "    split = int(0.8 * len(indices))\n",
    "    train_indices = indices[:split]\n",
    "    val_indices = indices[split:]\n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(\n",
    "        X_balanced[train_indices], \n",
    "        y_balanced[train_indices]\n",
    "    )\n",
    "    val_data = torch.utils.data.TensorDataset(\n",
    "        X_balanced[val_indices], \n",
    "        y_balanced[val_indices]\n",
    "    )\n",
    "    \n",
    "    train_loader_attack = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    val_loader_attack = DataLoader(val_data, batch_size=64)\n",
    "    \n",
    "    # Training with validation\n",
    "    best_val_acc = 0\n",
    "    epochs_without_improvement = 0\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    print(\"\\nTraining attack model with extended epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        attack.attack_model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader_attack:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            attack.optimizer.zero_grad()\n",
    "            outputs = attack.attack_model(inputs)\n",
    "            loss = attack.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            attack.optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        attack.attack_model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader_attack:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = attack.attack_model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1:3d} | Train Loss: {train_loss/len(train_loader_attack):.4f} | '\n",
    "              f'Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        # Stop if no improvement for 5 epochs\n",
    "        if epochs_without_improvement >= 5:\n",
    "            print(f'\\nEarly stopping at epoch {epoch+1} due to no improvement in validation accuracy')\n",
    "            break\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nEvaluating final attack performance...\")\n",
    "    train_accuracy = evaluate_attack(attack.attack_model, model, train_loader, True, device)\n",
    "    test_accuracy = evaluate_attack(attack.attack_model, model, test_loader, False, device)\n",
    "    \n",
    "    print(f\"\\nFinal Attack Results:\")\n",
    "    print(f\"Accuracy on training samples: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"Accuracy on test samples: {test_accuracy*100:.2f}%\")\n",
    "    print(f\"Overall attack accuracy: {((train_accuracy + test_accuracy)/2)*100:.2f}%\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return attack, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AttackModel, self).__init__()\n",
    "        self.input_size = num_classes * 2  # logits + probabilities\n",
    "        \n",
    "        # Deeper architecture with batch normalization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class MembershipInferenceAttack:\n",
    "    def __init__(self, target_model, device='cpu'):\n",
    "        self.target_model = target_model\n",
    "        self.attack_model = AttackModel().to(device)\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.attack_model.parameters(), lr=0.001)\n",
    "    \n",
    "    def prepare_attack_data(self, train_loader, test_loader):\n",
    "        attack_inputs = []\n",
    "        attack_labels = []\n",
    "        \n",
    "        self.target_model.eval()\n",
    "        with torch.no_grad():\n",
    "            print(\"Processing training data...\")\n",
    "            for data, labels in train_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.target_model(data)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits, probabilities = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                features = torch.cat([logits, probabilities], dim=1)\n",
    "                attack_inputs.append(features.cpu())\n",
    "                attack_labels.extend([1] * logits.size(0))\n",
    "            \n",
    "            print(\"\\nProcessing test data...\")\n",
    "            for data, labels in test_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.target_model(data)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    logits, probabilities = outputs\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                \n",
    "                features = torch.cat([logits, probabilities], dim=1)\n",
    "                attack_inputs.append(features.cpu())\n",
    "                attack_labels.extend([0] * logits.size(0))\n",
    "        \n",
    "        attack_inputs = torch.cat(attack_inputs, dim=0)\n",
    "        attack_labels = torch.tensor(attack_labels, dtype=torch.long)\n",
    "        return attack_inputs, attack_labels\n",
    "\n",
    "    def train_attack(self, train_loader, test_loader, epochs=10):\n",
    "        print(\"Preparing attack data...\")\n",
    "        X, y = self.prepare_attack_data(train_loader, test_loader)\n",
    "        \n",
    "        # Create balanced dataset\n",
    "        member_mask = y == 1\n",
    "        non_member_mask = y == 0\n",
    "        \n",
    "        member_samples = X[member_mask]\n",
    "        non_member_samples = X[non_member_mask]\n",
    "        \n",
    "        min_samples = min(len(member_samples), len(non_member_samples))\n",
    "        \n",
    "        if len(member_samples) > min_samples:\n",
    "            indices = torch.randperm(len(member_samples))[:min_samples]\n",
    "            member_samples = member_samples[indices]\n",
    "        if len(non_member_samples) > min_samples:\n",
    "            indices = torch.randperm(len(non_member_samples))[:min_samples]\n",
    "            non_member_samples = non_member_samples[indices]\n",
    "        \n",
    "        X_balanced = torch.cat([member_samples, non_member_samples])\n",
    "        y_balanced = torch.cat([torch.ones(min_samples), torch.zeros(min_samples)]).long()\n",
    "        \n",
    "        # Split into training and validation\n",
    "        indices = torch.randperm(len(X_balanced))\n",
    "        split = int(0.8 * len(indices))\n",
    "        train_indices = indices[:split]\n",
    "        val_indices = indices[split:]\n",
    "        \n",
    "        train_data = torch.utils.data.TensorDataset(\n",
    "            X_balanced[train_indices],\n",
    "            y_balanced[train_indices]\n",
    "        )\n",
    "        val_data = torch.utils.data.TensorDataset(\n",
    "            X_balanced[val_indices],\n",
    "            y_balanced[val_indices]\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=64)\n",
    "        \n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        best_val_acc = 0\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        print(\"\\nTraining attack model with extended epochs...\")\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.attack_model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.attack_model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.attack_model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.attack_model(inputs)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            val_acc = 100. * val_correct / val_total\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch+1:3d} | Train Loss: {train_loss/len(train_loader):.4f} | '\n",
    "                  f'Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            if epochs_without_improvement >= 5:\n",
    "                print(f'\\nEarly stopping at epoch {epoch+1} due to no improvement in validation accuracy')\n",
    "                break\n",
    "        \n",
    "        return train_accs, val_accs\n",
    "\n",
    "def evaluate_attack(attack_model, target_model, data_loader, is_member, device, num_samples=1000):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    total = 0\n",
    "    \n",
    "    target_model.eval()\n",
    "    attack_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            if total >= num_samples:\n",
    "                break\n",
    "            \n",
    "            data = data.to(device)\n",
    "            outputs = target_model(data)\n",
    "            if isinstance(outputs, tuple):\n",
    "                logits, probabilities = outputs\n",
    "            else:\n",
    "                logits = outputs\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            features = torch.cat([logits, probabilities], dim=1)\n",
    "            attack_outputs = attack_model(features)\n",
    "            predictions = (torch.softmax(attack_outputs, dim=1)[:, 1] > 0.5).long()\n",
    "            \n",
    "            # Calculate metrics based on membership status\n",
    "            if is_member:\n",
    "                true_positives += (predictions == 1).sum().item()\n",
    "                false_negatives += (predictions == 0).sum().item()\n",
    "            else:\n",
    "                true_negatives += (predictions == 0).sum().item()\n",
    "                false_positives += (predictions == 1).sum().item()\n",
    "            \n",
    "            total += data.size(0)\n",
    "            if total >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Calculate precision and recall for member identification\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'true_negatives': true_negatives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'accuracy': (true_positives + true_negatives) / total if total > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def run_extended_attack(model, train_loader, test_loader):\n",
    "    device = torch.device('cpu')\n",
    "    attack = MembershipInferenceAttack(model, device)\n",
    "    train_accs, val_accs = attack.train_attack(train_loader, test_loader)\n",
    "    \n",
    "    print(\"\\nEvaluating final attack performance...\")\n",
    "    train_metrics = evaluate_attack(attack.attack_model, model, train_loader, True, device)\n",
    "    test_metrics = evaluate_attack(attack.attack_model, model, test_loader, False, device)\n",
    "    \n",
    "    print(f\"\\nFinal Attack Results:\")\n",
    "    print(f\"Training Set Metrics:\")\n",
    "    print(f\"- Accuracy: {train_metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"- Precision: {train_metrics['precision']*100:.2f}%\")\n",
    "    print(f\"- Recall: {train_metrics['recall']*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTest Set Metrics:\")\n",
    "    print(f\"- Accuracy: {test_metrics['accuracy']*100:.2f}%\")\n",
    "    print(f\"- Precision: {test_metrics['precision']*100:.2f}%\")\n",
    "    print(f\"- Recall: {test_metrics['recall']*100:.2f}%\")\n",
    "    \n",
    "    overall_acc = (train_metrics['accuracy'] + test_metrics['accuracy']) / 2\n",
    "    print(f\"\\nOverall attack accuracy: {overall_acc*100:.2f}%\")\n",
    "    \n",
    "    return attack, train_accs, val_accs, train_metrics['accuracy'], test_metrics['accuracy']\n",
    "\n",
    "def attack_checkpoint(epoch_num, model, train_loader, test_loader):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing model checkpoint from epoch {epoch_num}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    model_path = os.path.join('model_checkpoints_extended', f'model_epoch_{epoch_num}.pt')\n",
    "    saved_model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(saved_model['model_state_dict'])\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    attack, train_accs, val_accs, final_train_acc, final_test_acc = run_extended_attack(\n",
    "        model, train_loader, test_loader)\n",
    "    \n",
    "    return {\n",
    "        'epoch': epoch_num,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'final_train_acc': final_train_acc * 100,  # Convert to percentage\n",
    "        'final_val_acc': final_test_acc * 100  # Convert to percentage\n",
    "    }\n",
    "\n",
    "def analyze_multiple_checkpoints(model, train_loader, test_loader, epochs=[100, 600, 1200, 1500]):\n",
    "    results = []\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        result = attack_checkpoint(epoch, model, train_loader, test_loader)\n",
    "        results.append(result)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Final accuracies for each checkpoint\n",
    "    plt.subplot(2, 1, 1)\n",
    "    epochs = [r['epoch'] for r in results]\n",
    "    train_accs = [r['final_train_acc'] for r in results]\n",
    "    val_accs = [r['final_val_acc'] for r in results]\n",
    "    \n",
    "    plt.plot(epochs, train_accs, 'b-o', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accs, 'r-o', label='Validation Accuracy')\n",
    "    plt.axhline(y=50, color='gray', linestyle='--', label='Random Guess (50%)')\n",
    "    plt.xlabel('Training Epoch of Target Model')\n",
    "    plt.ylabel('Attack Success Rate (%)')\n",
    "    plt.title('Membership Inference Attack Success Rate vs Model Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Attack training curves for each checkpoint\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for result in results:\n",
    "        epoch_num = result['epoch']\n",
    "        epochs_x = range(1, len(result['train_accs']) + 1)\n",
    "        plt.plot(epochs_x, result['train_accs'],\n",
    "                label=f'Train (Epoch {epoch_num})',\n",
    "                linestyle='-', alpha=0.7)\n",
    "        plt.plot(epochs_x, result['val_accs'],\n",
    "                label=f'Val (Epoch {epoch_num})',\n",
    "                linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.axhline(y=50, color='gray', linestyle='--', label='Random Guess (50%)')\n",
    "    plt.xlabel('Attack Model Training Epoch')\n",
    "    plt.ylabel('Attack Accuracy (%)')\n",
    "    plt.title('Attack Model Training Progress for Different Checkpoints')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Checkpoint':^15} | {'Train Acc':^15} | {'Val Acc':^15} | {'Vulnerability':^25}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for result in results:\n",
    "        vulnerability = \"Low\"\n",
    "        acc_diff = result['final_train_acc'] - result['final_val_acc']\n",
    "        if acc_diff > 5 or result['final_train_acc'] > 55:\n",
    "            vulnerability = \"Moderate\"\n",
    "        if acc_diff > 10 or result['final_train_acc'] > 60:\n",
    "            vulnerability = \"High\"\n",
    "        \n",
    "        print(f\"Epoch {result['epoch']:^8} | \"\n",
    "              f\"{result['final_train_acc']:^13.2f}% | \"\n",
    "              f\"{result['final_val_acc']:^13.2f}% | \"\n",
    "              f\"{vulnerability:^25}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_to_test = [120, 600, 1200, 1500]\n",
    "results = analyze_multiple_checkpoints(model, train_loader, test_loader, epochs_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
