{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python --version\n",
    "#!pip install --upgrade pip\n",
    "#!pip uninstall keras tensorflow\n",
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_cifar10, get_class_names\n",
    "\n",
    "# Load data consistently\n",
    "train_loader, test_loader, X_train, X_test, Y_train, Y_test = load_cifar10(batch_size=64, seed=42)\n",
    "class_names = get_class_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pytorch_model(model, filepath, epoch=None):\n",
    "    \"\"\"\n",
    "    Save a PyTorch model to disk, including both architecture and weights.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model (nn.Module)\n",
    "        filepath: Path to save the model\n",
    "        epoch: Optional epoch number to include in the save\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "    \n",
    "    # Prepare the save dictionary\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_args': model.args,  # Saving the model arguments\n",
    "        'epoch': epoch if epoch is not None else None\n",
    "    }\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(save_dict, filepath)\n",
    "    print(f\"Model saved successfully to {filepath}\")\n",
    "\n",
    "def load_pytorch_model(filepath):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model from disk.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded PyTorch model\n",
    "        epoch: Epoch number when the model was saved (if available)\n",
    "    \"\"\"\n",
    "    # Load the save dictionary\n",
    "    save_dict = torch.load(filepath)\n",
    "    \n",
    "    # Create a new model instance with the saved arguments\n",
    "    model = ImageMamba(args=save_dict['model_args'], num_classes=1000)  # Adjust num_classes as needed\n",
    "    \n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(save_dict['model_state_dict'])\n",
    "    \n",
    "    return model, save_dict.get('epoch')\n",
    "\n",
    "# Example usage:\n",
    "# Saving the model\n",
    "# save_pytorch_model(complex_model, '25epoch_complex_model.pt', epoch=25)\n",
    "\n",
    "# Loading the model\n",
    "# loaded_model, epoch = load_pytorch_model('25epoch_complex_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create an instance of ModelArgs\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "\n",
    "# Instantiate the ImageMamba model\n",
    "model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MAMBA on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model + testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 2, more epochs\n",
    "As we can see there is not much difference between the probabilities of true class predictions on the traindataset instances, and the testdataset instances.\n",
    "\n",
    "This might indicate that the dataset was hard easy for 10 epochs, lets add 15 more epochs, to get to a total of 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    " \n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see still no significant diffirence, lets make a stronger model, and train more epochs to achieve a loss close to 0, and then try inference again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 3, complex model\n",
    "To get a much lower loss, we will both increase the number of epochs and make the model more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "d_model = 128\n",
    "n_layer = 8\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create an instance of ModelArgs\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "\n",
    "# Instantiate the ImageMamba model\n",
    "complex_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "complex_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(complex_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25 \n",
    "for epoch in range(num_epochs):\n",
    "    complex_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = complex_model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after 25 epochs\n",
    "save_pytorch_model(complex_model, '../trained_models/25epoch_complex_model.pt', epoch=25)\n",
    "\n",
    "# Later, to load the model:\n",
    "# loaded_model, epoch = load_pytorch_model('25epoch_complex_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "complex_model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = complex_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "complex_model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = complex_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference in percentage is: \", (average_prob_correct_class_train - average_prob_correct_class_test)/average_prob_correct_class_train*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the model started to overfit which is why the loss stopped decreasing, now we will try a different optimiser and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer with AdamW\n",
    "optimizer = optim.AdamW(complex_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Cosine Annealing Scheduler\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# Training Loop with Scheduler\n",
    "num_epochs = 50  # More epochs to ensure convergence\n",
    "for epoch in range(num_epochs):\n",
    "    complex_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = complex_model(inputs)\n",
    "\n",
    "        # Flatten labels if necessary\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Step the scheduler at each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after 25 epochs\n",
    "save_pytorch_model(complex_model, '../trained_models/different_50epoch_complex_model.pt', epoch=25)\n",
    "\n",
    "# Later, to load the model:\n",
    "# loaded_model, epoch = load_pytorch_model('25epoch_complex_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "complex_model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = complex_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "complex_model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = complex_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference in percentage is: \", (average_prob_correct_class_train - average_prob_correct_class_test)/average_prob_correct_class_train*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "class TrainingMetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.test_accuracies = []\n",
    "        self.train_confidences = []\n",
    "        self.test_confidences = []\n",
    "        self.confidence_gaps = []\n",
    "        self.epoch_train_confidences = []  # Store all confidence values for each epoch\n",
    "        self.epoch_test_confidences = []\n",
    "\n",
    "    def update(self, train_loss, test_loss, train_acc, test_acc, \n",
    "              train_conf, test_conf, train_epoch_confs, test_epoch_confs):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.test_losses.append(test_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.test_accuracies.append(test_acc)\n",
    "        self.train_confidences.append(train_conf)\n",
    "        self.test_confidences.append(test_conf)\n",
    "        self.confidence_gaps.append(train_conf - test_conf)\n",
    "        self.epoch_train_confidences.append(train_epoch_confs)\n",
    "        self.epoch_test_confidences.append(test_epoch_confs)\n",
    "\n",
    "    def plot_metrics(self, save_path=None):\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        # Create a figure with subplots\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot Loss\n",
    "        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        ax1.plot(epochs, self.test_losses, 'r-', label='Test Loss')\n",
    "        ax1.set_title('Loss over Epochs')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Plot Accuracy\n",
    "        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')\n",
    "        ax2.plot(epochs, self.test_accuracies, 'r-', label='Test Accuracy')\n",
    "        ax2.set_title('Accuracy over Epochs')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        # Plot Confidence\n",
    "        ax3.plot(epochs, self.train_confidences, 'b-', label='Training Confidence')\n",
    "        ax3.plot(epochs, self.test_confidences, 'r-', label='Test Confidence')\n",
    "        ax3.set_title('Average Confidence over Epochs')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Confidence')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "\n",
    "        # Plot Confidence Gap\n",
    "        ax4.plot(epochs, self.confidence_gaps, 'g-', label='Confidence Gap')\n",
    "        ax4.set_title('Confidence Gap over Epochs')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Gap (Train - Test)')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confidence_distributions(self, epoch):\n",
    "        \"\"\"Plot confidence distributions for a specific epoch\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create density plots\n",
    "        sns.kdeplot(data=self.epoch_train_confidences[epoch], label='Training', color='blue')\n",
    "        sns.kdeplot(data=self.epoch_test_confidences[epoch], label='Test', color='red')\n",
    "        \n",
    "        plt.title(f'Confidence Distributions (Epoch {epoch+1})')\n",
    "        plt.xlabel('Confidence')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def statistical_tests(self, epoch=-1):\n",
    "        \"\"\"Perform statistical tests on confidence distributions\"\"\"\n",
    "        if epoch == -1:\n",
    "            epoch = len(self.epoch_train_confidences) - 1\n",
    "\n",
    "        train_conf = self.epoch_train_confidences[epoch]\n",
    "        test_conf = self.epoch_test_confidences[epoch]\n",
    "\n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_stat, ks_pval = stats.ks_2samp(train_conf, test_conf)\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        mw_stat, mw_pval = stats.mannwhitneyu(train_conf, test_conf, alternative='two-sided')\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        cohens_d = (np.mean(train_conf) - np.mean(test_conf)) / np.sqrt(\n",
    "            (np.var(train_conf) + np.var(test_conf)) / 2)\n",
    "\n",
    "        return {\n",
    "            'ks_test': {'statistic': ks_stat, 'p_value': ks_pval},\n",
    "            'mw_test': {'statistic': mw_stat, 'p_value': mw_pval},\n",
    "            'cohens_d': cohens_d\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In both the training loop and evaluation function, we need to detach tensors before converting to numpy\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    confidence_sum = 0\n",
    "    all_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            \n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "            confidence_sum += confidence.sum().item()\n",
    "            all_confidences.extend(confidence.detach().cpu().numpy())  # Fixed here\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_confidence = confidence_sum / total\n",
    "    \n",
    "    return accuracy, avg_loss, avg_confidence, all_confidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "    train_confidence_sum = 0\n",
    "    train_epoch_confidences = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        logits, probabilities = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        confidence, _ = torch.max(probabilities, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        train_confidence_sum += confidence.sum().item()\n",
    "        train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = (running_correct / total_samples) * 100\n",
    "    train_avg_confidence = train_confidence_sum / total_samples\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy, test_loss, test_avg_confidence, test_epoch_confidences = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(\n",
    "        train_loss, test_loss,\n",
    "        train_accuracy, test_accuracy,\n",
    "        train_avg_confidence, test_avg_confidence,\n",
    "        train_epoch_confidences, test_epoch_confidences\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    print(f'  Training:')\n",
    "    print(f'    Loss: {train_loss:.4f}')\n",
    "    print(f'    Accuracy: {train_accuracy:.2f}%')\n",
    "    print(f'    Average Confidence: {train_avg_confidence:.4f}')\n",
    "    print(f'  Testing:')\n",
    "    print(f'    Loss: {test_loss:.4f}')\n",
    "    print(f'    Accuracy: {test_accuracy:.2f}%')\n",
    "    print(f'    Average Confidence: {test_avg_confidence:.4f}')\n",
    "    \n",
    "    # Save model every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
    "        save_pytorch_model(model, checkpoint_path, epoch=epoch+1)\n",
    "        print(f'Saved model checkpoint at epoch {epoch+1}')\n",
    "    \n",
    "    # Perform statistical tests every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"\\nStatistical Tests:\")\n",
    "        stats_results = metrics_tracker.statistical_tests(epoch)\n",
    "        print(f\"  Kolmogorov-Smirnov test:\")\n",
    "        print(f\"    Statistic: {stats_results['ks_test']['statistic']:.4f}\")\n",
    "        print(f\"    p-value: {stats_results['ks_test']['p_value']:.4f}\")\n",
    "        print(f\"  Mann-Whitney U test:\")\n",
    "        print(f\"    Statistic: {stats_results['mw_test']['statistic']:.4f}\")\n",
    "        print(f\"    p-value: {stats_results['mw_test']['p_value']:.4f}\")\n",
    "        print(f\"  Cohen's d: {stats_results['cohens_d']:.4f}\")\n",
    "        \n",
    "        # Plot confidence distributions\n",
    "        metrics_tracker.plot_confidence_distributions(epoch)\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "# After training, plot all metrics\n",
    "metrics_tracker.plot_metrics(save_path='training_metrics.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the saved model from epoch 100\n",
    "checkpoint_path = os.path.join('model_checkpoints', 'model_epoch_100.pt')\n",
    "print(f\"Loading model from {checkpoint_path}\")\n",
    "saved_model = torch.load(checkpoint_path)\n",
    "model.load_state_dict(saved_model['model_state_dict'])\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics_tracker = TrainingMetricsTracker()\n",
    "\n",
    "# Create directories for model checkpoints and plots\n",
    "checkpoint_dir = 'model_checkpoints_extended'\n",
    "plots_dir = 'training_plots'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Training loop - start from epoch 101 to 400\n",
    "start_epoch = 101\n",
    "num_epochs = 400\n",
    "last_plot_epoch = start_epoch - 1\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "    train_confidence_sum = 0\n",
    "    train_epoch_confidences = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        logits, probabilities = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        confidence, _ = torch.max(probabilities, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        train_confidence_sum += confidence.sum().item()\n",
    "        train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = (running_correct / total_samples) * 100\n",
    "    train_avg_confidence = train_confidence_sum / total_samples\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy, test_loss, test_avg_confidence, test_epoch_confidences = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(\n",
    "        train_loss, test_loss,\n",
    "        train_accuracy, test_accuracy,\n",
    "        train_avg_confidence, test_avg_confidence,\n",
    "        train_epoch_confidences, test_epoch_confidences\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "    print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "    print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "    \n",
    "    # Save model every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt')\n",
    "        save_pytorch_model(model, checkpoint_path, epoch=epoch)\n",
    "        print(f'Saved checkpoint: epoch {epoch}')\n",
    "    \n",
    "    # Create summary plots every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        plot_filename = os.path.join(plots_dir, f'training_metrics_epoch_{epoch}.png')\n",
    "        metrics_tracker.plot_metrics(save_path=plot_filename)\n",
    "        print(f'Saved plots for epochs {last_plot_epoch+1}-{epoch}')\n",
    "        \n",
    "        if epoch < num_epochs:\n",
    "            last_plot_epoch = epoch\n",
    "            metrics_tracker = TrainingMetricsTracker()\n",
    "    \n",
    "    # Perform statistical tests every 5 epochs, but only after we have enough data\n",
    "    if epoch % 5 == 0 and (epoch - last_plot_epoch) > 0:\n",
    "        try:\n",
    "            stats_results = metrics_tracker.statistical_tests(epoch - last_plot_epoch - 1)  # Use -1 to get last complete epoch\n",
    "            print(f\"Stats: KS={stats_results['ks_test']['statistic']:.4f}(p={stats_results['ks_test']['p_value']:.4f}), \" \n",
    "                  f\"MW={stats_results['mw_test']['statistic']:.4f}(p={stats_results['mw_test']['p_value']:.4f}), \"\n",
    "                  f\"Cohen's d={stats_results['cohens_d']:.4f}\")\n",
    "            \n",
    "            metrics_tracker.plot_confidence_distributions(epoch - last_plot_epoch - 1)\n",
    "        except IndexError:\n",
    "            print(\"Skipping statistical tests - not enough data yet\")\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "# Final summary plot\n",
    "final_plot_filename = os.path.join(plots_dir, 'training_metrics_final.png')\n",
    "metrics_tracker.plot_metrics(save_path=final_plot_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test inference from loaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_saved_model(model_path, test_loader, train_loader):\n",
    "    global model  # Use the globally defined model\n",
    "    saved_model = torch.load(model_path, map_location=torch.device('cpu')) # Only if on machine without GPU\n",
    "    model.load_state_dict(saved_model['model_state_dict'])\n",
    "    model = model.to('cpu')\n",
    "    device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    metrics = {}\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0\n",
    "    test_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            test_loss += loss.item()\n",
    "            test_confidences.extend(confidence.detach().numpy())\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    train_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            train_loss += loss.item()\n",
    "            train_confidences.extend(confidence.detach().numpy())\n",
    "    \n",
    "    metrics['test_accuracy'] = 100 * test_correct / test_total\n",
    "    metrics['test_loss'] = test_loss / len(test_loader)\n",
    "    metrics['test_confidence'] = np.mean(test_confidences)\n",
    "    metrics['train_accuracy'] = 100 * train_correct / train_total\n",
    "    metrics['train_loss'] = train_loss / len(train_loader)\n",
    "    metrics['train_confidence'] = np.mean(train_confidences)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Collect epochs and metrics\n",
    "epochs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_confidences = []\n",
    "test_confidences = []\n",
    "\n",
    "# Load and evaluate checkpoints from both directories\n",
    "for directory in ['model_checkpoints', 'model_checkpoints_extended']:\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.startswith('model_epoch_'):\n",
    "            epoch = int(filename.split('_')[-1].split('.')[0])\n",
    "            print(f\"Evaluating epoch {epoch}...\")\n",
    "            model_path = os.path.join(directory, filename)\n",
    "            metrics = evaluate_saved_model(model_path, test_loader, train_loader)\n",
    "            # Rest of the code remains same\n",
    "\n",
    "            epochs.append(epoch)\n",
    "            train_losses.append(metrics['train_loss'])\n",
    "            test_losses.append(metrics['test_loss'])\n",
    "            train_accuracies.append(metrics['train_accuracy'])\n",
    "            test_accuracies.append(metrics['test_accuracy'])\n",
    "            train_confidences.append(metrics['train_confidence'])\n",
    "            test_confidences.append(metrics['test_confidence'])\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, test_accuracies, 'r-', label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_confidences, 'b-', label='Training Confidence')\n",
    "plt.plot(epochs, test_confidences, 'r-', label='Test Confidence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Average Confidence over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "confidence_gap = np.array(train_confidences) - np.array(test_confidences)\n",
    "plt.plot(epochs, confidence_gap, 'g-', label='Confidence Gap')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gap (Train - Test)')\n",
    "plt.title('Confidence Gap over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('complete_training_history.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference in percentage is: \", (average_prob_correct_class_train - average_prob_correct_class_test)/average_prob_correct_class_train*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training from epoch 900 to 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the saved model from epoch 100\n",
    "checkpoint_path = os.path.join('model_checkpoints_extended', 'model_epoch_900.pt')\n",
    "print(f\"Loading model from {checkpoint_path}\")\n",
    "saved_model = torch.load(checkpoint_path)\n",
    "model.load_state_dict(saved_model['model_state_dict'])\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics_tracker = TrainingMetricsTracker()\n",
    "\n",
    "# Create directories for model checkpoints and plots\n",
    "checkpoint_dir = 'model_checkpoints_extended'\n",
    "plots_dir = 'training_plots'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Training loop - start from epoch 101 to 400\n",
    "start_epoch = 901\n",
    "num_epochs = 1500\n",
    "last_plot_epoch = start_epoch - 1\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total_samples = 0\n",
    "    train_confidence_sum = 0\n",
    "    train_epoch_confidences = []\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        logits, probabilities = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        confidence, _ = torch.max(probabilities, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        train_confidence_sum += confidence.sum().item()\n",
    "        train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = (running_correct / total_samples) * 100\n",
    "    train_avg_confidence = train_confidence_sum / total_samples\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy, test_loss, test_avg_confidence, test_epoch_confidences = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(\n",
    "        train_loss, test_loss,\n",
    "        train_accuracy, test_accuracy,\n",
    "        train_avg_confidence, test_avg_confidence,\n",
    "        train_epoch_confidences, test_epoch_confidences\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "    print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "    print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "    \n",
    "    # Create summary plots every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt')\n",
    "        save_pytorch_model(model, checkpoint_path, epoch=epoch)\n",
    "        print(f'Saved checkpoint: epoch {epoch}')\n",
    "        plot_filename = os.path.join(plots_dir, f'training_metrics_epoch_{epoch}.png')\n",
    "        metrics_tracker.plot_metrics(save_path=plot_filename)\n",
    "        print(f'Saved plots for epochs {last_plot_epoch+1}-{epoch}')\n",
    "        \n",
    "        if epoch < num_epochs:\n",
    "            last_plot_epoch = epoch\n",
    "            metrics_tracker = TrainingMetricsTracker()\n",
    "    \n",
    "    # Perform statistical tests every 50 epochs, but only after we have enough data\n",
    "    if epoch % 50 == 0 and (epoch - last_plot_epoch) > 0:\n",
    "        try:\n",
    "            stats_results = metrics_tracker.statistical_tests(epoch - last_plot_epoch - 1)  # Use -1 to get last complete epoch\n",
    "            print(f\"Stats: KS={stats_results['ks_test']['statistic']:.4f}(p={stats_results['ks_test']['p_value']:.4f}), \" \n",
    "                  f\"MW={stats_results['mw_test']['statistic']:.4f}(p={stats_results['mw_test']['p_value']:.4f}), \"\n",
    "                  f\"Cohen's d={stats_results['cohens_d']:.4f}\")\n",
    "            \n",
    "            metrics_tracker.plot_confidence_distributions(epoch - last_plot_epoch - 1)\n",
    "        except IndexError:\n",
    "            print(\"Skipping statistical tests - not enough data yet\")\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "# Final summary plot\n",
    "final_plot_filename = os.path.join(plots_dir, f'training_metrics_final_{num_epochs}.png')\n",
    "metrics_tracker.plot_metrics(save_path=final_plot_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1500 epooch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_saved_model(model_path, test_loader, train_loader):\n",
    "    global model  # Use the globally defined model\n",
    "    saved_model = torch.load(model_path, map_location=torch.device('cpu')) # Only if on machine without GPU\n",
    "    model.load_state_dict(saved_model['model_state_dict'])\n",
    "    model = model.to('cpu')\n",
    "    device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    metrics = {}\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0\n",
    "    test_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "            test_loss += loss.item()\n",
    "            test_confidences.extend(confidence.detach().numpy())\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    train_confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            train_loss += loss.item()\n",
    "            train_confidences.extend(confidence.detach().numpy())\n",
    "    \n",
    "    metrics['test_accuracy'] = 100 * test_correct / test_total\n",
    "    metrics['test_loss'] = test_loss / len(test_loader)\n",
    "    metrics['test_confidence'] = np.mean(test_confidences)\n",
    "    metrics['train_accuracy'] = 100 * train_correct / train_total\n",
    "    metrics['train_loss'] = train_loss / len(train_loader)\n",
    "    metrics['train_confidence'] = np.mean(train_confidences)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Collect epochs and metrics\n",
    "epochs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_confidences = []\n",
    "test_confidences = []\n",
    "import os\n",
    "from natsort import natsorted\n",
    "\n",
    "for directory in ['model_checkpoints', 'model_checkpoints_extended']:\n",
    "    for filename in natsorted(os.listdir(directory)):\n",
    "        if filename.startswith('model_epoch_'):\n",
    "            epoch = int(filename.split('_')[-1].split('.')[0])\n",
    "            print(f\"Evaluating epoch {epoch}...\")\n",
    "            model_path = os.path.join(directory, filename)\n",
    "            metrics = evaluate_saved_model(model_path, test_loader, train_loader)\n",
    "            # Rest of the code remains same\n",
    "\n",
    "            epochs.append(epoch)\n",
    "            train_losses.append(metrics['train_loss'])\n",
    "            test_losses.append(metrics['test_loss'])\n",
    "            train_accuracies.append(metrics['train_accuracy'])\n",
    "            test_accuracies.append(metrics['test_accuracy'])\n",
    "            train_confidences.append(metrics['train_confidence'])\n",
    "            test_confidences.append(metrics['test_confidence'])\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, test_accuracies, 'r-', label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_confidences, 'b-', label='Training Confidence')\n",
    "plt.plot(epochs, test_confidences, 'r-', label='Test Confidence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Average Confidence over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "confidence_gap = np.array(train_confidences) - np.array(test_confidences)\n",
    "plt.plot(epochs, confidence_gap, 'g-', label='Confidence Gap')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gap (Train - Test)')\n",
    "plt.title('Confidence Gap over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('complete_training_history.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New with JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_mamba(model, train_loader, test_loader, num_epochs=400, device='cuda', \n",
    "                         checkpoint_dir='model_checkpoints_extended'):\n",
    "    \"\"\"Train and evaluate MAMBA with checkpointing - exact same format as CNN\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    \n",
    "    # Use the same dictionary format as CNN\n",
    "    metrics = {\n",
    "        'train_losses': [], 'test_losses': [],\n",
    "        'train_accuracies': [], 'test_accuracies': [],\n",
    "        'train_confidences': [], 'test_confidences': [],\n",
    "        'epoch_train_confidences': [], 'epoch_test_confidences': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        train_confidence_sum = 0\n",
    "        train_epoch_confidences = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            train_confidence_sum += confidence.sum().item()\n",
    "            train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = (running_correct / total_samples) * 100\n",
    "        train_avg_confidence = train_confidence_sum / total_samples\n",
    "        \n",
    "        # Testing phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_confidence_sum = 0\n",
    "        test_epoch_confidences = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                labels = labels.view(-1)\n",
    "                logits, probabilities = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                confidence, _ = torch.max(probabilities, 1)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_confidence_sum += confidence.sum().item()\n",
    "                test_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = (test_correct / test_total) * 100\n",
    "        test_avg_confidence = test_confidence_sum / test_total\n",
    "        \n",
    "        # Store metrics exactly like CNN\n",
    "        metrics['train_losses'].append(train_loss)\n",
    "        metrics['test_losses'].append(test_loss)\n",
    "        metrics['train_accuracies'].append(train_accuracy)\n",
    "        metrics['test_accuracies'].append(test_accuracy)\n",
    "        metrics['train_confidences'].append(train_avg_confidence)\n",
    "        metrics['test_confidences'].append(test_avg_confidence)\n",
    "        metrics['epoch_train_confidences'].append(train_epoch_confidences)\n",
    "        metrics['epoch_test_confidences'].append(test_epoch_confidences)\n",
    "        \n",
    "        # Save metrics to JSON exactly like CNN\n",
    "        metrics_path = os.path.join(checkpoint_dir, 'training_metrics.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json_metrics = {\n",
    "                'train_losses': [round(float(x), 4) for x in metrics['train_losses']],\n",
    "                'test_losses': [round(float(x), 4) for x in metrics['test_losses']],\n",
    "                'train_accuracies': [round(float(x), 4) for x in metrics['train_accuracies']],\n",
    "                'test_accuracies': [round(float(x), 4) for x in metrics['test_accuracies']],\n",
    "                'train_confidences': [round(float(x), 4) for x in metrics['train_confidences']],\n",
    "                'test_confidences': [round(float(x), 4) for x in metrics['test_confidences']],\n",
    "                'epoch_train_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) \n",
    "                                          else round(float(arr), 4) for arr in metrics['epoch_train_confidences']],\n",
    "                'epoch_test_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) \n",
    "                                         else round(float(arr), 4) for arr in metrics['epoch_test_confidences']],\n",
    "                'current_epoch': epoch + 1\n",
    "            }\n",
    "            json.dump(json_metrics, f, indent=4)\n",
    "        \n",
    "        # Print progress every 10 epochs (exactly like CNN)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "            print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "            print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "        \n",
    "        # Save checkpoint every 50 epochs (exactly like CNN)\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'mamba_model_epoch_{epoch+1}.pt')\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create model args and initialize model\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)\n",
    "model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device and move model to it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the model\n",
    "metrics = train_evaluate_mamba(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=400,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
