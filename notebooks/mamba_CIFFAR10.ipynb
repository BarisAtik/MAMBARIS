{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset\n",
    "Find new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 1)) (0.0.7)\n",
      "Requirement already satisfied: transformers==4.30.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 2)) (4.30.0)\n",
      "Requirement already satisfied: einops==0.6.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: scikit-learn==1.4.1.post1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 4)) (1.4.1.post1)\n",
      "Requirement already satisfied: pandas==2.2.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: torch in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 6)) (2.1.2)\n",
      "Requirement already satisfied: numpy in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 7)) (1.24.3)\n",
      "Requirement already satisfied: keras==2.13.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 8)) (2.13.1)\n",
      "Requirement already satisfied: tensorflow in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from -r ../requirements.txt (line 9)) (2.13.0)\n",
      "Requirement already satisfied: filelock in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from transformers==4.30.0->-r ../requirements.txt (line 2)) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from scikit-learn==1.4.1.post1->-r ../requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from scikit-learn==1.4.1.post1->-r ../requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from scikit-learn==1.4.1.post1->-r ../requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from pandas==2.2.1->-r ../requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from pandas==2.2.1->-r ../requirements.txt (line 5)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from pandas==2.2.1->-r ../requirements.txt (line 5)) (2024.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from ucimlrepo->-r ../requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 6)) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 6)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from torch->-r ../requirements.txt (line 6)) (2024.9.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.13.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow->-r ../requirements.txt (line 9)) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (4.25.5)\n",
      "Requirement already satisfied: setuptools in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (2.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from jinja2->torch->-r ../requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from requests->transformers==4.30.0->-r ../requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from requests->transformers==4.30.0->-r ../requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from requests->transformers==4.30.0->-r ../requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from sympy->torch->-r ../requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (2.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow->-r ../requirements.txt (line 9)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model #save and load models\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (60000, 32, 32, 3)\n",
      "y shape: (60000, 1)\n",
      "y1 shape (60000, 10)\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# The data, split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "#combining all the available data to use it in a way we want\n",
    "x = np.vstack((X_train,X_test))\n",
    "y = np.vstack((Y_train,Y_test))\n",
    "print('x shape:', x.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "x = x.astype('float32')\n",
    "x = x/255\n",
    "num_classes = 10\n",
    "y1 = keras.utils.to_categorical(y, num_classes)\n",
    "print('y1 shape', y1.shape)\n",
    "print('Number of classes:', y1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The datta will be preprocessed, and converted into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of training data tensor:\n",
      "tensor([[[[ 59,  62,  63],\n",
      "          [ 43,  46,  45],\n",
      "          [ 50,  48,  43],\n",
      "          ...,\n",
      "          [158, 132, 108],\n",
      "          [152, 125, 102],\n",
      "          [148, 124, 103]],\n",
      "\n",
      "         [[ 16,  20,  20],\n",
      "          [  0,   0,   0],\n",
      "          [ 18,   8,   0],\n",
      "          ...,\n",
      "          [123,  88,  55],\n",
      "          [119,  83,  50],\n",
      "          [122,  87,  57]],\n",
      "\n",
      "         [[ 25,  24,  21],\n",
      "          [ 16,   7,   0],\n",
      "          [ 49,  27,   8],\n",
      "          ...,\n",
      "          [118,  84,  50],\n",
      "          [120,  84,  50],\n",
      "          [109,  73,  42]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[208, 170,  96],\n",
      "          [201, 153,  34],\n",
      "          [198, 161,  26],\n",
      "          ...,\n",
      "          [160, 133,  70],\n",
      "          [ 56,  31,   7],\n",
      "          [ 53,  34,  20]],\n",
      "\n",
      "         [[180, 139,  96],\n",
      "          [173, 123,  42],\n",
      "          [186, 144,  30],\n",
      "          ...,\n",
      "          [184, 148,  94],\n",
      "          [ 97,  62,  34],\n",
      "          [ 83,  53,  34]],\n",
      "\n",
      "         [[177, 144, 116],\n",
      "          [168, 129,  94],\n",
      "          [179, 142,  87],\n",
      "          ...,\n",
      "          [216, 184, 140],\n",
      "          [151, 118,  84],\n",
      "          [123,  92,  72]]],\n",
      "\n",
      "\n",
      "        [[[154, 177, 187],\n",
      "          [126, 137, 136],\n",
      "          [105, 104,  95],\n",
      "          ...,\n",
      "          [ 91,  95,  71],\n",
      "          [ 87,  90,  71],\n",
      "          [ 79,  81,  70]],\n",
      "\n",
      "         [[140, 160, 169],\n",
      "          [145, 153, 154],\n",
      "          [125, 125, 118],\n",
      "          ...,\n",
      "          [ 96,  99,  78],\n",
      "          [ 77,  80,  62],\n",
      "          [ 71,  73,  61]],\n",
      "\n",
      "         [[140, 155, 164],\n",
      "          [139, 146, 149],\n",
      "          [115, 115, 112],\n",
      "          ...,\n",
      "          [ 79,  82,  64],\n",
      "          [ 68,  70,  55],\n",
      "          [ 67,  69,  55]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[175, 167, 166],\n",
      "          [156, 154, 160],\n",
      "          [154, 160, 170],\n",
      "          ...,\n",
      "          [ 42,  34,  36],\n",
      "          [ 61,  53,  57],\n",
      "          [ 93,  83,  91]],\n",
      "\n",
      "         [[165, 154, 128],\n",
      "          [156, 152, 130],\n",
      "          [159, 161, 142],\n",
      "          ...,\n",
      "          [103,  93,  96],\n",
      "          [123, 114, 120],\n",
      "          [131, 121, 131]],\n",
      "\n",
      "         [[163, 148, 120],\n",
      "          [158, 148, 122],\n",
      "          [163, 156, 133],\n",
      "          ...,\n",
      "          [143, 133, 139],\n",
      "          [143, 134, 142],\n",
      "          [143, 133, 144]]]])\n",
      "\n",
      "Sample of training target tensor:\n",
      "tensor([[6],\n",
      "        [9]])\n",
      "\n",
      "Sample of testing data tensor:\n",
      "tensor([[[[158, 112,  49],\n",
      "          [159, 111,  47],\n",
      "          [165, 116,  51],\n",
      "          ...,\n",
      "          [137,  95,  36],\n",
      "          [126,  91,  36],\n",
      "          [116,  85,  33]],\n",
      "\n",
      "         [[152, 112,  51],\n",
      "          [151, 110,  40],\n",
      "          [159, 114,  45],\n",
      "          ...,\n",
      "          [136,  95,  31],\n",
      "          [125,  91,  32],\n",
      "          [119,  88,  34]],\n",
      "\n",
      "         [[151, 110,  47],\n",
      "          [151, 109,  33],\n",
      "          [158, 111,  36],\n",
      "          ...,\n",
      "          [139,  98,  34],\n",
      "          [130,  95,  34],\n",
      "          [120,  89,  33]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 68, 124, 177],\n",
      "          [ 42, 100, 148],\n",
      "          [ 31,  88, 137],\n",
      "          ...,\n",
      "          [ 38,  97, 146],\n",
      "          [ 13,  64, 108],\n",
      "          [ 40,  85, 127]],\n",
      "\n",
      "         [[ 61, 116, 168],\n",
      "          [ 49, 102, 148],\n",
      "          [ 35,  85, 132],\n",
      "          ...,\n",
      "          [ 26,  82, 130],\n",
      "          [ 29,  82, 126],\n",
      "          [ 20,  64, 107]],\n",
      "\n",
      "         [[ 54, 107, 160],\n",
      "          [ 56, 105, 149],\n",
      "          [ 45,  89, 132],\n",
      "          ...,\n",
      "          [ 24,  77, 124],\n",
      "          [ 34,  84, 129],\n",
      "          [ 21,  67, 110]]],\n",
      "\n",
      "\n",
      "        [[[235, 235, 235],\n",
      "          [231, 231, 231],\n",
      "          [232, 232, 232],\n",
      "          ...,\n",
      "          [233, 233, 233],\n",
      "          [233, 233, 233],\n",
      "          [232, 232, 232]],\n",
      "\n",
      "         [[238, 238, 238],\n",
      "          [235, 235, 235],\n",
      "          [235, 235, 235],\n",
      "          ...,\n",
      "          [236, 236, 236],\n",
      "          [236, 236, 236],\n",
      "          [235, 235, 235]],\n",
      "\n",
      "         [[237, 237, 237],\n",
      "          [234, 234, 234],\n",
      "          [234, 234, 234],\n",
      "          ...,\n",
      "          [235, 235, 235],\n",
      "          [235, 235, 235],\n",
      "          [234, 234, 234]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 87,  99,  89],\n",
      "          [ 43,  51,  37],\n",
      "          [ 19,  23,  11],\n",
      "          ...,\n",
      "          [169, 184, 179],\n",
      "          [182, 197, 193],\n",
      "          [188, 202, 201]],\n",
      "\n",
      "         [[ 82,  96,  82],\n",
      "          [ 46,  57,  36],\n",
      "          [ 36,  44,  22],\n",
      "          ...,\n",
      "          [174, 189, 183],\n",
      "          [185, 200, 196],\n",
      "          [187, 202, 200]],\n",
      "\n",
      "         [[ 85, 101,  83],\n",
      "          [ 62,  75,  48],\n",
      "          [ 58,  67,  38],\n",
      "          ...,\n",
      "          [168, 183, 178],\n",
      "          [180, 195, 191],\n",
      "          [186, 200, 199]]]])\n",
      "\n",
      "Sample of testing target tensor:\n",
      "tensor([[3],\n",
      "        [8]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the train/test data into PyTorch tensors\n",
    "# We must do this because PyTorch models only accept tensors as input\n",
    "# Both the MambaClassifier and Mamba classes inherit from torch.nn.Module\n",
    "# which is the base class for all neural network modules in PyTorch.\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Lets see how these tensors look like\n",
    "print(\"\\nSample of training data tensor:\")\n",
    "print(X_train_tensor[0:2])  # Display a sample of the training data tensor\n",
    "print(\"\\nSample of training target tensor:\")\n",
    "print(Y_train_tensor[0:2])  # Display a sample of the training target tensor\n",
    "print(\"\\nSample of testing data tensor:\")\n",
    "print(X_test_tensor[0:2])  # Display a sample of the testing data tensor\n",
    "print(\"\\nSample of testing target tensor:\")\n",
    "print(Y_test_tensor[0:2])  # Display a sample of the testing target tensor\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Union' from 'dataclasses' (/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/dataclasses.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageMamba, ModelArgs\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# I also want another mamba model which was not pretrained\u001b[39;00m\n\u001b[1;32m      3\u001b[0m d_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "File \u001b[0;32m~/MAMBARIS/MAMBARIS/notebooks/model.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass, Union\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelArgs\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Union' from 'dataclasses' (/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/dataclasses.py)"
     ]
    }
   ],
   "source": [
    "from model import ImageMamba, ModelArgs\n",
    "# I also want another mamba model which was not pretrained\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "vocabsize = np.max(x) + 1  # Assuming x contains integer values representing indices.\n",
    "# or might it be vocabsize = X.apply(lambda col: col.nunique()).max()\n",
    "# Reshape the CIFAR-10 dataset for PyTorch models\n",
    "X_train = X_train.transpose(0, 3, 1, 2)  # Shape (batch_size, channels, height, width)\n",
    "X_test = X_test.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Normalize the pixel values\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "Y_train = torch.tensor(Y_train)\n",
    "Y_test = torch.tensor(Y_test)\n",
    "\n",
    "# Instantiate model and test forward pass\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10\n",
    "\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Test model forward pass\n",
    "logits, probs = model(X_train[:32])  # For a batch of 32 samples\n",
    "print(logits.shape, probs.shape)  # Should be (32, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Unique values in input: {torch.unique(inputs)}\")\n",
    "        print(f\"Vocabulary size: {vocabsize}\")\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size * seq_length]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MAMBA on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size * seq_length]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model + testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = [prob * 100 for prob in y_prob]\n",
    "\n",
    "# Map the predicted class indices to their corresponding class names\n",
    "class_names = unique_targets  # Use the unique target classes from the dataset\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = [prob * 100 for prob in y_prob_train]\n",
    "\n",
    "# Map the predicted class indices to their corresponding class names\n",
    "class_names = unique_targets  # Use the unique target classes from the dataset\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 2, simpler model\n",
    "As we can see there is not much difference between the probabilities of true class predictions on the traindataset instances, and the testdataset instances.\n",
    "\n",
    "This might indicate that the dataset was too easy for this model. Below i will try the same on a simpler version of the same mamba model e.g. less layers and lower dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also want another mamba model which was not pretrained\n",
    "d_model = 16\n",
    "n_layer = 3\n",
    "vocabsize = len(X.nunique())\n",
    "# or might it be vocabsize = X.apply(lambda col: col.nunique()).max()\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=vocabsize)\n",
    "num_classes = len(nursery.data.targets['class'].unique())\n",
    "smaller_model = Mamba(model_args, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(smaller_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "smaller_model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    smaller_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = smaller_model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size * seq_length]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "smaller_model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = smaller_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = [prob * 100 for prob in y_prob]\n",
    "\n",
    "# Map the predicted class indices to their corresponding class names\n",
    "class_names = unique_targets  # Use the unique target classes from the dataset\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode\n",
    "smaller_model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = smaller_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = [prob * 100 for prob in y_prob_train]\n",
    "\n",
    "# Map the predicted class indices to their corresponding class names\n",
    "class_names = unique_targets  # Use the unique target classes from the dataset\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see even on a smaller model there is no siginificant difference in the probability the model outputs for a correct class.\n",
    "Therefore I will move on to different datasets / problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
