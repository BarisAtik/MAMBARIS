{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52kjBh23OeHt"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1zSxPYkYqmJf"
      },
      "outputs": [],
      "source": [
        "#!python -m pip uninstall torch torchvision torchaudio -y\n",
        "#!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD_xSC9yOeHv",
        "outputId": "1621f684-83b6-4a92-ac4e-bc5ddce766de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.6.0+cu118\n",
            "Uninstalling torch-2.6.0+cu118:\n",
            "  Successfully uninstalled torch-2.6.0+cu118\n",
            "Found existing installation: torchvision 0.21.0+cu118\n",
            "Uninstalling torchvision-0.21.0+cu118:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu118\n",
            "Found existing installation: torchaudio 2.6.0+cu118\n",
            "Uninstalling torchaudio-2.6.0+cu118:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu118\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118\n"
          ]
        }
      ],
      "source": [
        "!python -m pip uninstall -y torch torchvision torchaudio\n",
        "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQD3XGnjOeHx",
        "outputId": "e14b30fe-cb3e-4c90-dd6a-33943c943090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.11/dist-packages (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (2.6.0+cu118)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (1.11.1.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (4.48.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.8.86)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mamba-ssm) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install mamba-ssm\n",
        "#!python --version\n",
        "#!pip install --upgrade pip\n",
        "#!pip uninstall keras tensorflow\n",
        "#!pip install -r ../requirements.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyQ30LJROeH1"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nstep3JHOeH2"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import json\n",
        "\n",
        "from data_loader import load_cifar10, get_class_names\n",
        "from training_utils import train_model, continue_training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETeZrLp4OeH3"
      },
      "source": [
        "## CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phsFgOFJOeH4",
        "outputId": "9e4926a4-028c-4855-f712-89c0f2cf3d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is CUDA available? True\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XiPnUSQgOeH5"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCF9N_2UOeH6"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qIfRejqnOeH7"
      },
      "outputs": [],
      "source": [
        "# Set the device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dS4SlfggOeH7"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader, X_train, X_test, Y_train, Y_test = load_cifar10(batch_size=64, seed=42)\n",
        "class_names = get_class_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF9ouRtWOeH8"
      },
      "source": [
        "# Train mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hqxtWZ_LOeH9"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim # Import optim\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "# Test training loop\n",
        "def test_training(model, train_loader, test_loader, device, num_epochs=10):\n",
        "    model = model.to(device)\n",
        "    # Ensure all parameters require gradients - No longer needed\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = True\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # ... (rest of your code) # Keep the rest of your training loop\n",
        "\n",
        "    print(\"Starting test training loop...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            # Debug info\n",
        "            print(f\"Input shape: {inputs.shape}, dtype: {inputs.dtype}\")\n",
        "            print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}\")\n",
        "\n",
        "            # Move to GPU with explicit type casting\n",
        "            try:\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error moving data to GPU: {e}\")\n",
        "                print(f\"Memory stats before error:\")\n",
        "                print(torch.cuda.memory_summary())\n",
        "                raise\n",
        "\n",
        "            # Rest of the training loop...\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits, probs = model(inputs)\n",
        "\n",
        "            # Check for NaN in outputs\n",
        "            if torch.isnan(logits).any() or torch.isnan(probs).any():\n",
        "                print(f\"\\nNaN detected in model outputs at batch {i}!\")\n",
        "                return False\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # Check for NaN in loss\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"\\nNaN detected in loss at batch {i}!\")\n",
        "                return False\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Check for NaN in gradients\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None and torch.isnan(param.grad).any():\n",
        "                    print(f\"\\nNaN detected in gradients for {name}!\")\n",
        "                    return False\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute batch statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total += labels.size(0)\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Print batch progress every 50 batches\n",
        "            if (i + 1) % 50 == 0:\n",
        "                batch_loss = running_loss / (i + 1)\n",
        "                batch_acc = 100 * running_correct / total\n",
        "                print(f\"\\rEpoch [{epoch+1}/{num_epochs}] \"\n",
        "                      f\"Batch [{i+1}/{len(train_loader)}] \"\n",
        "                      f\"Loss: {batch_loss:.4f} \"\n",
        "                      f\"Acc: {batch_acc:.2f}%\", end=\"\")\n",
        "\n",
        "        # Compute epoch statistics\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = 100 * running_correct / total\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model.eval()\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                logits, probs = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = torch.max(logits.data, 1)\n",
        "                test_total += labels.size(0)\n",
        "                test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc = 100 * test_correct / test_total\n",
        "\n",
        "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] \"\n",
        "              f\"Train Loss: {epoch_loss:.4f} \"\n",
        "              f\"Train Acc: {epoch_acc:.2f}% \"\n",
        "              f\"Test Loss: {test_loss:.4f} \"\n",
        "              f\"Test Acc: {test_acc:.2f}%\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    print(\"\\nTest training completed successfully!\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h9eXTVtSpqy",
        "outputId": "7ac52929-3293-4af6-c2a1-0de4e9993207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter name: conv1.weight, requires_grad: True\n",
            "Parameter name: conv1.bias, requires_grad: True\n",
            "Parameter name: bn1.weight, requires_grad: True\n",
            "Parameter name: bn1.bias, requires_grad: True\n",
            "Parameter name: patch_to_seq.0.weight, requires_grad: True\n",
            "Parameter name: patch_to_seq.0.bias, requires_grad: True\n",
            "Parameter name: patch_to_seq.1.weight, requires_grad: True\n",
            "Parameter name: patch_to_seq.1.bias, requires_grad: True\n",
            "Parameter name: layers.0.A_log, requires_grad: True\n",
            "Parameter name: layers.0.D, requires_grad: True\n",
            "Parameter name: layers.0.in_proj.weight, requires_grad: True\n",
            "Parameter name: layers.0.in_proj.bias, requires_grad: True\n",
            "Parameter name: layers.0.conv1d.weight, requires_grad: True\n",
            "Parameter name: layers.0.conv1d.bias, requires_grad: True\n",
            "Parameter name: layers.0.x_proj.weight, requires_grad: True\n",
            "Parameter name: layers.0.dt_proj.weight, requires_grad: True\n",
            "Parameter name: layers.0.dt_proj.bias, requires_grad: True\n",
            "Parameter name: layers.0.out_proj.weight, requires_grad: True\n",
            "Parameter name: layers.0.out_proj.bias, requires_grad: True\n",
            "Parameter name: layers.1.A_log, requires_grad: True\n",
            "Parameter name: layers.1.D, requires_grad: True\n",
            "Parameter name: layers.1.in_proj.weight, requires_grad: True\n",
            "Parameter name: layers.1.in_proj.bias, requires_grad: True\n",
            "Parameter name: layers.1.conv1d.weight, requires_grad: True\n",
            "Parameter name: layers.1.conv1d.bias, requires_grad: True\n",
            "Parameter name: layers.1.x_proj.weight, requires_grad: True\n",
            "Parameter name: layers.1.dt_proj.weight, requires_grad: True\n",
            "Parameter name: layers.1.dt_proj.bias, requires_grad: True\n",
            "Parameter name: layers.1.out_proj.weight, requires_grad: True\n",
            "Parameter name: layers.1.out_proj.bias, requires_grad: True\n",
            "Parameter name: layers.2.A_log, requires_grad: True\n",
            "Parameter name: layers.2.D, requires_grad: True\n",
            "Parameter name: layers.2.in_proj.weight, requires_grad: True\n",
            "Parameter name: layers.2.in_proj.bias, requires_grad: True\n",
            "Parameter name: layers.2.conv1d.weight, requires_grad: True\n",
            "Parameter name: layers.2.conv1d.bias, requires_grad: True\n",
            "Parameter name: layers.2.x_proj.weight, requires_grad: True\n",
            "Parameter name: layers.2.dt_proj.weight, requires_grad: True\n",
            "Parameter name: layers.2.dt_proj.bias, requires_grad: True\n",
            "Parameter name: layers.2.out_proj.weight, requires_grad: True\n",
            "Parameter name: layers.2.out_proj.bias, requires_grad: True\n",
            "Parameter name: layers.3.A_log, requires_grad: True\n",
            "Parameter name: layers.3.D, requires_grad: True\n",
            "Parameter name: layers.3.in_proj.weight, requires_grad: True\n",
            "Parameter name: layers.3.in_proj.bias, requires_grad: True\n",
            "Parameter name: layers.3.conv1d.weight, requires_grad: True\n",
            "Parameter name: layers.3.conv1d.bias, requires_grad: True\n",
            "Parameter name: layers.3.x_proj.weight, requires_grad: True\n",
            "Parameter name: layers.3.dt_proj.weight, requires_grad: True\n",
            "Parameter name: layers.3.dt_proj.bias, requires_grad: True\n",
            "Parameter name: layers.3.out_proj.weight, requires_grad: True\n",
            "Parameter name: layers.3.out_proj.bias, requires_grad: True\n",
            "Parameter name: norm.weight, requires_grad: True\n",
            "Parameter name: fc.weight, requires_grad: True\n",
            "Parameter name: fc.bias, requires_grad: True\n"
          ]
        }
      ],
      "source": [
        "from ssm import FastImageMamba, ModelArgs, FastMambaBlock\n",
        "import torch.optim as optim # Import optim\n",
        "import torch.nn as nn\n",
        "model_args = ModelArgs(\n",
        "    d_model=128,       # Match the channel size in your initial conv layer\n",
        "    n_layer=4,         # Number of Mamba blocks\n",
        "    vocab_size=10,     # Not used for images, but required\n",
        "    d_state=16,        # State space dimension\n",
        "    expand=2,          # Expansion factor\n",
        "    dt_rank=16,        # Rank for delta computation\n",
        "    d_conv=4,          # Convolution kernel size\n",
        "    seq_len=256        # Sequence length (16x16 for CIFAR-10)\n",
        ")\n",
        "\n",
        "model = FastImageMamba(model_args, num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "# **Important:** Make sure parameters require gradients\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter name: {name}, requires_grad: {param.requires_grad}\") # Print to check\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7raQuwDms9u",
        "outputId": "a9445a6b-afa9-44f1-d165-10c176def400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cu118\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "Current CUDA device: 0\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "    print(f\"Device name: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "AG31_b6ROeH-",
        "outputId": "7cd7ac15-dd32-4aee-bed0-12e197966fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Starting test training loop...\n",
            "==================================================\n",
            "Input shape: torch.Size([64, 3, 32, 32]), dtype: torch.float32\n",
            "Labels shape: torch.Size([64]), dtype: torch.int64\n",
            "After initial conv: torch.Size([64, 128, 32, 32]) torch.float32 cuda:0 True\n",
            "After patch_to_seq: torch.Size([64, 128, 16, 16]) torch.float32 cuda:0 True\n",
            "Before mamba blocks: torch.Size([64, 256, 128]) torch.float32 cuda:0 True\n",
            "Error in mamba block 0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (255) must match the size of tensor b (256) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-619febd00ec8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run test training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training is stable - no NaNs detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b2c705a86b97>\u001b[0m in \u001b[0;36mtest_training\u001b[0;34m(model, train_loader, test_loader, device, num_epochs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# Check for NaN in outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ssm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"After mamba block {i}:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ssm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# Residual connection with SiLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Final projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (255) must match the size of tensor b (256) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "from model import ImageMamba, ModelArgs, ProperImageMamba\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Run test training\n",
        "success = test_training(model, train_loader, test_loader, device)\n",
        "if success:\n",
        "    print(\"Training is stable - no NaNs detected\")\n",
        "else:\n",
        "    print(\"Training failed - NaNs detected\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nl9ZyAOOeH-"
      },
      "outputs": [],
      "source": [
        "from model import ImageMamba, ModelArgs\n",
        "\n",
        "# Load your model and data\n",
        "model_args = ModelArgs(d_model=128, n_layer=8, vocab_size=0)\n",
        "model = ImageMamba(args=model_args, num_classes=10)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YElzpXnpOeH_"
      },
      "outputs": [],
      "source": [
        "metrics = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    model_name='mamba',\n",
        "    num_epochs=2000,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAXa6H_jOeH_"
      },
      "source": [
        "# Continue training\n",
        "If there was a crash. Which can be when dealing with so many epochs, one can continue from here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4hHMwz8OeIA"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = 'mamba_checkpoints'\n",
        "with open(f'{checkpoint_dir}/training_metrics.json', 'r') as f:\n",
        "    metrics = json.load(f)\n",
        "print(f\"Last completed epoch: {metrics['current_epoch']}\")\n",
        "\n",
        "metrics = continue_training(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    model_name='mamba',\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    target_epochs=2000,\n",
        "    device=device\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
