{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset\n",
    "Find new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model #save and load models\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Reshape and preprocess the CIFAR-10 dataset for PyTorch models\n",
    "X_train = X_train.transpose(0, 3, 1, 2)  # Shape: (batch_size, channels, height, width)\n",
    "X_test = X_test.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Convert data to float and normalize pixel values in the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the train/test data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create an instance of ModelArgs\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "\n",
    "# Instantiate the ImageMamba model\n",
    "model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training MAMBA on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model + testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 2, more epochs\n",
    "As we can see there is not much difference between the probabilities of true class predictions on the traindataset instances, and the testdataset instances.\n",
    "\n",
    "This might indicate that the dataset was hard easy for 10 epochs, lets add 15 more epochs, to get to a total of 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    " \n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataset:  # Use test_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = np.array(y_prob) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode for training data\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_dataset:  # Use train_dataset directly\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = np.array(y_prob_train) * 100  # Convert probabilities to percentages\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see still no significant diffirence, lets make a stronger model, and train more epochs to achieve a loss close to 0, and then try inference again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 3, complex model\n",
    "To get a much lower loss, we will both increase the number of epochs and make the model more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "d_model = 128\n",
    "n_layer = 8\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Create an instance of ModelArgs\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=0)  # vocab_size is unused here\n",
    "\n",
    "# Instantiate the ImageMamba model\n",
    "complex_model = ImageMamba(model_args, num_classes=num_classes)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "complex_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(complex_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25 \n",
    "for epoch in range(num_epochs):\n",
    "    complex_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = complex_model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will do 25 more, to have a total of 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(complex_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    complex_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = complex_model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
