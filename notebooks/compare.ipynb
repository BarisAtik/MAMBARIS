{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure MAMBA and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     27\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/your/model/directory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageMamba, ModelArgs\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "import sys\n",
    "sys.path.append('/path/to/your/model/directory')\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoint(model, loader, device):\n",
    "    \"\"\"Evaluate a model checkpoint comprehensively\"\"\"\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    running_loss = 0\n",
    "    confidences = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels.squeeze())\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.squeeze()).sum().item()\n",
    "            running_loss += loss.item()\n",
    "            confidences.extend(confidence.cpu().numpy())\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_confidence = np.mean(confidences)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'confidences': confidences\n",
    "    }\n",
    "\n",
    "def reconstruct_metrics_from_checkpoints(model_class, checkpoint_base_path, \n",
    "                                       train_loader, test_loader, \n",
    "                                       json_save_path, device='cuda'):\n",
    "    \"\"\"Reconstruct metrics from saved checkpoints\"\"\"\n",
    "    model = model_class().to(device)\n",
    "    metrics = {\n",
    "        'train_accuracies': [],\n",
    "        'test_accuracies': [],\n",
    "        'train_losses': [],\n",
    "        'test_losses': [],\n",
    "        'train_confidences': [],\n",
    "        'test_confidences': [],\n",
    "        'epochs': [],\n",
    "        'train_confidence_distributions': [],\n",
    "        'test_confidence_distributions': []\n",
    "    }\n",
    "    \n",
    "    # Evaluate checkpoints at every 100 epochs\n",
    "    for epoch in range(100, 1501, 100):\n",
    "        checkpoint_path = os.path.join(checkpoint_base_path, f'model_epoch_{epoch}.pt')\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"Skipping epoch {epoch} - checkpoint not found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing epoch {epoch}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Evaluate on train and test sets\n",
    "        train_metrics = evaluate_checkpoint(model, train_loader, device)\n",
    "        test_metrics = evaluate_checkpoint(model, test_loader, device)\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['epochs'].append(epoch)\n",
    "        metrics['train_accuracies'].append(train_metrics['accuracy'])\n",
    "        metrics['test_accuracies'].append(test_metrics['accuracy'])\n",
    "        metrics['train_losses'].append(train_metrics['loss'])\n",
    "        metrics['test_losses'].append(test_metrics['loss'])\n",
    "        metrics['train_confidences'].append(train_metrics['avg_confidence'])\n",
    "        metrics['test_confidences'].append(test_metrics['avg_confidence'])\n",
    "        metrics['train_confidence_distributions'].append(train_metrics['confidences'])\n",
    "        metrics['test_confidence_distributions'].append(test_metrics['confidences'])\n",
    "    \n",
    "    # Save metrics to JSON\n",
    "    with open(json_save_path, 'w') as f:\n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        json_metrics = {\n",
    "            'train_accuracies': [float(x) for x in metrics['train_accuracies']],\n",
    "            'test_accuracies': [float(x) for x in metrics['test_accuracies']],\n",
    "            'train_losses': [float(x) for x in metrics['train_losses']],\n",
    "            'test_losses': [float(x) for x in metrics['test_losses']],\n",
    "            'train_confidences': [float(x) for x in metrics['train_confidences']],\n",
    "            'test_confidences': [float(x) for x in metrics['test_confidences']],\n",
    "            'epochs': metrics['epochs'],\n",
    "            # Store only summary statistics for confidence distributions to keep JSON size manageable\n",
    "            'train_confidence_distributions_stats': [\n",
    "                {\n",
    "                    'mean': float(np.mean(dist)),\n",
    "                    'std': float(np.std(dist)),\n",
    "                    'min': float(np.min(dist)),\n",
    "                    'max': float(np.max(dist)),\n",
    "                    'median': float(np.median(dist)),\n",
    "                    'q1': float(np.percentile(dist, 25)),\n",
    "                    'q3': float(np.percentile(dist, 75))\n",
    "                } for dist in metrics['train_confidence_distributions']\n",
    "            ],\n",
    "            'test_confidence_distributions_stats': [\n",
    "                {\n",
    "                    'mean': float(np.mean(dist)),\n",
    "                    'std': float(np.std(dist)),\n",
    "                    'min': float(np.min(dist)),\n",
    "                    'max': float(np.max(dist)),\n",
    "                    'median': float(np.median(dist)),\n",
    "                    'q1': float(np.percentile(dist, 25)),\n",
    "                    'q3': float(np.percentile(dist, 75))\n",
    "                } for dist in metrics['test_confidence_distributions']\n",
    "            ]\n",
    "        }\n",
    "        json.dump(json_metrics, f, indent=4)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_comparative_metrics(cnn_metrics, mamba_metrics, save_dir='comparison_plots'):\n",
    "    \"\"\"Create comparative plots of CNN vs MAMBA metrics\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up the plots\n",
    "    plt.style.use('seaborn')\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    epochs = cnn_metrics['epochs']\n",
    "    \n",
    "    # 1. Accuracy Comparison\n",
    "    ax1.plot(epochs, cnn_metrics['train_accuracies'], 'b-', label='CNN Train')\n",
    "    ax1.plot(epochs, cnn_metrics['test_accuracies'], 'b--', label='CNN Test')\n",
    "    ax1.plot(epochs, mamba_metrics['train_accuracies'], 'r-', label='MAMBA Train')\n",
    "    ax1.plot(epochs, mamba_metrics['test_accuracies'], 'r--', label='MAMBA Test')\n",
    "    ax1.set_title('Accuracy Comparison')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Loss Comparison\n",
    "    ax2.plot(epochs, cnn_metrics['train_losses'], 'b-', label='CNN Train')\n",
    "    ax2.plot(epochs, cnn_metrics['test_losses'], 'b--', label='CNN Test')\n",
    "    ax2.plot(epochs, mamba_metrics['train_losses'], 'r-', label='MAMBA Train')\n",
    "    ax2.plot(epochs, mamba_metrics['test_losses'], 'r--', label='MAMBA Test')\n",
    "    ax2.set_title('Loss Comparison')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Confidence Comparison\n",
    "    ax3.plot(epochs, cnn_metrics['train_confidences'], 'b-', label='CNN Train')\n",
    "    ax3.plot(epochs, cnn_metrics['test_confidences'], 'b--', label='CNN Test')\n",
    "    ax3.plot(epochs, mamba_metrics['train_confidences'], 'r-', label='MAMBA Train')\n",
    "    ax3.plot(epochs, mamba_metrics['test_confidences'], 'r--', label='MAMBA Test')\n",
    "    ax3.set_title('Average Confidence Comparison')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Confidence')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # 4. Overfitting Analysis (Train-Test Accuracy Gap)\n",
    "    cnn_gap = [t - v for t, v in zip(cnn_metrics['train_accuracies'], cnn_metrics['test_accuracies'])]\n",
    "    mamba_gap = [t - v for t, v in zip(mamba_metrics['train_accuracies'], mamba_metrics['test_accuracies'])]\n",
    "    \n",
    "    ax4.plot(epochs, cnn_gap, 'b-', label='CNN')\n",
    "    ax4.plot(epochs, mamba_gap, 'r-', label='MAMBA')\n",
    "    ax4.set_title('Overfitting Analysis (Train-Test Accuracy Gap)')\n",
    "    ax4.set_xlabel('Epochs')\n",
    "    ax4.set_ylabel('Accuracy Gap (%)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'model_comparisons.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First reconstruct metrics for CNN\n",
    "cnn_metrics = reconstruct_metrics_from_checkpoints(\n",
    "    model_class=SmallerComparableCNN,\n",
    "    checkpoint_base_path='smaller_cnn_checkpoints',\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    json_save_path='New_CNN_Metrics.json',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Then reconstruct metrics for MAMBA\n",
    "mamba_metrics = reconstruct_metrics_from_checkpoints(\n",
    "    model_class=ImageMamba,  # your MAMBA model class\n",
    "    checkpoint_base_path='model_checkpoints_extended',\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    json_save_path='New_MAMBA_Metrics.json',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create comparison plots\n",
    "plot_comparative_metrics(cnn_metrics, mamba_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
