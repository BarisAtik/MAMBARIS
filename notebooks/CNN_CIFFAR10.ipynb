{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Sep_12_02:55:00_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.77\n",
      "Build cuda_12.6.r12.6/compiler.34841621_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Reshape and preprocess the CIFAR-10 dataset for PyTorch models\n",
    "X_train = X_train.transpose(0, 3, 1, 2)  # Shape: (batch_size, channels, height, width)\n",
    "X_test = X_test.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Convert data to float and normalize pixel values in the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the train/test data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Your existing train loader code\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Add test loader\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # Note: shuffle=False for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComparableCNN, self).__init__()\n",
    "        # First conv block - similar to MAMBA's first conv\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Second conv block - similar to MAMBA's second conv\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Additional conv layers to match MAMBA's 4 layers\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Global average pooling and final dense layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Fourth block\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return logits, probabilities\n",
    "\n",
    "def load_training_metrics(metrics_path):\n",
    "    \"\"\"Load training metrics from JSON file\"\"\"\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "        # Convert lists back to numpy arrays where needed\n",
    "        metrics['epoch_train_confidences'] = [np.array(arr) for arr in metrics['epoch_train_confidences']]\n",
    "        metrics['epoch_test_confidences'] = [np.array(arr) for arr in metrics['epoch_test_confidences']]\n",
    "    return metrics\n",
    "\n",
    "def save_cnn_checkpoint(model, optimizer, epoch, metrics, path):\n",
    "    \"\"\"Save CNN model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f'Checkpoint saved: {path}')\n",
    "\n",
    "def train_evaluate_cnn(model, train_loader, test_loader, num_epochs=100, device='cuda', \n",
    "                      checkpoint_dir='cnn_checkpoints'):\n",
    "    \"\"\"Train and evaluate CNN with checkpointing\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_losses': [], 'test_losses': [],\n",
    "        'train_accuracies': [], 'test_accuracies': [],\n",
    "        'train_confidences': [], 'test_confidences': [],\n",
    "        'epoch_train_confidences': [], 'epoch_test_confidences': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        train_confidence_sum = 0\n",
    "        train_epoch_confidences = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            running_correct += (predicted == labels.squeeze()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            train_confidence_sum += confidence.sum().item()\n",
    "            train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = (running_correct / total_samples) * 100\n",
    "        train_avg_confidence = train_confidence_sum / total_samples\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_confidence_sum = 0\n",
    "        test_epoch_confidences = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits, probabilities = model(inputs)\n",
    "                loss = criterion(logits, labels.squeeze())\n",
    "                \n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                confidence, _ = torch.max(probabilities, 1)\n",
    "                test_correct += (predicted == labels.squeeze()).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_confidence_sum += confidence.sum().item()\n",
    "                test_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = (test_correct / test_total) * 100\n",
    "        test_avg_confidence = test_confidence_sum / test_total\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['train_losses'].append(train_loss)\n",
    "        metrics['test_losses'].append(test_loss)\n",
    "        metrics['train_accuracies'].append(train_accuracy)\n",
    "        metrics['test_accuracies'].append(test_accuracy)\n",
    "        metrics['train_confidences'].append(train_avg_confidence)\n",
    "        metrics['test_confidences'].append(test_avg_confidence)\n",
    "        metrics['epoch_train_confidences'].append(train_epoch_confidences)\n",
    "        metrics['epoch_test_confidences'].append(test_epoch_confidences)\n",
    "        \n",
    "        # Save metrics to JSON file after each epoch\n",
    "        metrics_path = os.path.join(checkpoint_dir, 'training_metrics.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            # Convert to float with reduced precision (4 decimal places)\n",
    "            json_metrics = {\n",
    "                'train_losses': [round(float(x), 4) for x in metrics['train_losses']],\n",
    "                'test_losses': [round(float(x), 4) for x in metrics['test_losses']],\n",
    "                'train_accuracies': [round(float(x), 4) for x in metrics['train_accuracies']],\n",
    "                'test_accuracies': [round(float(x), 4) for x in metrics['test_accuracies']],\n",
    "                'train_confidences': [round(float(x), 4) for x in metrics['train_confidences']],\n",
    "                'test_confidences': [round(float(x), 4) for x in metrics['test_confidences']],\n",
    "                'epoch_train_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) else round(float(arr), 4)\n",
    "                                          for arr in metrics['epoch_train_confidences']],\n",
    "                'epoch_test_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) else round(float(arr), 4)\n",
    "                                         for arr in metrics['epoch_test_confidences']],\n",
    "                'current_epoch': epoch + 1\n",
    "            }\n",
    "            json.dump(json_metrics, f, indent=4)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "            print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "            print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "        \n",
    "        # Save checkpoint every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cnn_model_epoch_{epoch+1}.pt')\n",
    "            save_cnn_checkpoint(model, optimizer, epoch, metrics, checkpoint_path)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and data loaders\n",
    "cnn_model = ComparableCNN().to(device)\n",
    "checkpoint_dir = 'cnn_checkpoints'\n",
    "\n",
    "print(f\"Model device: {next(cnn_model.parameters()).device}\")\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "# Train model with checkpointing\n",
    "metrics = train_evaluate_cnn(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=400,  # matching your MAMBA training\n",
    "    device=device,\n",
    "    checkpoint_dir=checkpoint_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Is CUDA available? True\n",
      "Epoch [0/400]\n",
      "  Training: Loss: 1.9716, Accuracy: 30.76%, Confidence: 0.2065\n",
      "  Testing:  Loss: 1.8292, Accuracy: 38.22%, Confidence: 0.2355\n",
      "Epoch [10/400]\n",
      "  Training: Loss: 1.3993, Accuracy: 51.56%, Confidence: 0.4150\n",
      "  Testing:  Loss: 1.4306, Accuracy: 48.93%, Confidence: 0.4302\n",
      "Epoch [20/400]\n",
      "  Training: Loss: 1.2921, Accuracy: 55.39%, Confidence: 0.4616\n",
      "  Testing:  Loss: 1.3186, Accuracy: 53.02%, Confidence: 0.4654\n",
      "Epoch [30/400]\n",
      "  Training: Loss: 1.2209, Accuracy: 57.70%, Confidence: 0.4916\n",
      "  Testing:  Loss: 1.2365, Accuracy: 57.20%, Confidence: 0.4861\n",
      "Epoch [40/400]\n",
      "  Training: Loss: 1.1728, Accuracy: 59.38%, Confidence: 0.5115\n",
      "  Testing:  Loss: 1.1988, Accuracy: 57.94%, Confidence: 0.5187\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_50.pt\n",
      "Epoch [50/400]\n",
      "  Training: Loss: 1.1275, Accuracy: 61.03%, Confidence: 0.5286\n",
      "  Testing:  Loss: 1.1371, Accuracy: 60.48%, Confidence: 0.5240\n",
      "Epoch [60/400]\n",
      "  Training: Loss: 1.0950, Accuracy: 62.32%, Confidence: 0.5409\n",
      "  Testing:  Loss: 1.1163, Accuracy: 61.76%, Confidence: 0.5448\n",
      "Epoch [70/400]\n",
      "  Training: Loss: 1.0650, Accuracy: 63.56%, Confidence: 0.5524\n",
      "  Testing:  Loss: 1.0839, Accuracy: 62.45%, Confidence: 0.5537\n",
      "Epoch [80/400]\n",
      "  Training: Loss: 1.0389, Accuracy: 64.37%, Confidence: 0.5624\n",
      "  Testing:  Loss: 1.0884, Accuracy: 62.30%, Confidence: 0.5746\n",
      "Epoch [90/400]\n",
      "  Training: Loss: 1.0137, Accuracy: 65.37%, Confidence: 0.5708\n",
      "  Testing:  Loss: 1.0869, Accuracy: 62.20%, Confidence: 0.5859\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_100.pt\n",
      "Epoch [100/400]\n",
      "  Training: Loss: 0.9932, Accuracy: 65.89%, Confidence: 0.5793\n",
      "  Testing:  Loss: 1.0563, Accuracy: 63.43%, Confidence: 0.5887\n",
      "Epoch [110/400]\n",
      "  Training: Loss: 0.9713, Accuracy: 66.71%, Confidence: 0.5875\n",
      "  Testing:  Loss: 1.0158, Accuracy: 64.53%, Confidence: 0.5961\n",
      "Epoch [120/400]\n",
      "  Training: Loss: 0.9551, Accuracy: 67.17%, Confidence: 0.5935\n",
      "  Testing:  Loss: 0.9952, Accuracy: 65.97%, Confidence: 0.5936\n",
      "Epoch [130/400]\n",
      "  Training: Loss: 0.9370, Accuracy: 68.27%, Confidence: 0.6003\n",
      "  Testing:  Loss: 0.9940, Accuracy: 65.83%, Confidence: 0.5933\n",
      "Epoch [140/400]\n",
      "  Training: Loss: 0.9228, Accuracy: 68.63%, Confidence: 0.6057\n",
      "  Testing:  Loss: 1.0104, Accuracy: 64.15%, Confidence: 0.6065\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_150.pt\n",
      "Epoch [150/400]\n",
      "  Training: Loss: 0.9059, Accuracy: 69.30%, Confidence: 0.6124\n",
      "  Testing:  Loss: 0.9738, Accuracy: 66.27%, Confidence: 0.6143\n",
      "Epoch [160/400]\n",
      "  Training: Loss: 0.8928, Accuracy: 69.66%, Confidence: 0.6171\n",
      "  Testing:  Loss: 1.0037, Accuracy: 64.66%, Confidence: 0.6122\n",
      "Epoch [170/400]\n",
      "  Training: Loss: 0.8795, Accuracy: 70.19%, Confidence: 0.6225\n",
      "  Testing:  Loss: 0.9798, Accuracy: 64.78%, Confidence: 0.6241\n",
      "Epoch [180/400]\n",
      "  Training: Loss: 0.8654, Accuracy: 70.53%, Confidence: 0.6266\n",
      "  Testing:  Loss: 0.9699, Accuracy: 65.62%, Confidence: 0.6255\n",
      "Epoch [190/400]\n",
      "  Training: Loss: 0.8557, Accuracy: 70.91%, Confidence: 0.6316\n",
      "  Testing:  Loss: 0.9332, Accuracy: 67.79%, Confidence: 0.6369\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_200.pt\n",
      "Epoch [200/400]\n",
      "  Training: Loss: 0.8421, Accuracy: 71.36%, Confidence: 0.6358\n",
      "  Testing:  Loss: 0.9511, Accuracy: 67.54%, Confidence: 0.6263\n",
      "Epoch [210/400]\n",
      "  Training: Loss: 0.8334, Accuracy: 71.53%, Confidence: 0.6404\n",
      "  Testing:  Loss: 0.9133, Accuracy: 68.80%, Confidence: 0.6228\n",
      "Epoch [220/400]\n",
      "  Training: Loss: 0.8234, Accuracy: 72.11%, Confidence: 0.6436\n",
      "  Testing:  Loss: 0.9024, Accuracy: 68.68%, Confidence: 0.6400\n",
      "Epoch [230/400]\n",
      "  Training: Loss: 0.8124, Accuracy: 72.55%, Confidence: 0.6473\n",
      "  Testing:  Loss: 0.9908, Accuracy: 65.16%, Confidence: 0.6377\n",
      "Epoch [240/400]\n",
      "  Training: Loss: 0.8005, Accuracy: 72.94%, Confidence: 0.6525\n",
      "  Testing:  Loss: 0.9369, Accuracy: 66.71%, Confidence: 0.6483\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_250.pt\n",
      "Epoch [250/400]\n",
      "  Training: Loss: 0.7944, Accuracy: 73.14%, Confidence: 0.6551\n",
      "  Testing:  Loss: 0.9060, Accuracy: 67.73%, Confidence: 0.6521\n",
      "Epoch [260/400]\n",
      "  Training: Loss: 0.7861, Accuracy: 73.41%, Confidence: 0.6576\n",
      "  Testing:  Loss: 0.8912, Accuracy: 69.04%, Confidence: 0.6534\n",
      "Epoch [270/400]\n",
      "  Training: Loss: 0.7780, Accuracy: 73.68%, Confidence: 0.6614\n",
      "  Testing:  Loss: 0.9165, Accuracy: 67.93%, Confidence: 0.6518\n",
      "Epoch [280/400]\n",
      "  Training: Loss: 0.7681, Accuracy: 74.06%, Confidence: 0.6652\n",
      "  Testing:  Loss: 0.9080, Accuracy: 68.45%, Confidence: 0.6596\n",
      "Epoch [290/400]\n",
      "  Training: Loss: 0.7626, Accuracy: 74.31%, Confidence: 0.6676\n",
      "  Testing:  Loss: 0.8975, Accuracy: 69.10%, Confidence: 0.6658\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_300.pt\n",
      "Epoch [300/400]\n",
      "  Training: Loss: 0.7545, Accuracy: 74.60%, Confidence: 0.6708\n",
      "  Testing:  Loss: 0.8933, Accuracy: 68.17%, Confidence: 0.6692\n",
      "Epoch [310/400]\n",
      "  Training: Loss: 0.7483, Accuracy: 74.65%, Confidence: 0.6744\n",
      "  Testing:  Loss: 0.8754, Accuracy: 69.21%, Confidence: 0.6670\n",
      "Epoch [320/400]\n",
      "  Training: Loss: 0.7391, Accuracy: 75.01%, Confidence: 0.6766\n",
      "  Testing:  Loss: 0.9051, Accuracy: 68.21%, Confidence: 0.6688\n",
      "Epoch [330/400]\n",
      "  Training: Loss: 0.7350, Accuracy: 75.22%, Confidence: 0.6789\n",
      "  Testing:  Loss: 0.8850, Accuracy: 69.41%, Confidence: 0.6717\n",
      "Epoch [340/400]\n",
      "  Training: Loss: 0.7264, Accuracy: 75.43%, Confidence: 0.6817\n",
      "  Testing:  Loss: 0.9616, Accuracy: 66.17%, Confidence: 0.6735\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_350.pt\n",
      "Epoch [350/400]\n",
      "  Training: Loss: 0.7212, Accuracy: 75.79%, Confidence: 0.6853\n",
      "  Testing:  Loss: 0.9018, Accuracy: 68.60%, Confidence: 0.6711\n",
      "Epoch [360/400]\n",
      "  Training: Loss: 0.7163, Accuracy: 75.89%, Confidence: 0.6864\n",
      "  Testing:  Loss: 0.8755, Accuracy: 69.03%, Confidence: 0.6733\n",
      "Epoch [370/400]\n",
      "  Training: Loss: 0.7083, Accuracy: 75.98%, Confidence: 0.6896\n",
      "  Testing:  Loss: 0.9363, Accuracy: 67.06%, Confidence: 0.6942\n",
      "Epoch [380/400]\n",
      "  Training: Loss: 0.7027, Accuracy: 76.37%, Confidence: 0.6918\n",
      "  Testing:  Loss: 0.8746, Accuracy: 69.51%, Confidence: 0.6796\n",
      "Epoch [390/400]\n",
      "  Training: Loss: 0.6983, Accuracy: 76.60%, Confidence: 0.6943\n",
      "  Testing:  Loss: 0.8421, Accuracy: 70.98%, Confidence: 0.6902\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_400.pt\n"
     ]
    }
   ],
   "source": [
    "class SmallerComparableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallerComparableCNN, self).__init__()\n",
    "        # Reduced initial channels and total layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Reduced from 64 to 32\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Reduced from 128 to 64\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Removed two conv layers to reduce parameters\n",
    "        \n",
    "        # Global average pooling and final dense layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 10)  # Changed input features to match last conv layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return logits, probabilities\n",
    "\n",
    "# Initialize the smaller model and new checkpoint directory\n",
    "smaller_cnn_model = SmallerComparableCNN().to(device)\n",
    "new_checkpoint_dir = 'smaller_cnn_checkpoints'  # New directory for the smaller model\n",
    "print(f\"Model device: {next(smaller_cnn_model.parameters()).device}\")\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "# Train model with checkpointing in new directory\n",
    "metrics = train_evaluate_cnn(\n",
    "    model=smaller_cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=400,\n",
    "    device=device,\n",
    "    checkpoint_dir=new_checkpoint_dir  # Using new directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
