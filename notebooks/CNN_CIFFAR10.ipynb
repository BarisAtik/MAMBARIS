{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Sep_12_02:55:00_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.77\n",
      "Build cuda_12.6.r12.6/compiler.34841621_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:14<00:00, 11.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from data_loader import load_cifar10, get_class_names\n",
    "\n",
    "# Load data consistently\n",
    "train_loader, test_loader, X_train, X_test, Y_train, Y_test = load_cifar10(batch_size=64, seed=42)\n",
    "class_names = get_class_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComparableCNN, self).__init__()\n",
    "        # First conv block - similar to MAMBA's first conv\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Second conv block - similar to MAMBA's second conv\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Additional conv layers to match MAMBA's 4 layers\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Global average pooling and final dense layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Fourth block\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return logits, probabilities\n",
    "\n",
    "def load_training_metrics(metrics_path):\n",
    "    \"\"\"Load training metrics from JSON file\"\"\"\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "        # Convert lists back to numpy arrays where needed\n",
    "        metrics['epoch_train_confidences'] = [np.array(arr) for arr in metrics['epoch_train_confidences']]\n",
    "        metrics['epoch_test_confidences'] = [np.array(arr) for arr in metrics['epoch_test_confidences']]\n",
    "    return metrics\n",
    "\n",
    "def save_cnn_checkpoint(model, optimizer, epoch, metrics, path):\n",
    "    \"\"\"Save CNN model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f'Checkpoint saved: {path}')\n",
    "\n",
    "def train_evaluate_cnn(model, train_loader, test_loader, num_epochs=100, device='cuda', \n",
    "                      checkpoint_dir='cnn_checkpoints'):\n",
    "    \"\"\"Train and evaluate CNN with checkpointing\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_losses': [], 'test_losses': [],\n",
    "        'train_accuracies': [], 'test_accuracies': [],\n",
    "        'train_confidences': [], 'test_confidences': [],\n",
    "        'epoch_train_confidences': [], 'epoch_test_confidences': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        train_confidence_sum = 0\n",
    "        train_epoch_confidences = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            running_correct += (predicted == labels.squeeze()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            train_confidence_sum += confidence.sum().item()\n",
    "            train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = (running_correct / total_samples) * 100\n",
    "        train_avg_confidence = train_confidence_sum / total_samples\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_confidence_sum = 0\n",
    "        test_epoch_confidences = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits, probabilities = model(inputs)\n",
    "                loss = criterion(logits, labels.squeeze())\n",
    "                \n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                confidence, _ = torch.max(probabilities, 1)\n",
    "                test_correct += (predicted == labels.squeeze()).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_confidence_sum += confidence.sum().item()\n",
    "                test_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = (test_correct / test_total) * 100\n",
    "        test_avg_confidence = test_confidence_sum / test_total\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['train_losses'].append(train_loss)\n",
    "        metrics['test_losses'].append(test_loss)\n",
    "        metrics['train_accuracies'].append(train_accuracy)\n",
    "        metrics['test_accuracies'].append(test_accuracy)\n",
    "        metrics['train_confidences'].append(train_avg_confidence)\n",
    "        metrics['test_confidences'].append(test_avg_confidence)\n",
    "        metrics['epoch_train_confidences'].append(train_epoch_confidences)\n",
    "        metrics['epoch_test_confidences'].append(test_epoch_confidences)\n",
    "        \n",
    "        # Save metrics to JSON file after each epoch\n",
    "        metrics_path = os.path.join(checkpoint_dir, 'training_metrics.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            # Convert to float with reduced precision (4 decimal places)\n",
    "            json_metrics = {\n",
    "                'train_losses': [round(float(x), 4) for x in metrics['train_losses']],\n",
    "                'test_losses': [round(float(x), 4) for x in metrics['test_losses']],\n",
    "                'train_accuracies': [round(float(x), 4) for x in metrics['train_accuracies']],\n",
    "                'test_accuracies': [round(float(x), 4) for x in metrics['test_accuracies']],\n",
    "                'train_confidences': [round(float(x), 4) for x in metrics['train_confidences']],\n",
    "                'test_confidences': [round(float(x), 4) for x in metrics['test_confidences']],\n",
    "                'epoch_train_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) else round(float(arr), 4)\n",
    "                                          for arr in metrics['epoch_train_confidences']],\n",
    "                'epoch_test_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) else round(float(arr), 4)\n",
    "                                         for arr in metrics['epoch_test_confidences']],\n",
    "                'current_epoch': epoch + 1\n",
    "            }\n",
    "            json.dump(json_metrics, f, indent=4)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "            print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "            print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "        \n",
    "        # Save checkpoint every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cnn_model_epoch_{epoch+1}.pt')\n",
    "            save_cnn_checkpoint(model, optimizer, epoch, metrics, checkpoint_path)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and data loaders\n",
    "cnn_model = ComparableCNN().to(device)\n",
    "checkpoint_dir = 'cnn_checkpoints'\n",
    "\n",
    "print(f\"Model device: {next(cnn_model.parameters()).device}\")\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "\n",
    "# Train model with checkpointing\n",
    "metrics = train_evaluate_cnn(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=400,  # matching your MAMBA training\n",
    "    device=device,\n",
    "    checkpoint_dir=checkpoint_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallerComparableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallerComparableCNN, self).__init__()\n",
    "        # Reduced initial channels and total layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Reduced from 64 to 32\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Reduced from 128 to 64\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Removed two conv layers to reduce parameters\n",
    "        \n",
    "        # Global average pooling and final dense layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 10)  # Changed input features to match last conv layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return logits, probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 400 to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_training_from_checkpoint(model, train_loader, test_loader, \n",
    "                                last_epoch=400, target_epochs=1000, \n",
    "                                checkpoint_dir='smaller_cnn_checkpoints', device='cuda'):\n",
    "    \"\"\"Continue training from the last checkpoint up to target epochs\"\"\"\n",
    "    \n",
    "    # Load the last checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'cnn_model_epoch_{last_epoch}.pt')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load model and optimizer state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Load existing metrics\n",
    "    metrics_path = os.path.join(checkpoint_dir, 'training_metrics.json')\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Convert metrics lists back to proper format\n",
    "    metrics['epoch_train_confidences'] = [np.array(conf) for conf in metrics['epoch_train_confidences']]\n",
    "    metrics['epoch_test_confidences'] = [np.array(conf) for conf in metrics['epoch_test_confidences']]\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Continue training from last_epoch to target_epochs\n",
    "    for epoch in range(last_epoch, target_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        train_confidence_sum = 0\n",
    "        train_epoch_confidences = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            running_correct += (predicted == labels.squeeze()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            train_confidence_sum += confidence.sum().item()\n",
    "            train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = (running_correct / total_samples) * 100\n",
    "        train_avg_confidence = train_confidence_sum / total_samples\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_confidence_sum = 0\n",
    "        test_epoch_confidences = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits, probabilities = model(inputs)\n",
    "                loss = criterion(logits, labels.squeeze())\n",
    "                \n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                confidence, _ = torch.max(probabilities, 1)\n",
    "                test_correct += (predicted == labels.squeeze()).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_confidence_sum += confidence.sum().item()\n",
    "                test_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = (test_correct / test_total) * 100\n",
    "        test_avg_confidence = test_confidence_sum / test_total\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['train_losses'].append(train_loss)\n",
    "        metrics['test_losses'].append(test_loss)\n",
    "        metrics['train_accuracies'].append(train_accuracy)\n",
    "        metrics['test_accuracies'].append(test_accuracy)\n",
    "        metrics['train_confidences'].append(train_avg_confidence)\n",
    "        metrics['test_confidences'].append(test_avg_confidence)\n",
    "        metrics['epoch_train_confidences'].append(train_epoch_confidences)\n",
    "        metrics['epoch_test_confidences'].append(test_epoch_confidences)\n",
    "        \n",
    "        # Save metrics to JSON file after each epoch\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json_metrics = {\n",
    "                'train_losses': [round(float(x), 4) for x in metrics['train_losses']],\n",
    "                'test_losses': [round(float(x), 4) for x in metrics['test_losses']],\n",
    "                'train_accuracies': [round(float(x), 4) for x in metrics['train_accuracies']],\n",
    "                'test_accuracies': [round(float(x), 4) for x in metrics['test_accuracies']],\n",
    "                'train_confidences': [round(float(x), 4) for x in metrics['train_confidences']],\n",
    "                'test_confidences': [round(float(x), 4) for x in metrics['test_confidences']],\n",
    "                'epoch_train_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) else round(float(arr), 4)\n",
    "                                          for arr in metrics['epoch_train_confidences']],\n",
    "                'epoch_test_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) else round(float(arr), 4)\n",
    "                                         for arr in metrics['epoch_test_confidences']],\n",
    "                'current_epoch': epoch + 1\n",
    "            }\n",
    "            json.dump(json_metrics, f, indent=4)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{target_epochs}]')\n",
    "            print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "            print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "        \n",
    "        # Save checkpoint every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cnn_model_epoch_{epoch+1}.pt')\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baris\\AppData\\Local\\Temp\\ipykernel_9232\\4248689771.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/1000]\n",
      "  Training: Loss: 0.6960, Accuracy: 76.71%, Confidence: 0.6963\n",
      "  Testing:  Loss: 0.8859, Accuracy: 69.49%, Confidence: 0.6957\n",
      "Epoch [410/1000]\n",
      "  Training: Loss: 0.6870, Accuracy: 76.91%, Confidence: 0.6975\n",
      "  Testing:  Loss: 0.8848, Accuracy: 68.89%, Confidence: 0.6946\n",
      "Epoch [420/1000]\n",
      "  Training: Loss: 0.6837, Accuracy: 76.94%, Confidence: 0.7002\n",
      "  Testing:  Loss: 0.8646, Accuracy: 69.93%, Confidence: 0.6880\n",
      "Epoch [430/1000]\n",
      "  Training: Loss: 0.6774, Accuracy: 77.22%, Confidence: 0.7030\n",
      "  Testing:  Loss: 0.9861, Accuracy: 66.10%, Confidence: 0.6905\n",
      "Epoch [440/1000]\n",
      "  Training: Loss: 0.6740, Accuracy: 77.30%, Confidence: 0.7041\n",
      "  Testing:  Loss: 0.8687, Accuracy: 69.70%, Confidence: 0.6998\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_450.pt\n",
      "Epoch [450/1000]\n",
      "  Training: Loss: 0.6718, Accuracy: 77.29%, Confidence: 0.7064\n",
      "  Testing:  Loss: 0.8864, Accuracy: 69.41%, Confidence: 0.7069\n",
      "Epoch [460/1000]\n",
      "  Training: Loss: 0.6656, Accuracy: 77.69%, Confidence: 0.7074\n",
      "  Testing:  Loss: 0.9179, Accuracy: 68.24%, Confidence: 0.6958\n",
      "Epoch [470/1000]\n",
      "  Training: Loss: 0.6617, Accuracy: 77.60%, Confidence: 0.7090\n",
      "  Testing:  Loss: 0.8461, Accuracy: 70.49%, Confidence: 0.7022\n",
      "Epoch [480/1000]\n",
      "  Training: Loss: 0.6546, Accuracy: 78.09%, Confidence: 0.7111\n",
      "  Testing:  Loss: 0.8716, Accuracy: 69.98%, Confidence: 0.6921\n",
      "Epoch [490/1000]\n",
      "  Training: Loss: 0.6505, Accuracy: 77.99%, Confidence: 0.7130\n",
      "  Testing:  Loss: 0.8724, Accuracy: 70.00%, Confidence: 0.7088\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_500.pt\n",
      "Epoch [500/1000]\n",
      "  Training: Loss: 0.6485, Accuracy: 78.12%, Confidence: 0.7144\n",
      "  Testing:  Loss: 0.8587, Accuracy: 70.03%, Confidence: 0.7034\n",
      "Epoch [510/1000]\n",
      "  Training: Loss: 0.6437, Accuracy: 78.44%, Confidence: 0.7163\n",
      "  Testing:  Loss: 0.8784, Accuracy: 69.75%, Confidence: 0.7177\n",
      "Epoch [520/1000]\n",
      "  Training: Loss: 0.6405, Accuracy: 78.55%, Confidence: 0.7178\n",
      "  Testing:  Loss: 0.8412, Accuracy: 71.00%, Confidence: 0.7022\n",
      "Epoch [530/1000]\n",
      "  Training: Loss: 0.6339, Accuracy: 78.63%, Confidence: 0.7198\n",
      "  Testing:  Loss: 0.9216, Accuracy: 67.91%, Confidence: 0.7118\n",
      "Epoch [540/1000]\n",
      "  Training: Loss: 0.6327, Accuracy: 78.73%, Confidence: 0.7212\n",
      "  Testing:  Loss: 0.9453, Accuracy: 67.78%, Confidence: 0.7230\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_550.pt\n",
      "Epoch [550/1000]\n",
      "  Training: Loss: 0.6282, Accuracy: 78.89%, Confidence: 0.7218\n",
      "  Testing:  Loss: 0.8481, Accuracy: 70.19%, Confidence: 0.7140\n",
      "Epoch [560/1000]\n",
      "  Training: Loss: 0.6240, Accuracy: 78.92%, Confidence: 0.7235\n",
      "  Testing:  Loss: 0.8749, Accuracy: 69.18%, Confidence: 0.7123\n",
      "Epoch [570/1000]\n",
      "  Training: Loss: 0.6225, Accuracy: 78.98%, Confidence: 0.7251\n",
      "  Testing:  Loss: 0.8723, Accuracy: 69.29%, Confidence: 0.7100\n",
      "Epoch [580/1000]\n",
      "  Training: Loss: 0.6172, Accuracy: 79.23%, Confidence: 0.7268\n",
      "  Testing:  Loss: 0.8932, Accuracy: 69.27%, Confidence: 0.7119\n",
      "Epoch [590/1000]\n",
      "  Training: Loss: 0.6152, Accuracy: 79.32%, Confidence: 0.7289\n",
      "  Testing:  Loss: 0.8881, Accuracy: 69.76%, Confidence: 0.7222\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_600.pt\n",
      "Epoch [600/1000]\n",
      "  Training: Loss: 0.6132, Accuracy: 79.42%, Confidence: 0.7296\n",
      "  Testing:  Loss: 0.8677, Accuracy: 70.09%, Confidence: 0.7143\n",
      "Epoch [610/1000]\n",
      "  Training: Loss: 0.6083, Accuracy: 79.54%, Confidence: 0.7305\n",
      "  Testing:  Loss: 0.8690, Accuracy: 70.33%, Confidence: 0.7145\n",
      "Epoch [620/1000]\n",
      "  Training: Loss: 0.6061, Accuracy: 79.53%, Confidence: 0.7322\n",
      "  Testing:  Loss: 0.8852, Accuracy: 69.82%, Confidence: 0.7290\n",
      "Epoch [630/1000]\n",
      "  Training: Loss: 0.6049, Accuracy: 79.61%, Confidence: 0.7324\n",
      "  Testing:  Loss: 0.8786, Accuracy: 69.75%, Confidence: 0.7184\n",
      "Epoch [640/1000]\n",
      "  Training: Loss: 0.5987, Accuracy: 79.82%, Confidence: 0.7349\n",
      "  Testing:  Loss: 0.8460, Accuracy: 70.80%, Confidence: 0.7182\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_650.pt\n",
      "Epoch [650/1000]\n",
      "  Training: Loss: 0.5955, Accuracy: 79.88%, Confidence: 0.7361\n",
      "  Testing:  Loss: 0.8573, Accuracy: 70.62%, Confidence: 0.7256\n",
      "Epoch [660/1000]\n",
      "  Training: Loss: 0.5949, Accuracy: 79.83%, Confidence: 0.7370\n",
      "  Testing:  Loss: 0.8691, Accuracy: 69.83%, Confidence: 0.7232\n",
      "Epoch [670/1000]\n",
      "  Training: Loss: 0.5911, Accuracy: 79.96%, Confidence: 0.7382\n",
      "  Testing:  Loss: 0.8756, Accuracy: 69.93%, Confidence: 0.7367\n",
      "Epoch [680/1000]\n",
      "  Training: Loss: 0.5893, Accuracy: 80.06%, Confidence: 0.7392\n",
      "  Testing:  Loss: 0.8265, Accuracy: 71.35%, Confidence: 0.7262\n",
      "Epoch [690/1000]\n",
      "  Training: Loss: 0.5860, Accuracy: 80.32%, Confidence: 0.7408\n",
      "  Testing:  Loss: 0.8640, Accuracy: 70.06%, Confidence: 0.7300\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_700.pt\n",
      "Epoch [700/1000]\n",
      "  Training: Loss: 0.5818, Accuracy: 80.19%, Confidence: 0.7417\n",
      "  Testing:  Loss: 0.8751, Accuracy: 70.31%, Confidence: 0.7299\n",
      "Epoch [710/1000]\n",
      "  Training: Loss: 0.5804, Accuracy: 80.39%, Confidence: 0.7423\n",
      "  Testing:  Loss: 0.8911, Accuracy: 69.63%, Confidence: 0.7335\n",
      "Epoch [720/1000]\n",
      "  Training: Loss: 0.5746, Accuracy: 80.47%, Confidence: 0.7444\n",
      "  Testing:  Loss: 0.8780, Accuracy: 69.90%, Confidence: 0.7339\n",
      "Epoch [730/1000]\n",
      "  Training: Loss: 0.5758, Accuracy: 80.55%, Confidence: 0.7449\n",
      "  Testing:  Loss: 0.9422, Accuracy: 68.30%, Confidence: 0.7277\n",
      "Epoch [740/1000]\n",
      "  Training: Loss: 0.5729, Accuracy: 80.61%, Confidence: 0.7472\n",
      "  Testing:  Loss: 0.8815, Accuracy: 69.92%, Confidence: 0.7374\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_750.pt\n",
      "Epoch [750/1000]\n",
      "  Training: Loss: 0.5687, Accuracy: 80.68%, Confidence: 0.7469\n",
      "  Testing:  Loss: 0.9145, Accuracy: 69.05%, Confidence: 0.7271\n",
      "Epoch [760/1000]\n",
      "  Training: Loss: 0.5684, Accuracy: 80.76%, Confidence: 0.7485\n",
      "  Testing:  Loss: 0.8675, Accuracy: 70.18%, Confidence: 0.7307\n",
      "Epoch [770/1000]\n",
      "  Training: Loss: 0.5631, Accuracy: 80.88%, Confidence: 0.7500\n",
      "  Testing:  Loss: 0.8906, Accuracy: 70.04%, Confidence: 0.7317\n",
      "Epoch [780/1000]\n",
      "  Training: Loss: 0.5626, Accuracy: 80.92%, Confidence: 0.7501\n",
      "  Testing:  Loss: 0.8431, Accuracy: 71.01%, Confidence: 0.7405\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model (same as before)\n",
    "smaller_cnn_model = SmallerComparableCNN().to(device)\n",
    "\n",
    "# Continue training from epoch 400 to 1000\n",
    "metrics = continue_training_from_checkpoint(\n",
    "    model=smaller_cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    last_epoch=400,\n",
    "    target_epochs=1000,\n",
    "    checkpoint_dir='smaller_cnn_checkpoints',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training to epoch 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_checkpoint(checkpoint_dir='smaller_cnn_checkpoints'):\n",
    "    \"\"\"Find the most recent checkpoint file\"\"\"\n",
    "    checkpoints = []\n",
    "    for file in os.listdir(checkpoint_dir):\n",
    "        if file.startswith('cnn_model_epoch_') and file.endswith('.pt'):\n",
    "            epoch = int(file.split('_')[-1].replace('.pt', ''))\n",
    "            checkpoints.append(epoch)\n",
    "    \n",
    "    if not checkpoints:\n",
    "        raise FileNotFoundError(\"No checkpoints found in directory\")\n",
    "    \n",
    "    last_epoch = max(checkpoints)\n",
    "    return last_epoch, os.path.join(checkpoint_dir, f'cnn_model_epoch_{last_epoch}.pt')\n",
    "\n",
    "def continue_training(model, train_loader, test_loader, \n",
    "                     target_epochs=1500, \n",
    "                     checkpoint_dir='smaller_cnn_checkpoints',\n",
    "                     device='cuda'):\n",
    "    \"\"\"Continue training from last checkpoint to target epochs\"\"\"\n",
    "    \n",
    "    # Find and load the last checkpoint\n",
    "    last_epoch, checkpoint_path = find_last_checkpoint(checkpoint_dir)\n",
    "    print(f\"Found last checkpoint at epoch {last_epoch}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\"Continuing training from epoch {last_epoch} to {target_epochs}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(last_epoch, target_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            running_correct += (predicted == labels.squeeze()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = (running_correct / total_samples) * 100\n",
    "        \n",
    "        # Testing phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits, probabilities = model(inputs)\n",
    "                loss = criterion(logits, labels.squeeze())\n",
    "                \n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                test_correct += (predicted == labels.squeeze()).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = (test_correct / test_total) * 100\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{target_epochs}]')\n",
    "            print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    "            print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')\n",
    "        \n",
    "        # Save checkpoint every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cnn_model_epoch_{epoch+1}.pt')\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f'Checkpoint saved: {checkpoint_path}')\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found last checkpoint at epoch 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baris\\AppData\\Local\\Temp\\ipykernel_18584\\97861349.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing training from epoch 750 to 1500\n",
      "Epoch [750/1500]\n",
      "  Training: Loss: 0.5724, Accuracy: 80.51%\n",
      "  Testing:  Loss: 0.8450, Accuracy: 71.03%\n",
      "Epoch [760/1500]\n",
      "  Training: Loss: 0.5668, Accuracy: 81.00%\n",
      "  Testing:  Loss: 0.8788, Accuracy: 69.55%\n",
      "Epoch [770/1500]\n",
      "  Training: Loss: 0.5668, Accuracy: 80.84%\n",
      "  Testing:  Loss: 0.9401, Accuracy: 68.40%\n",
      "Epoch [780/1500]\n",
      "  Training: Loss: 0.5613, Accuracy: 80.88%\n",
      "  Testing:  Loss: 0.8488, Accuracy: 70.98%\n",
      "Epoch [790/1500]\n",
      "  Training: Loss: 0.5614, Accuracy: 81.03%\n",
      "  Testing:  Loss: 0.8679, Accuracy: 70.04%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_800.pt\n",
      "Epoch [800/1500]\n",
      "  Training: Loss: 0.5581, Accuracy: 81.08%\n",
      "  Testing:  Loss: 0.8693, Accuracy: 70.34%\n",
      "Epoch [810/1500]\n",
      "  Training: Loss: 0.5564, Accuracy: 81.08%\n",
      "  Testing:  Loss: 0.8695, Accuracy: 70.20%\n",
      "Epoch [820/1500]\n",
      "  Training: Loss: 0.5543, Accuracy: 81.17%\n",
      "  Testing:  Loss: 0.9483, Accuracy: 68.39%\n",
      "Epoch [830/1500]\n",
      "  Training: Loss: 0.5524, Accuracy: 81.25%\n",
      "  Testing:  Loss: 0.9440, Accuracy: 68.49%\n",
      "Epoch [840/1500]\n",
      "  Training: Loss: 0.5515, Accuracy: 81.37%\n",
      "  Testing:  Loss: 0.8852, Accuracy: 69.31%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_850.pt\n",
      "Epoch [850/1500]\n",
      "  Training: Loss: 0.5482, Accuracy: 81.26%\n",
      "  Testing:  Loss: 0.8934, Accuracy: 69.41%\n",
      "Epoch [860/1500]\n",
      "  Training: Loss: 0.5457, Accuracy: 81.46%\n",
      "  Testing:  Loss: 0.9492, Accuracy: 68.20%\n",
      "Epoch [870/1500]\n",
      "  Training: Loss: 0.5441, Accuracy: 81.51%\n",
      "  Testing:  Loss: 0.8660, Accuracy: 70.64%\n",
      "Epoch [880/1500]\n",
      "  Training: Loss: 0.5443, Accuracy: 81.40%\n",
      "  Testing:  Loss: 0.8546, Accuracy: 71.10%\n",
      "Epoch [890/1500]\n",
      "  Training: Loss: 0.5391, Accuracy: 81.80%\n",
      "  Testing:  Loss: 0.8800, Accuracy: 70.43%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_900.pt\n",
      "Epoch [900/1500]\n",
      "  Training: Loss: 0.5384, Accuracy: 81.64%\n",
      "  Testing:  Loss: 0.8989, Accuracy: 70.11%\n",
      "Epoch [910/1500]\n",
      "  Training: Loss: 0.5353, Accuracy: 81.82%\n",
      "  Testing:  Loss: 0.8790, Accuracy: 70.38%\n",
      "Epoch [920/1500]\n",
      "  Training: Loss: 0.5307, Accuracy: 82.05%\n",
      "  Testing:  Loss: 0.8785, Accuracy: 70.48%\n",
      "Epoch [930/1500]\n",
      "  Training: Loss: 0.5312, Accuracy: 81.94%\n",
      "  Testing:  Loss: 0.8769, Accuracy: 71.06%\n",
      "Epoch [940/1500]\n",
      "  Training: Loss: 0.5296, Accuracy: 81.88%\n",
      "  Testing:  Loss: 1.0077, Accuracy: 66.35%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_950.pt\n",
      "Epoch [950/1500]\n",
      "  Training: Loss: 0.5261, Accuracy: 82.20%\n",
      "  Testing:  Loss: 0.9366, Accuracy: 68.97%\n",
      "Epoch [960/1500]\n",
      "  Training: Loss: 0.5263, Accuracy: 82.17%\n",
      "  Testing:  Loss: 0.8757, Accuracy: 69.85%\n",
      "Epoch [970/1500]\n",
      "  Training: Loss: 0.5264, Accuracy: 82.03%\n",
      "  Testing:  Loss: 1.0846, Accuracy: 64.20%\n",
      "Epoch [980/1500]\n",
      "  Training: Loss: 0.5208, Accuracy: 82.43%\n",
      "  Testing:  Loss: 0.9871, Accuracy: 67.47%\n",
      "Epoch [990/1500]\n",
      "  Training: Loss: 0.5202, Accuracy: 82.38%\n",
      "  Testing:  Loss: 0.8618, Accuracy: 70.74%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1000.pt\n",
      "Epoch [1000/1500]\n",
      "  Training: Loss: 0.5174, Accuracy: 82.37%\n",
      "  Testing:  Loss: 0.8759, Accuracy: 70.42%\n",
      "Epoch [1010/1500]\n",
      "  Training: Loss: 0.5175, Accuracy: 82.42%\n",
      "  Testing:  Loss: 0.8733, Accuracy: 70.68%\n",
      "Epoch [1020/1500]\n",
      "  Training: Loss: 0.5187, Accuracy: 82.58%\n",
      "  Testing:  Loss: 0.8974, Accuracy: 69.90%\n",
      "Epoch [1030/1500]\n",
      "  Training: Loss: 0.5133, Accuracy: 82.48%\n",
      "  Testing:  Loss: 0.9395, Accuracy: 68.44%\n",
      "Epoch [1040/1500]\n",
      "  Training: Loss: 0.5127, Accuracy: 82.71%\n",
      "  Testing:  Loss: 0.8898, Accuracy: 69.81%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1050.pt\n",
      "Epoch [1050/1500]\n",
      "  Training: Loss: 0.5101, Accuracy: 82.53%\n",
      "  Testing:  Loss: 0.9702, Accuracy: 67.61%\n",
      "Epoch [1060/1500]\n",
      "  Training: Loss: 0.5100, Accuracy: 82.55%\n",
      "  Testing:  Loss: 0.9795, Accuracy: 67.64%\n",
      "Epoch [1070/1500]\n",
      "  Training: Loss: 0.5074, Accuracy: 82.72%\n",
      "  Testing:  Loss: 0.8931, Accuracy: 70.12%\n",
      "Epoch [1080/1500]\n",
      "  Training: Loss: 0.5077, Accuracy: 82.53%\n",
      "  Testing:  Loss: 0.9127, Accuracy: 69.54%\n",
      "Epoch [1090/1500]\n",
      "  Training: Loss: 0.5046, Accuracy: 82.91%\n",
      "  Testing:  Loss: 0.9664, Accuracy: 68.53%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1100.pt\n",
      "Epoch [1100/1500]\n",
      "  Training: Loss: 0.5038, Accuracy: 82.71%\n",
      "  Testing:  Loss: 0.9881, Accuracy: 67.58%\n",
      "Epoch [1110/1500]\n",
      "  Training: Loss: 0.5026, Accuracy: 82.81%\n",
      "  Testing:  Loss: 0.9336, Accuracy: 68.59%\n",
      "Epoch [1120/1500]\n",
      "  Training: Loss: 0.5014, Accuracy: 82.71%\n",
      "  Testing:  Loss: 0.8906, Accuracy: 70.23%\n",
      "Epoch [1130/1500]\n",
      "  Training: Loss: 0.4995, Accuracy: 82.85%\n",
      "  Testing:  Loss: 0.8904, Accuracy: 70.39%\n",
      "Epoch [1140/1500]\n",
      "  Training: Loss: 0.4977, Accuracy: 83.01%\n",
      "  Testing:  Loss: 0.9347, Accuracy: 69.58%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1150.pt\n",
      "Epoch [1150/1500]\n",
      "  Training: Loss: 0.4963, Accuracy: 83.01%\n",
      "  Testing:  Loss: 0.9037, Accuracy: 70.46%\n",
      "Epoch [1160/1500]\n",
      "  Training: Loss: 0.4929, Accuracy: 83.24%\n",
      "  Testing:  Loss: 0.9071, Accuracy: 69.59%\n",
      "Epoch [1170/1500]\n",
      "  Training: Loss: 0.4939, Accuracy: 83.17%\n",
      "  Testing:  Loss: 0.9621, Accuracy: 69.30%\n",
      "Epoch [1180/1500]\n",
      "  Training: Loss: 0.4923, Accuracy: 83.20%\n",
      "  Testing:  Loss: 0.9259, Accuracy: 69.58%\n",
      "Epoch [1190/1500]\n",
      "  Training: Loss: 0.4918, Accuracy: 83.09%\n",
      "  Testing:  Loss: 0.9010, Accuracy: 69.92%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1200.pt\n",
      "Epoch [1200/1500]\n",
      "  Training: Loss: 0.4889, Accuracy: 83.51%\n",
      "  Testing:  Loss: 0.9287, Accuracy: 69.70%\n",
      "Epoch [1210/1500]\n",
      "  Training: Loss: 0.4882, Accuracy: 83.26%\n",
      "  Testing:  Loss: 0.8874, Accuracy: 70.37%\n",
      "Epoch [1220/1500]\n",
      "  Training: Loss: 0.4873, Accuracy: 83.43%\n",
      "  Testing:  Loss: 0.9113, Accuracy: 69.56%\n",
      "Epoch [1230/1500]\n",
      "  Training: Loss: 0.4862, Accuracy: 83.33%\n",
      "  Testing:  Loss: 0.9532, Accuracy: 68.78%\n",
      "Epoch [1240/1500]\n",
      "  Training: Loss: 0.4846, Accuracy: 83.37%\n",
      "  Testing:  Loss: 0.8937, Accuracy: 70.60%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1250.pt\n",
      "Epoch [1250/1500]\n",
      "  Training: Loss: 0.4828, Accuracy: 83.41%\n",
      "  Testing:  Loss: 0.9350, Accuracy: 68.94%\n",
      "Epoch [1260/1500]\n",
      "  Training: Loss: 0.4820, Accuracy: 83.67%\n",
      "  Testing:  Loss: 0.9430, Accuracy: 69.49%\n",
      "Epoch [1270/1500]\n",
      "  Training: Loss: 0.4827, Accuracy: 83.57%\n",
      "  Testing:  Loss: 0.8928, Accuracy: 70.63%\n",
      "Epoch [1280/1500]\n",
      "  Training: Loss: 0.4775, Accuracy: 83.52%\n",
      "  Testing:  Loss: 0.9453, Accuracy: 69.59%\n",
      "Epoch [1290/1500]\n",
      "  Training: Loss: 0.4769, Accuracy: 83.66%\n",
      "  Testing:  Loss: 0.9298, Accuracy: 69.57%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1300.pt\n",
      "Epoch [1300/1500]\n",
      "  Training: Loss: 0.4761, Accuracy: 83.54%\n",
      "  Testing:  Loss: 0.9787, Accuracy: 68.34%\n",
      "Epoch [1310/1500]\n",
      "  Training: Loss: 0.4767, Accuracy: 83.67%\n",
      "  Testing:  Loss: 0.9374, Accuracy: 69.71%\n",
      "Epoch [1320/1500]\n",
      "  Training: Loss: 0.4757, Accuracy: 83.74%\n",
      "  Testing:  Loss: 0.9216, Accuracy: 69.95%\n",
      "Epoch [1330/1500]\n",
      "  Training: Loss: 0.4733, Accuracy: 83.89%\n",
      "  Testing:  Loss: 0.9655, Accuracy: 68.68%\n",
      "Epoch [1340/1500]\n",
      "  Training: Loss: 0.4721, Accuracy: 83.67%\n",
      "  Testing:  Loss: 0.9445, Accuracy: 69.22%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1350.pt\n",
      "Epoch [1350/1500]\n",
      "  Training: Loss: 0.4724, Accuracy: 83.71%\n",
      "  Testing:  Loss: 0.9323, Accuracy: 69.40%\n",
      "Epoch [1360/1500]\n",
      "  Training: Loss: 0.4702, Accuracy: 84.01%\n",
      "  Testing:  Loss: 0.9487, Accuracy: 68.91%\n",
      "Epoch [1370/1500]\n",
      "  Training: Loss: 0.4692, Accuracy: 83.85%\n",
      "  Testing:  Loss: 0.9532, Accuracy: 69.60%\n",
      "Epoch [1380/1500]\n",
      "  Training: Loss: 0.4677, Accuracy: 83.95%\n",
      "  Testing:  Loss: 0.9600, Accuracy: 68.80%\n",
      "Epoch [1390/1500]\n",
      "  Training: Loss: 0.4652, Accuracy: 84.05%\n",
      "  Testing:  Loss: 1.0144, Accuracy: 67.83%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1400.pt\n",
      "Epoch [1400/1500]\n",
      "  Training: Loss: 0.4654, Accuracy: 84.02%\n",
      "  Testing:  Loss: 0.9450, Accuracy: 69.80%\n",
      "Epoch [1410/1500]\n",
      "  Training: Loss: 0.4655, Accuracy: 84.00%\n",
      "  Testing:  Loss: 0.9284, Accuracy: 70.06%\n",
      "Epoch [1420/1500]\n",
      "  Training: Loss: 0.4619, Accuracy: 84.20%\n",
      "  Testing:  Loss: 0.9711, Accuracy: 69.00%\n",
      "Epoch [1430/1500]\n",
      "  Training: Loss: 0.4624, Accuracy: 84.10%\n",
      "  Testing:  Loss: 0.9638, Accuracy: 69.24%\n",
      "Epoch [1440/1500]\n",
      "  Training: Loss: 0.4590, Accuracy: 84.37%\n",
      "  Testing:  Loss: 0.9661, Accuracy: 69.19%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1450.pt\n",
      "Epoch [1450/1500]\n",
      "  Training: Loss: 0.4612, Accuracy: 84.14%\n",
      "  Testing:  Loss: 1.0013, Accuracy: 67.95%\n",
      "Epoch [1460/1500]\n",
      "  Training: Loss: 0.4581, Accuracy: 84.26%\n",
      "  Testing:  Loss: 1.0088, Accuracy: 68.06%\n",
      "Epoch [1470/1500]\n",
      "  Training: Loss: 0.4603, Accuracy: 84.07%\n",
      "  Testing:  Loss: 0.9593, Accuracy: 69.03%\n",
      "Epoch [1480/1500]\n",
      "  Training: Loss: 0.4576, Accuracy: 84.27%\n",
      "  Testing:  Loss: 1.0616, Accuracy: 67.26%\n",
      "Epoch [1490/1500]\n",
      "  Training: Loss: 0.4562, Accuracy: 84.37%\n",
      "  Testing:  Loss: 0.9437, Accuracy: 69.51%\n",
      "Checkpoint saved: smaller_cnn_checkpoints\\cnn_model_epoch_1500.pt\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "smaller_cnn_model = SmallerComparableCNN().to(device)\n",
    "\n",
    "# Continue training\n",
    "model = continue_training(\n",
    "    model=smaller_cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    target_epochs=1500,\n",
    "    checkpoint_dir='smaller_cnn_checkpoints',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
