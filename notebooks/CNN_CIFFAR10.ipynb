{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from keras.datasets import cifar10\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation\n",
    "# Assuming the model classes are defined in `model.py`\n",
    "from model import ImageMamba, ModelArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Reshape and preprocess the CIFAR-10 dataset for PyTorch models\n",
    "X_train = X_train.transpose(0, 3, 1, 2)  # Shape: (batch_size, channels, height, width)\n",
    "X_test = X_test.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Convert data to float and normalize pixel values in the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the train/test data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Your existing train loader code\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Add test loader\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # Note: shuffle=False for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class ComparableCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComparableCNN, self).__init__()\n",
    "        # First conv block - similar to MAMBA's first conv\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Second conv block - similar to MAMBA's second conv\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Additional conv layers to match MAMBA's 4 layers\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Global average pooling and final dense layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Fourth block\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.fc(x)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        return logits, probabilities\n",
    "\n",
    "def load_training_metrics(metrics_path):\n",
    "    \"\"\"Load training metrics from JSON file\"\"\"\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "        # Convert lists back to numpy arrays where needed\n",
    "        metrics['epoch_train_confidences'] = [np.array(arr) for arr in metrics['epoch_train_confidences']]\n",
    "        metrics['epoch_test_confidences'] = [np.array(arr) for arr in metrics['epoch_test_confidences']]\n",
    "    return metrics\n",
    "\n",
    "def save_cnn_checkpoint(model, optimizer, epoch, metrics, path):\n",
    "    \"\"\"Save CNN model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f'Checkpoint saved: {path}')\n",
    "\n",
    "def train_evaluate_cnn(model, train_loader, test_loader, num_epochs=100, device='cuda', \n",
    "                      checkpoint_dir='cnn_checkpoints'):\n",
    "    \"\"\"Train and evaluate CNN with checkpointing\"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_losses': [], 'test_losses': [],\n",
    "        'train_accuracies': [], 'test_accuracies': [],\n",
    "        'train_confidences': [], 'test_confidences': [],\n",
    "        'epoch_train_confidences': [], 'epoch_test_confidences': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        train_confidence_sum = 0\n",
    "        train_epoch_confidences = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            running_correct += (predicted == labels.squeeze()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            train_confidence_sum += confidence.sum().item()\n",
    "            train_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = (running_correct / total_samples) * 100\n",
    "        train_avg_confidence = train_confidence_sum / total_samples\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_confidence_sum = 0\n",
    "        test_epoch_confidences = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits, probabilities = model(inputs)\n",
    "                loss = criterion(logits, labels.squeeze())\n",
    "                \n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                confidence, _ = torch.max(probabilities, 1)\n",
    "                test_correct += (predicted == labels.squeeze()).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "                test_confidence_sum += confidence.sum().item()\n",
    "                test_epoch_confidences.extend(confidence.detach().cpu().numpy())\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = (test_correct / test_total) * 100\n",
    "        test_avg_confidence = test_confidence_sum / test_total\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['train_losses'].append(train_loss)\n",
    "        metrics['test_losses'].append(test_loss)\n",
    "        metrics['train_accuracies'].append(train_accuracy)\n",
    "        metrics['test_accuracies'].append(test_accuracy)\n",
    "        metrics['train_confidences'].append(train_avg_confidence)\n",
    "        metrics['test_confidences'].append(test_avg_confidence)\n",
    "        metrics['epoch_train_confidences'].append(train_epoch_confidences)\n",
    "        metrics['epoch_test_confidences'].append(test_epoch_confidences)\n",
    "        \n",
    "        # Save metrics to JSON file after each epoch\n",
    "        metrics_path = os.path.join(checkpoint_dir, 'training_metrics.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            # Convert to float with reduced precision (4 decimal places)\n",
    "            json_metrics = {\n",
    "                'train_losses': [round(float(x), 4) for x in metrics['train_losses']],\n",
    "                'test_losses': [round(float(x), 4) for x in metrics['test_losses']],\n",
    "                'train_accuracies': [round(float(x), 4) for x in metrics['train_accuracies']],\n",
    "                'test_accuracies': [round(float(x), 4) for x in metrics['test_accuracies']],\n",
    "                'train_confidences': [round(float(x), 4) for x in metrics['train_confidences']],\n",
    "                'test_confidences': [round(float(x), 4) for x in metrics['test_confidences']],\n",
    "                'epoch_train_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) else round(float(arr), 4)\n",
    "                                          for arr in metrics['epoch_train_confidences']],\n",
    "                'epoch_test_confidences': [[round(float(x), 4) for x in arr] if isinstance(arr, (np.ndarray, list)) else round(float(arr), 4)\n",
    "                                         for arr in metrics['epoch_test_confidences']],\n",
    "                'current_epoch': epoch + 1\n",
    "            }\n",
    "            json.dump(json_metrics, f, indent=4)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}]')\n",
    "            print(f'  Training: Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "            print(f'  Testing:  Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "        \n",
    "        # Save checkpoint every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cnn_model_epoch_{epoch+1}.pt')\n",
    "            save_cnn_checkpoint(model, optimizer, epoch, metrics, checkpoint_path)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/11]\n",
      "  Training: Loss: 1.4983, Accuracy: 48.75%, Confidence: 0.3584\n",
      "  Testing:  Loss: 1.2561, Accuracy: 57.00%, Confidence: 0.4621\n",
      "Epoch [10/11]\n",
      "  Training: Loss: 0.5373, Accuracy: 81.76%, Confidence: 0.7680\n",
      "  Testing:  Loss: 0.7226, Accuracy: 75.19%, Confidence: 0.7684\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and data loaders\n",
    "cnn_model = ComparableCNN().to(device)\n",
    "checkpoint_dir = 'cnn_checkpoints'\n",
    "\n",
    "# Train model with checkpointing\n",
    "metrics = train_evaluate_cnn(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=11,  # matching your MAMBA training\n",
    "    device=device,\n",
    "    checkpoint_dir=checkpoint_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
