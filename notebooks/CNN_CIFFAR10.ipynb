{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from data_loader import load_cifar10, get_class_names\n",
    "from check_and_measure import evaluate_model, save_checkpoint, load_last_checkpoint# For consistent data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, X_train, X_test, Y_train, Y_test = load_cifar10(batch_size=64, seed=42)\n",
    "class_names = get_class_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SmallerComparableCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, test_loader, num_epochs=2000, device='cuda',\n",
    "              checkpoint_dir='cnn_checkpoints', checkpoint_freq=100):\n",
    "    \"\"\"Train CNN model with comprehensive metrics tracking.\"\"\"\n",
    "    \n",
    "    # Check if directory exists and contains files\n",
    "    if os.path.exists(checkpoint_dir) and os.listdir(checkpoint_dir):\n",
    "        raise RuntimeError(\n",
    "            f\"Directory {checkpoint_dir} already contains files. Using this directory would overwrite \"\n",
    "            \"existing training data. To prevent data loss, please use an empty directory \"\n",
    "            \"or use continue_training() to resume from the last checkpoint.\"\n",
    "        )\n",
    "        \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.1)  # Same as MAMBA\n",
    "    \n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'train_losses': [], 'test_losses': [],\n",
    "        'train_accuracies': [], 'test_accuracies': [],\n",
    "        'train_confidences': [], 'test_confidences': [],\n",
    "        'epoch_train_confidences': [], 'epoch_test_confidences': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        train_confidences = []\n",
    "\n",
    "        tqdm_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', disable=(epoch % 100 != 0))\n",
    "        for inputs, labels in tqdm_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            train_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * running_correct / total_samples\n",
    "        train_avg_confidence = np.mean(train_confidences)\n",
    "        \n",
    "        test_loss, test_accuracy, test_avg_confidence, test_confidences = evaluate_model(\n",
    "            model, test_loader, criterion, device)\n",
    "        \n",
    "        metrics['train_losses'].append(train_loss)\n",
    "        metrics['test_losses'].append(test_loss)\n",
    "        metrics['train_accuracies'].append(train_accuracy)\n",
    "        metrics['test_accuracies'].append(test_accuracy)\n",
    "        metrics['train_confidences'].append(train_avg_confidence)\n",
    "        metrics['test_confidences'].append(test_avg_confidence)\n",
    "        metrics['epoch_train_confidences'].append(train_confidences)\n",
    "        metrics['epoch_test_confidences'].append(test_confidences)\n",
    "\n",
    "        # print once every 400 epochs\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "            print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_freq == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pt')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, metrics, checkpoint_path)\n",
    "            \n",
    "            metrics_path = os.path.join(checkpoint_dir, 'training_metrics.json')\n",
    "            json_metrics = {\n",
    "                'train_losses': [float(x) for x in metrics['train_losses']],\n",
    "                'test_losses': [float(x) for x in metrics['test_losses']],\n",
    "                'train_accuracies': [float(x) for x in metrics['train_accuracies']],\n",
    "                'test_accuracies': [float(x) for x in metrics['test_accuracies']],\n",
    "                'train_confidences': [float(x) for x in metrics['train_confidences']],\n",
    "                'test_confidences': [float(x) for x in metrics['test_confidences']],\n",
    "                'current_epoch': epoch + 1\n",
    "            }\n",
    "            with open(metrics_path, 'w') as f:\n",
    "                json.dump(json_metrics, f, indent=4)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200: 100%|██████████| 782/782 [00:25<00:00, 30.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1200]\n",
      "Train Loss: 1.1683, Accuracy: 59.82%, Confidence: 0.5118\n",
      "Test Loss: 1.2565, Accuracy: 55.50%, Confidence: 0.5101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/1200: 100%|██████████| 782/782 [00:03<00:00, 204.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1200]\n",
      "Train Loss: 1.1099, Accuracy: 62.29%, Confidence: 0.5300\n",
      "Test Loss: 1.3378, Accuracy: 52.64%, Confidence: 0.5437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 201/1200: 100%|██████████| 782/782 [00:04<00:00, 181.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1200]\n",
      "Train Loss: 1.1047, Accuracy: 62.46%, Confidence: 0.5301\n",
      "Test Loss: 1.3079, Accuracy: 52.26%, Confidence: 0.5635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 301/1200: 100%|██████████| 782/782 [00:05<00:00, 131.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/1200]\n",
      "Train Loss: 1.1046, Accuracy: 62.50%, Confidence: 0.5308\n",
      "Test Loss: 1.2681, Accuracy: 55.30%, Confidence: 0.5291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 401/1200: 100%|██████████| 782/782 [00:09<00:00, 84.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/1200]\n",
      "Train Loss: 1.0972, Accuracy: 62.59%, Confidence: 0.5337\n",
      "Test Loss: 1.2974, Accuracy: 54.68%, Confidence: 0.5523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 501/1200: 100%|██████████| 782/782 [00:09<00:00, 86.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [600/1200]\n",
      "Train Loss: 1.0886, Accuracy: 63.09%, Confidence: 0.5364\n",
      "Test Loss: 1.1685, Accuracy: 59.14%, Confidence: 0.5376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 601/1200: 100%|██████████| 782/782 [00:09<00:00, 80.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [700/1200]\n",
      "Train Loss: 1.0735, Accuracy: 63.70%, Confidence: 0.5421\n",
      "Test Loss: 1.2357, Accuracy: 56.04%, Confidence: 0.5318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 701/1200: 100%|██████████| 782/782 [00:08<00:00, 88.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [800/1200]\n",
      "Train Loss: 1.0502, Accuracy: 64.51%, Confidence: 0.5469\n",
      "Test Loss: 1.1048, Accuracy: 62.11%, Confidence: 0.5550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 801/1200: 100%|██████████| 782/782 [00:08<00:00, 94.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [900/1200]\n",
      "Train Loss: 1.0248, Accuracy: 65.63%, Confidence: 0.5549\n",
      "Test Loss: 1.0619, Accuracy: 63.32%, Confidence: 0.5532\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:603] . unexpected pos 64 vs 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\torch\\serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 850\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\torch\\serialization.py:1090\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m   1089\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m-> 1090\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_value\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;66;03m# Write byte order marker\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:783] . PytorchStreamWriter failed writing file data.pkl: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 83\u001b[0m, in \u001b[0;36mtrain_cnn\u001b[1;34m(model, train_loader, test_loader, num_epochs, device, checkpoint_dir, checkpoint_freq)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m checkpoint_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     82\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m     \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     metrics_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_metrics.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m     json_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_losses\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     94\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\notebooks\\check_and_measure.py:14\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[1;34m(model, optimizer, scheduler, epoch, metrics, filepath)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save model checkpoint and metrics.\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics\n\u001b[0;32m     13\u001b[0m }\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\torch\\serialization.py:857\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    850\u001b[0m         _save(\n\u001b[0;32m    851\u001b[0m             obj,\n\u001b[0;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    856\u001b[0m         )\n\u001b[1;32m--> 857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n",
      "File \u001b[1;32mc:\\Users\\baris\\masterthesis\\MAMBARIS\\.conda\\lib\\site-packages\\torch\\serialization.py:690\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 64 vs 0"
     ]
    }
   ],
   "source": [
    "# Initialize model and set device\n",
    "model = SmallerComparableCNN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Train model\n",
    "metrics = train_cnn(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=1200,\n",
    "    device=device,\n",
    "    checkpoint_freq=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_cnn_training(model, train_loader, test_loader, checkpoint_dir, target_epochs=2000, device='cuda'):\n",
    "    \"\"\"Continue training from last checkpoint.\"\"\"\n",
    "    checkpoint, last_epoch = load_last_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        epochs=target_epochs - last_epoch,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    if checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Load existing metrics\n",
    "    with open(os.path.join(checkpoint_dir, 'training_metrics.json'), 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    complete_metrics = {\n",
    "        'train_losses': metrics['train_losses'],\n",
    "        'test_losses': metrics['test_losses'],\n",
    "        'train_accuracies': metrics['train_accuracies'],\n",
    "        'test_accuracies': metrics['test_accuracies'],\n",
    "        'train_confidences': metrics['train_confidences'],\n",
    "        'test_confidences': metrics['test_confidences'],\n",
    "        'epoch_train_confidences': checkpoint['metrics']['epoch_train_confidences'],\n",
    "        'epoch_test_confidences': checkpoint['metrics']['epoch_test_confidences']\n",
    "    }\n",
    "    \n",
    "    print(f\"Continuing training from epoch {last_epoch} to {target_epochs}\")\n",
    "    \n",
    "    for epoch in range(last_epoch, target_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        train_confidences = []\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch [{epoch+1}/{target_epochs}]'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits, probabilities = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            confidence, _ = torch.max(probabilities, 1)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            train_confidences.extend(confidence.detach().cpu().numpy())\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * running_correct / total_samples\n",
    "        train_avg_confidence = np.mean(train_confidences)\n",
    "        \n",
    "        test_loss, test_accuracy, test_avg_confidence, test_confidences = evaluate_model(\n",
    "            model, test_loader, criterion, device)\n",
    "        \n",
    "        complete_metrics['train_losses'].append(train_loss)\n",
    "        complete_metrics['test_losses'].append(test_loss)\n",
    "        complete_metrics['train_accuracies'].append(train_accuracy)\n",
    "        complete_metrics['test_accuracies'].append(test_accuracy)\n",
    "        complete_metrics['train_confidences'].append(train_avg_confidence)\n",
    "        complete_metrics['test_confidences'].append(test_avg_confidence)\n",
    "        complete_metrics['epoch_train_confidences'].append(train_confidences)\n",
    "        complete_metrics['epoch_test_confidences'].append(test_confidences)\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Confidence: {train_avg_confidence:.4f}')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Confidence: {test_avg_confidence:.4f}')\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cnn_model_epoch_{epoch+1}.pt')\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, complete_metrics, checkpoint_path)\n",
    "            \n",
    "            json_metrics = {\n",
    "                'train_losses': [float(x) for x in complete_metrics['train_losses']],\n",
    "                'test_losses': [float(x) for x in complete_metrics['test_losses']],\n",
    "                'train_accuracies': [float(x) for x in complete_metrics['train_accuracies']],\n",
    "                'test_accuracies': [float(x) for x in complete_metrics['test_accuracies']],\n",
    "                'train_confidences': [float(x) for x in complete_metrics['train_confidences']],\n",
    "                'test_confidences': [float(x) for x in complete_metrics['test_confidences']],\n",
    "                'current_epoch': epoch + 1\n",
    "            }\n",
    "            with open(os.path.join(checkpoint_dir, 'training_metrics.json'), 'w') as f:\n",
    "                json.dump(json_metrics, f, indent=4)\n",
    "    \n",
    "    return complete_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "model = SmallerComparableCNN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Find the last checkpoint\n",
    "checkpoint_dir = 'cnn_checkpoints'  \n",
    "with open(f'{checkpoint_dir}/training_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "print(f\"Last completed epoch: {metrics['current_epoch']}\")\n",
    "\n",
    "# Continue training\n",
    "metrics = continue_cnn_training(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    target_epochs=1200,  \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
