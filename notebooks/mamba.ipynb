{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nursery dataset\n",
    "Was originally created to rank and evaluate nursery school applications. \n",
    "So an application with for example, a family that is financially stable, has good housing, and has no social or health problems would be classified as priority.\n",
    "And applications that may involve severe financial, social, or health issues that make it highly unlikely for the application to be accepted, would be classified as\n",
    "\n",
    "Dit weet ik niet zeker??..\n",
    "\n",
    "So the ranking from best to worst is:\n",
    "\n",
    "Very Recommended > Recommended > Priority > Special Priority > Not Recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 76, 'name': 'Nursery', 'repository_url': 'https://archive.ics.uci.edu/dataset/76/nursery', 'data_url': 'https://archive.ics.uci.edu/static/public/76/data.csv', 'abstract': ' Nursery Database was derived from a hierarchical decision model originally developed to rank applications for nursery schools.', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 12960, 'num_features': 8, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1989, 'last_updated': 'Sun Jan 14 2024', 'dataset_doi': '10.24432/C5P88W', 'creators': ['Vladislav Rajkovic'], 'intro_paper': {'ID': 372, 'type': 'NATIVE', 'title': 'An application for admission in public school systems', 'authors': 'M. Olave, V. Rajkovic, M. Bohanec', 'venue': 'Expert Systems in Public Administration', 'year': 1989, 'journal': None, 'DOI': None, 'URL': 'https://www.academia.edu/16670755/An_application_for_admission_in_public_school_systems', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': \"Nursery Database was derived from a hierarchical decision model originally developed to rank applications for nursery schools. It was used during several years in 1980's when there was excessive enrollment to these schools in Ljubljana, Slovenia, and the rejected applications frequently needed an objective explanation. The final decision depended on three subproblems: occupation of parents and child's nursery, family structure and financial standing, and social and health picture of the family. The model was developed within expert system shell for decision making DEX (M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.).\\r\\n\\r\\nThe hierarchical model ranks nursery-school applications according to the following concept structure:\\r\\n\\r\\n NURSERY            Evaluation of applications for nursery schools\\r\\n . EMPLOY           Employment of parents and child's nursery\\r\\n . . parents        Parents' occupation\\r\\n . . has_nurs       Child's nursery\\r\\n . STRUCT_FINAN     Family structure and financial standings\\r\\n . . STRUCTURE      Family structure\\r\\n . . . form         Form of the family\\r\\n . . . children     Number of children\\r\\n . . housing        Housing conditions\\r\\n . . finance        Financial standing of the family\\r\\n . SOC_HEALTH       Social and health picture of the family\\r\\n . . social         Social conditions\\r\\n . . health         Health conditions\\r\\n\\r\\nInput attributes are printed in lowercase. Besides the target concept (NURSERY) the model includes four intermediate concepts: EMPLOY, STRUCT_FINAN, STRUCTURE, SOC_HEALTH. Every concept is in the original model related to its lower level descendants by a set of examples (for these examples sets see http://www-ai.ijs.si/BlazZupan/nursery.html).\\r\\n\\r\\nThe Nursery Database contains examples with the structural information removed, i.e., directly relates NURSERY to the eight input attributes: parents, has_nurs, form, children, housing, finance, social, health.\\r\\n\\r\\nBecause of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '   parents:        usual, pretentious, great_pret\\r\\n   has_nurs:       proper, less_proper, improper, critical, very_crit\\r\\n   form:           complete, completed, incomplete, foster\\r\\n   children:       1, 2, 3, more\\r\\n   housing:        convenient, less_conv, critical\\r\\n   finance:        convenient, inconv\\r\\n   social:         non-prob, slightly_prob, problematic\\r\\n   health:         recommended, priority, not_recom', 'citation': None}}\n",
      "       name     role         type demographic  \\\n",
      "0   parents  Feature  Categorical        None   \n",
      "1  has_nurs  Feature  Categorical        None   \n",
      "2      form  Feature  Categorical        None   \n",
      "3  children  Feature  Categorical        None   \n",
      "4   housing  Feature  Categorical        None   \n",
      "5   finance  Feature  Categorical        None   \n",
      "6    social  Feature  Categorical        None   \n",
      "7    health  Feature  Categorical        None   \n",
      "8     class   Target  Categorical        None   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0                     usual, pretentious, great_pret  None             no  \n",
      "1  proper, less_proper, improper, critical, very_...  None             no  \n",
      "2            complete, completed, incomplete, foster  None             no  \n",
      "3                                      1, 2, 3, more  None             no  \n",
      "4                    convenient, less_conv, critical  None             no  \n",
      "5                                 convenient, inconv  None             no  \n",
      "6               non-prob, slightly_prob, problematic  None             no  \n",
      "7                   recommended, priority, not_recom  None             no  \n",
      "8                   recommended, priority, not_recom  None             no  \n",
      "\n",
      "The first 5 rows of the dataset:\n",
      "  parents has_nurs      form children     housing     finance         social  \\\n",
      "0   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "1   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "2   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "3   usual   proper  complete        1  convenient  convenient  slightly_prob   \n",
      "4   usual   proper  complete        1  convenient  convenient  slightly_prob   \n",
      "\n",
      "        health  \n",
      "0  recommended  \n",
      "1     priority  \n",
      "2    not_recom  \n",
      "3  recommended  \n",
      "4     priority  \n",
      "\n",
      "Possible target classes:\n",
      "['recommend' 'priority' 'not_recom' 'very_recom' 'spec_prior']\n"
     ]
    }
   ],
   "source": [
    "# ucimlrepo is a tool that provides easy access to datasets hosted on the UCI Machine Learning Repository\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset from UCI repository, which has ud 76\n",
    "nursery = fetch_ucirepo(id=76)\n",
    "\n",
    "# Display metadata and variable information\n",
    "print(nursery.metadata) # metadata \n",
    "print(nursery.variables) # variable information \n",
    "print(\"\\n\"+ \"The first 5 rows of the dataset:\")\n",
    "print(nursery.data.features.head())  # Display first 5 rows of features\n",
    "# Show the target variable, possibilities\n",
    "\n",
    "# Show the target variable and its unique possibilities\n",
    "unique_targets = nursery.data.targets['class'].unique()  # Access the 'class' column and get unique values\n",
    "print(\"\\nPossible target classes:\")\n",
    "print(unique_targets)  # Display unique target classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The datta will be preprocessed, and converted into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original  values (before encoding):\n",
      "  parents has_nurs      form children     housing     finance         social  \\\n",
      "1   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "2   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "3   usual   proper  complete        1  convenient  convenient  slightly_prob   \n",
      "\n",
      "        health  \n",
      "1     priority  \n",
      "2    not_recom  \n",
      "3  recommended  \n",
      "       class\n",
      "1   priority\n",
      "2  not_recom\n",
      "3  recommend\n",
      "\n",
      "Encoded target values (after encoding):\n",
      "   parents  has_nurs  form  children  housing  finance  social  health\n",
      "1        2         3     0         0        0        0       0       1\n",
      "2        2         3     0         0        0        0       0       0\n",
      "3        2         3     0         0        0        0       2       2\n",
      "[1 0 2]\n",
      "\n",
      "Sample of training data tensor:\n",
      "tensor([[2., 0., 2., 1., 1., 0., 2., 1.],\n",
      "        [0., 4., 3., 3., 0., 0., 0., 0.]])\n",
      "\n",
      "Sample of training target tensor:\n",
      "tensor([3, 0])\n",
      "\n",
      "Sample of testing data tensor:\n",
      "tensor([[1., 1., 1., 2., 2., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 0., 1.]])\n",
      "\n",
      "Sample of testing target tensor:\n",
      "tensor([0, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X = nursery.data.features  # These are all the feature columns in the dataset\n",
    "Y = nursery.data.targets  # This is the target column in the dataset\n",
    "print(\"\\nOriginal  values (before encoding):\")\n",
    "print(X[1:4])  # Display a sample of the feature values\n",
    "print(Y[1:4])  # Display a sample of the target values\n",
    "\n",
    "# In case of future errors: Y = Y.values.ravel()  # Flatten Y to make it a 1D array if needed\n",
    "label_encoder = LabelEncoder()  # Used to encode the categorical target variables into numerical values\n",
    "X = X.apply(label_encoder.fit_transform)  # Encode the feature variables (X)\n",
    "Y = label_encoder.fit_transform(Y)  # Encode the target variable (Y)\n",
    "\n",
    "print(\"\\nEncoded target values (after encoding):\")\n",
    "print(X[1:4])  # Display the encoded feature values\n",
    "print(Y[1:4])  # Display the encoded target values\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the train/test data into PyTorch tensors\n",
    "# We must do this because PyTorch models only accept tensors as input\n",
    "# Both the MambaClassifier and Mamba classes inherit from torch.nn.Module\n",
    "# which is the base class for all neural network modules in PyTorch.\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Lets see how these tensors look like\n",
    "print(\"\\nSample of training data tensor:\")\n",
    "print(X_train_tensor[0:2])  # Display a sample of the training data tensor\n",
    "print(\"\\nSample of training target tensor:\")\n",
    "print(Y_train_tensor[0:2])  # Display a sample of the training target tensor\n",
    "print(\"\\nSample of testing data tensor:\")\n",
    "print(X_test_tensor[0:2])  # Display a sample of the testing data tensor\n",
    "print(\"\\nSample of testing target tensor:\")\n",
    "print(Y_test_tensor[0:2])  # Display a sample of the testing target tensor\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "# DataLoader to help in batch processing during model training/testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MAMBARIS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, repeat, einsum\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMAMBARIS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMSNorm, ResidualBlock\n\u001b[1;32m     12\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelArgs\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     d_model: \u001b[38;5;28mint\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MAMBARIS'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import json\n",
    "from typing import Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat, einsum\n",
    "\n",
    "from MAMBARIS.models.model import RMSNorm, ResidualBlock\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    d_state: int = 16\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "    d_conv: int = 4 \n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        \n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "            \n",
    "        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n",
    "            self.vocab_size += (self.pad_vocab_size_multiple\n",
    "                                - self.vocab_size % self.pad_vocab_size_multiple)\n",
    "\n",
    "class Mambasss(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Full Mamba model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
    "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "        self.norm_f = RMSNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
    "                                                     # See \"Weight Tying\" paper\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "    \n",
    "        Returns:\n",
    "            logits: shape (b, l, vocab_size)\n",
    "\n",
    "        Official Implementation:\n",
    "            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Define model parameters\n",
    "input_size = X_train.shape[1]  # Number of input features\n",
    "num_classes = len(nursery.data.targets['class'].unique())  # Number of target classes\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "\n",
    "# Instantiate the Mamba model\n",
    "model = Mambasss(input_size=input_size, num_classes=num_classes, d_model=d_model, n_layer=n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MambaClassifier for tabular data\n",
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, d_model=64, n_layer=4):\n",
    "        super(MambaClassifier, self).__init__()\n",
    "        \n",
    "        # Initial Linear Layer to map tabular data to a higher-dimensional space\n",
    "        self.initial_fc = nn.Linear(input_size, d_model)\n",
    "\n",
    "        # Simulate Mamba-like architecture using transformer blocks\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through initial fully connected layer\n",
    "        x = self.initial_fc(x)\n",
    "\n",
    "        # Pass through transformer-like blocks\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        # Classification head\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n",
    "# Define model parameters\n",
    "input_size = X_train.shape[1]  # Number of input features\n",
    "num_classes = len(nursery.data.targets['class'].unique())  # Number of target classes\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "\n",
    "# Instantiate the Mamba model\n",
    "model = MambaClassifier(input_size=input_size, num_classes=num_classes, d_model=d_model, n_layer=n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_state=16,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        dt_rank=\"auto\",\n",
    "        dt_min=0.001,\n",
    "        dt_max=0.1,\n",
    "        dt_init=\"random\",\n",
    "        dt_scale=1.0,\n",
    "        dt_init_floor=1e-4,\n",
    "        conv_bias=True,\n",
    "        bias=False,\n",
    "        use_fast_path=True,  # Fused kernel options\n",
    "        layer_idx=None,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "        self.use_fast_path = use_fast_path\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            bias=conv_bias,\n",
    "            kernel_size=d_conv,\n",
    "            groups=self.d_inner,\n",
    "            padding=d_conv - 1,\n",
    "            **factory_kwargs,\n",
    "        )\n",
    "\n",
    "        self.activation = \"silu\"\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.x_proj = nn.Linear(\n",
    "            self.d_inner, self.dt_rank + self.d_state * 2, bias=False, **factory_kwargs\n",
    "        )\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True, **factory_kwargs)\n",
    "\n",
    "        # Initialize special dt projection to preserve variance at initialization\n",
    "        dt_init_std = self.dt_rank**-0.5 * dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))\n",
    "            + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        self.dt_proj.bias._no_reinit = True\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = repeat(\n",
    "            torch.arange(1, self.d_state + 1, dtype=torch.float32, device=device),\n",
    "            \"n -> d n\",\n",
    "            d=self.d_inner,\n",
    "        ).contiguous()\n",
    "        A_log = torch.log(A)  # Keep A_log in fp32\n",
    "        self.A_log = nn.Parameter(A_log)\n",
    "        self.A_log._no_weight_decay = True\n",
    "\n",
    "        # D \"skip\" parameter\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner, device=device))  # Keep in fp32\n",
    "        self.D._no_weight_decay = True\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)\n",
    "\n",
    "    def forward(self, hidden_states, inference_params=None):\n",
    "        \"\"\"\n",
    "        hidden_states: (B, L, D)\n",
    "        Returns: same shape as hidden_states\n",
    "        \"\"\"\n",
    "        batch, seqlen, dim = hidden_states.shape\n",
    "\n",
    "        conv_state, ssm_state = None, None\n",
    "        if inference_params is not None:\n",
    "            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)\n",
    "            if inference_params.seqlen_offset > 0:\n",
    "                # The states are updated inplace\n",
    "                out, _, _ = self.step(hidden_states, conv_state, ssm_state)\n",
    "                return out\n",
    "\n",
    "        # We do matmul and transpose BLH -> HBL at the same time\n",
    "        xz = rearrange(\n",
    "            self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\n",
    "            \"d (b l) -> b d l\",\n",
    "            l=seqlen,\n",
    "        )\n",
    "        if self.in_proj.bias is not None:\n",
    "            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), \"d -> d 1\")\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n",
    "        # In the backward pass we write dx and dz next to each other to avoid torch.cat\n",
    "        if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None:  # Doesn't support outputting the states\n",
    "            out = mamba_inner_fn(\n",
    "                xz,\n",
    "                self.conv1d.weight,\n",
    "                self.conv1d.bias,\n",
    "                self.x_proj.weight,\n",
    "                self.dt_proj.weight,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "                A,\n",
    "                None,  # input-dependent B\n",
    "                None,  # input-dependent C\n",
    "                self.D.float(),\n",
    "                delta_bias=self.dt_proj.bias.float(),\n",
    "                delta_softplus=True,\n",
    "            )\n",
    "        else:\n",
    "            x, z = xz.chunk(2, dim=1)\n",
    "            # Compute short convolution\n",
    "            if conv_state is not None:\n",
    "                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n",
    "                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n",
    "                conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)))  # Update state (B D W)\n",
    "            if causal_conv1d_fn is None:\n",
    "                x = self.act(self.conv1d(x)[..., :seqlen])\n",
    "            else:\n",
    "                assert self.activation in [\"silu\", \"swish\"]\n",
    "                x = causal_conv1d_fn(\n",
    "                    x=x,\n",
    "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
    "                    bias=self.conv1d.bias,\n",
    "                    activation=self.activation,\n",
    "                )\n",
    "\n",
    "            # We're careful here about the layout, to avoid extra transposes.\n",
    "            # We want dt to have d as the slowest moving dimension\n",
    "            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "            x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"))  # (bl d)\n",
    "            dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "            dt = self.dt_proj.weight @ dt.t()\n",
    "            dt = rearrange(dt, \"d (b l) -> b d l\", l=seqlen)\n",
    "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            assert self.activation in [\"silu\", \"swish\"]\n",
    "            y = selective_scan_fn(\n",
    "                x,\n",
    "                dt,\n",
    "                A,\n",
    "                B,\n",
    "                C,\n",
    "                self.D.float(),\n",
    "                z=z,\n",
    "                delta_bias=self.dt_proj.bias.float(),\n",
    "                delta_softplus=True,\n",
    "                return_last_state=ssm_state is not None,\n",
    "            )\n",
    "            if ssm_state is not None:\n",
    "                y, last_state = y\n",
    "                ssm_state.copy_(last_state)\n",
    "            y = rearrange(y, \"b d l -> b l d\")\n",
    "            out = self.out_proj(y)\n",
    "        return out\n",
    "    \n",
    "# Define model parameters\n",
    "input_size = X_train.shape[1]  # Number of input features\n",
    "num_classes = len(nursery.data.targets['class'].unique())  # Number of target classes\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "\n",
    "# Instantiate the Mamba model\n",
    "model = MambaClassifier(input_size=input_size, num_classes=num_classes, d_model=d_model, n_layer=n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MAMBA on Nursery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.3713\n",
      "Epoch [2/10], Loss: 0.7990\n",
      "Epoch [3/10], Loss: 0.5517\n",
      "Epoch [4/10], Loss: 0.4732\n",
      "Epoch [5/10], Loss: 0.4202\n",
      "Epoch [6/10], Loss: 0.3816\n",
      "Epoch [7/10], Loss: 0.3568\n",
      "Epoch [8/10], Loss: 0.3380\n",
      "Epoch [9/10], Loss: 0.3209\n",
      "Epoch [10/10], Loss: 0.3061\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Define model parameters\n",
    "input_size = X_train.shape[1]  # Number of input features\n",
    "num_classes = len(nursery.data.targets['class'].unique())  # Number of target classes\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "\n",
    "# Instantiate the Mamba model\n",
    "model = MambaClassifier(input_size=input_size, num_classes=num_classes, d_model=d_model, n_layer=n_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.8688\n"
     ]
    }
   ],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
