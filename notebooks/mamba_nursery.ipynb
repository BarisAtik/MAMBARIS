{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nursery dataset\n",
    "Was originally created to rank and evaluate nursery school applications. \n",
    "So an application with for example, a family that is financially stable, has good housing, and has no social or health problems would be classified as priority.\n",
    "And applications that may involve severe financial, social, or health issues that make it highly unlikely for the application to be accepted, would be classified as\n",
    "\n",
    "Dit weet ik niet zeker??..\n",
    "\n",
    "So the ranking from best to worst is:\n",
    "\n",
    "Very Recommended > Recommended > Priority > Special Priority > Not Recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 76, 'name': 'Nursery', 'repository_url': 'https://archive.ics.uci.edu/dataset/76/nursery', 'data_url': 'https://archive.ics.uci.edu/static/public/76/data.csv', 'abstract': ' Nursery Database was derived from a hierarchical decision model originally developed to rank applications for nursery schools.', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 12960, 'num_features': 8, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1989, 'last_updated': 'Sun Jan 14 2024', 'dataset_doi': '10.24432/C5P88W', 'creators': ['Vladislav Rajkovic'], 'intro_paper': {'ID': 372, 'type': 'NATIVE', 'title': 'An application for admission in public school systems', 'authors': 'M. Olave, V. Rajkovic, M. Bohanec', 'venue': 'Expert Systems in Public Administration', 'year': 1989, 'journal': None, 'DOI': None, 'URL': 'https://www.academia.edu/16670755/An_application_for_admission_in_public_school_systems', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': \"Nursery Database was derived from a hierarchical decision model originally developed to rank applications for nursery schools. It was used during several years in 1980's when there was excessive enrollment to these schools in Ljubljana, Slovenia, and the rejected applications frequently needed an objective explanation. The final decision depended on three subproblems: occupation of parents and child's nursery, family structure and financial standing, and social and health picture of the family. The model was developed within expert system shell for decision making DEX (M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.).\\r\\n\\r\\nThe hierarchical model ranks nursery-school applications according to the following concept structure:\\r\\n\\r\\n NURSERY            Evaluation of applications for nursery schools\\r\\n . EMPLOY           Employment of parents and child's nursery\\r\\n . . parents        Parents' occupation\\r\\n . . has_nurs       Child's nursery\\r\\n . STRUCT_FINAN     Family structure and financial standings\\r\\n . . STRUCTURE      Family structure\\r\\n . . . form         Form of the family\\r\\n . . . children     Number of children\\r\\n . . housing        Housing conditions\\r\\n . . finance        Financial standing of the family\\r\\n . SOC_HEALTH       Social and health picture of the family\\r\\n . . social         Social conditions\\r\\n . . health         Health conditions\\r\\n\\r\\nInput attributes are printed in lowercase. Besides the target concept (NURSERY) the model includes four intermediate concepts: EMPLOY, STRUCT_FINAN, STRUCTURE, SOC_HEALTH. Every concept is in the original model related to its lower level descendants by a set of examples (for these examples sets see http://www-ai.ijs.si/BlazZupan/nursery.html).\\r\\n\\r\\nThe Nursery Database contains examples with the structural information removed, i.e., directly relates NURSERY to the eight input attributes: parents, has_nurs, form, children, housing, finance, social, health.\\r\\n\\r\\nBecause of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '   parents:        usual, pretentious, great_pret\\r\\n   has_nurs:       proper, less_proper, improper, critical, very_crit\\r\\n   form:           complete, completed, incomplete, foster\\r\\n   children:       1, 2, 3, more\\r\\n   housing:        convenient, less_conv, critical\\r\\n   finance:        convenient, inconv\\r\\n   social:         non-prob, slightly_prob, problematic\\r\\n   health:         recommended, priority, not_recom', 'citation': None}}\n",
      "       name     role         type demographic  \\\n",
      "0   parents  Feature  Categorical        None   \n",
      "1  has_nurs  Feature  Categorical        None   \n",
      "2      form  Feature  Categorical        None   \n",
      "3  children  Feature  Categorical        None   \n",
      "4   housing  Feature  Categorical        None   \n",
      "5   finance  Feature  Categorical        None   \n",
      "6    social  Feature  Categorical        None   \n",
      "7    health  Feature  Categorical        None   \n",
      "8     class   Target  Categorical        None   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0                     usual, pretentious, great_pret  None             no  \n",
      "1  proper, less_proper, improper, critical, very_...  None             no  \n",
      "2            complete, completed, incomplete, foster  None             no  \n",
      "3                                      1, 2, 3, more  None             no  \n",
      "4                    convenient, less_conv, critical  None             no  \n",
      "5                                 convenient, inconv  None             no  \n",
      "6               non-prob, slightly_prob, problematic  None             no  \n",
      "7                   recommended, priority, not_recom  None             no  \n",
      "8                   recommended, priority, not_recom  None             no  \n",
      "\n",
      "The first 5 rows of the dataset:\n",
      "  parents has_nurs      form children     housing     finance         social  \\\n",
      "0   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "1   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "2   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "3   usual   proper  complete        1  convenient  convenient  slightly_prob   \n",
      "4   usual   proper  complete        1  convenient  convenient  slightly_prob   \n",
      "\n",
      "        health  \n",
      "0  recommended  \n",
      "1     priority  \n",
      "2    not_recom  \n",
      "3  recommended  \n",
      "4     priority  \n",
      "\n",
      "Possible target classes:\n",
      "['recommend' 'priority' 'not_recom' 'very_recom' 'spec_prior']\n"
     ]
    }
   ],
   "source": [
    "# ucimlrepo is a tool that provides easy access to datasets hosted on the UCI Machine Learning Repository\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset from UCI repository, which has ud 76\n",
    "nursery = fetch_ucirepo(id=76)\n",
    "\n",
    "# Display metadata and variable information\n",
    "print(nursery.metadata) # metadata \n",
    "print(nursery.variables) # variable information \n",
    "print(\"\\n\"+ \"The first 5 rows of the dataset:\")\n",
    "print(nursery.data.features.head())  # Display first 5 rows of features\n",
    "# Show the target variable, possibilities\n",
    "\n",
    "# Show the target variable and its unique possibilities\n",
    "unique_targets = nursery.data.targets['class'].unique()  # Access the 'class' column and get unique values\n",
    "print(\"\\nPossible target classes:\")\n",
    "print(unique_targets)  # Display unique target classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from model import Mamba, ModelArgs  # Import your custom Mamba implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The datta will be preprocessed, and converted into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original  values (before encoding):\n",
      "  parents has_nurs      form children     housing     finance         social  \\\n",
      "1   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "2   usual   proper  complete        1  convenient  convenient        nonprob   \n",
      "3   usual   proper  complete        1  convenient  convenient  slightly_prob   \n",
      "\n",
      "        health  \n",
      "1     priority  \n",
      "2    not_recom  \n",
      "3  recommended  \n",
      "       class\n",
      "1   priority\n",
      "2  not_recom\n",
      "3  recommend\n",
      "\n",
      "Encoded target values (after encoding):\n",
      "   parents  has_nurs  form  children  housing  finance  social  health\n",
      "1        2         3     0         0        0        0       0       1\n",
      "2        2         3     0         0        0        0       0       0\n",
      "3        2         3     0         0        0        0       2       2\n",
      "[1 0 2]\n",
      "\n",
      "Sample of training data tensor:\n",
      "tensor([[0, 2, 0, 3, 2, 0, 2, 1],\n",
      "        [2, 0, 3, 2, 2, 1, 1, 1]])\n",
      "\n",
      "Sample of training target tensor:\n",
      "tensor([3, 3])\n",
      "\n",
      "Sample of testing data tensor:\n",
      "tensor([[1, 1, 1, 2, 2, 1, 1, 0],\n",
      "        [1, 1, 1, 0, 1, 0, 0, 1]])\n",
      "\n",
      "Sample of testing target tensor:\n",
      "tensor([0, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagriatik/MAMBARIS/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X = nursery.data.features  # These are all the feature columns in the dataset\n",
    "Y = nursery.data.targets  # This is the target column in the dataset\n",
    "print(\"\\nOriginal  values (before encoding):\")\n",
    "print(X[1:4])  # Display a sample of the feature values\n",
    "print(Y[1:4])  # Display a sample of the target values\n",
    "\n",
    "# In case of future errors: Y = Y.values.ravel()  # Flatten Y to make it a 1D array if needed\n",
    "label_encoder = LabelEncoder()  # Used to encode the categorical target variables into numerical values\n",
    "X = X.apply(label_encoder.fit_transform)  # Encode the feature variables (X)\n",
    "Y = label_encoder.fit_transform(Y)  # Encode the target variable (Y)\n",
    "\n",
    "print(\"\\nEncoded target values (after encoding):\")\n",
    "print(X[1:4])  # Display the encoded feature values\n",
    "print(Y[1:4])  # Display the encoded target values\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.21, random_state=42)\n",
    "\n",
    "# Convert the train/test data into PyTorch tensors\n",
    "# We must do this because PyTorch models only accept tensors as input\n",
    "# Both the MambaClassifier and Mamba classes inherit from torch.nn.Module\n",
    "# which is the base class for all neural network modules in PyTorch.\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.long)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)\n",
    "\n",
    "# Lets see how these tensors look like\n",
    "print(\"\\nSample of training data tensor:\")\n",
    "print(X_train_tensor[0:2])  # Display a sample of the training data tensor\n",
    "print(\"\\nSample of training target tensor:\")\n",
    "print(Y_train_tensor[0:2])  # Display a sample of the training target tensor\n",
    "print(\"\\nSample of testing data tensor:\")\n",
    "print(X_test_tensor[0:2])  # Display a sample of the testing data tensor\n",
    "print(\"\\nSample of testing target tensor:\")\n",
    "print(Y_test_tensor[0:2])  # Display a sample of the testing target tensor\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "# DataLoader to help in batch processing during model training/testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Mamba, ModelArgs\n",
    "# I also want another mamba model which was not pretrained\n",
    "d_model = 64\n",
    "n_layer = 4\n",
    "vocabsize = len(X.nunique())\n",
    "# or might it be vocabsize = X.apply(lambda col: col.nunique()).max()\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=vocabsize)\n",
    "num_classes = len(nursery.data.targets['class'].unique())\n",
    "model = Mamba(model_args, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MAMBA on Nursery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8944\n",
      "Epoch [2/10], Loss: 0.4998\n",
      "Epoch [3/10], Loss: 0.3088\n",
      "Epoch [4/10], Loss: 0.1135\n",
      "Epoch [5/10], Loss: 0.0523\n",
      "Epoch [6/10], Loss: 0.0291\n",
      "Epoch [7/10], Loss: 0.0182\n",
      "Epoch [8/10], Loss: 0.0124\n",
      "Epoch [9/10], Loss: 0.0085\n",
      "Epoch [10/10], Loss: 0.0066\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size * seq_length]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model + testing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.9993\n",
      "First few predicted class names and their probabilities (in percentages):\n",
      "Instance 1:\n",
      "  Correct Class: recommend\n",
      "  Class: recommend, Probability: 99.58%\n",
      "  Class: priority, Probability: 0.08%\n",
      "  Class: not_recom, Probability: 0.12%\n",
      "  Class: very_recom, Probability: 0.08%\n",
      "  Class: spec_prior, Probability: 0.15%\n",
      "Instance 2:\n",
      "  Correct Class: very_recom\n",
      "  Class: recommend, Probability: 0.10%\n",
      "  Class: priority, Probability: 0.13%\n",
      "  Class: not_recom, Probability: 0.11%\n",
      "  Class: very_recom, Probability: 99.57%\n",
      "  Class: spec_prior, Probability: 0.10%\n",
      "Instance 3:\n",
      "  Correct Class: priority\n",
      "  Class: recommend, Probability: 0.07%\n",
      "  Class: priority, Probability: 99.42%\n",
      "  Class: not_recom, Probability: 0.19%\n",
      "  Class: very_recom, Probability: 0.03%\n",
      "  Class: spec_prior, Probability: 0.28%\n",
      "Instance 4:\n",
      "  Correct Class: very_recom\n",
      "  Class: recommend, Probability: 0.10%\n",
      "  Class: priority, Probability: 0.03%\n",
      "  Class: not_recom, Probability: 0.20%\n",
      "  Class: very_recom, Probability: 99.51%\n",
      "  Class: spec_prior, Probability: 0.15%\n",
      "Instance 5:\n",
      "  Correct Class: recommend\n",
      "  Class: recommend, Probability: 99.55%\n",
      "  Class: priority, Probability: 0.10%\n",
      "  Class: not_recom, Probability: 0.15%\n",
      "  Class: very_recom, Probability: 0.07%\n",
      "  Class: spec_prior, Probability: 0.13%\n",
      "Accuracy on the training set: 1.0000\n",
      "First few predicted class names and their probabilities (in percentages) for training data:\n",
      "Instance 1:\n",
      "  Correct Class: recommend\n",
      "  Class: recommend, Probability: 99.53%\n",
      "  Class: priority, Probability: 0.09%\n",
      "  Class: not_recom, Probability: 0.13%\n",
      "  Class: very_recom, Probability: 0.10%\n",
      "  Class: spec_prior, Probability: 0.15%\n",
      "Instance 2:\n",
      "  Correct Class: priority\n",
      "  Class: recommend, Probability: 0.18%\n",
      "  Class: priority, Probability: 99.54%\n",
      "  Class: not_recom, Probability: 0.16%\n",
      "  Class: very_recom, Probability: 0.03%\n",
      "  Class: spec_prior, Probability: 0.08%\n",
      "Instance 3:\n",
      "  Correct Class: very_recom\n",
      "  Class: recommend, Probability: 0.15%\n",
      "  Class: priority, Probability: 0.03%\n",
      "  Class: not_recom, Probability: 0.16%\n",
      "  Class: very_recom, Probability: 99.54%\n",
      "  Class: spec_prior, Probability: 0.11%\n",
      "Instance 4:\n",
      "  Correct Class: recommend\n",
      "  Class: recommend, Probability: 99.56%\n",
      "  Class: priority, Probability: 0.05%\n",
      "  Class: not_recom, Probability: 0.12%\n",
      "  Class: very_recom, Probability: 0.12%\n",
      "  Class: spec_prior, Probability: 0.15%\n",
      "Instance 5:\n",
      "  Correct Class: spec_prior\n",
      "  Class: recommend, Probability: 0.28%\n",
      "  Class: priority, Probability: 0.56%\n",
      "  Class: not_recom, Probability: 0.45%\n",
      "  Class: very_recom, Probability: 0.12%\n",
      "  Class: spec_prior, Probability: 98.60%\n"
     ]
    }
   ],
   "source": [
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = [prob * 100 for prob in y_prob]\n",
    "\n",
    "# Map the predicted class indices to their corresponding class names\n",
    "class_names = unique_targets  # Use the unique target classes from the dataset\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = [prob * 100 for prob in y_prob_train]\n",
    "\n",
    "# Map the predicted class indices to their corresponding class names\n",
    "class_names = unique_targets  # Use the unique target classes from the dataset\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability for the correct class on the test data: 0.9927\n",
      "Average probability for the correct class on the training data: 0.9942\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 2, simpler model\n",
    "As we can see there is not much difference between the probabilities of true class predictions on the traindataset instances, and the testdataset instances.\n",
    "\n",
    "This might indicate that the dataset was too easy for this model. Below i will try the same on a simpler version of the same mamba model e.g. less layers and lower dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also want another mamba model which was not pretrained\n",
    "d_model = 16\n",
    "n_layer = 3\n",
    "vocabsize = len(X.nunique())\n",
    "# or might it be vocabsize = X.apply(lambda col: col.nunique()).max()\n",
    "model_args = ModelArgs(d_model=d_model, n_layer=n_layer, vocab_size=vocabsize)\n",
    "num_classes = len(nursery.data.targets['class'].unique())\n",
    "smaller_model = Mamba(model_args, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4998\n",
      "Epoch [2/10], Loss: 1.0255\n",
      "Epoch [3/10], Loss: 0.6812\n",
      "Epoch [4/10], Loss: 0.6144\n",
      "Epoch [5/10], Loss: 0.5780\n",
      "Epoch [6/10], Loss: 0.5498\n",
      "Epoch [7/10], Loss: 0.5099\n",
      "Epoch [8/10], Loss: 0.4239\n",
      "Epoch [9/10], Loss: 0.3326\n",
      "Epoch [10/10], Loss: 0.2680\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(smaller_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "smaller_model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    smaller_model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probabilities = smaller_model(inputs)  # Unpack the logits and probabilities\n",
    "\n",
    "        # Flatten labels if they are not already\n",
    "        labels = labels.view(-1)  # Flatten the labels to [batch_size * seq_length]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)  # Use logits for loss computation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.9269\n",
      "First few predicted class names and their probabilities (in percentages):\n",
      "Instance 1:\n",
      "  Correct Class: recommend\n",
      "  Class: recommend, Probability: 95.08%\n",
      "  Class: priority, Probability: 1.15%\n",
      "  Class: not_recom, Probability: 1.18%\n",
      "  Class: very_recom, Probability: 0.89%\n",
      "  Class: spec_prior, Probability: 1.70%\n",
      "Instance 2:\n",
      "  Correct Class: very_recom\n",
      "  Class: recommend, Probability: 0.97%\n",
      "  Class: priority, Probability: 39.42%\n",
      "  Class: not_recom, Probability: 1.39%\n",
      "  Class: very_recom, Probability: 56.56%\n",
      "  Class: spec_prior, Probability: 1.66%\n",
      "Instance 3:\n",
      "  Correct Class: priority\n",
      "  Class: recommend, Probability: 5.82%\n",
      "  Class: priority, Probability: 84.26%\n",
      "  Class: not_recom, Probability: 3.90%\n",
      "  Class: very_recom, Probability: 1.97%\n",
      "  Class: spec_prior, Probability: 4.06%\n",
      "Instance 4:\n",
      "  Correct Class: very_recom\n",
      "  Class: recommend, Probability: 1.76%\n",
      "  Class: priority, Probability: 2.19%\n",
      "  Class: not_recom, Probability: 1.68%\n",
      "  Class: very_recom, Probability: 93.26%\n",
      "  Class: spec_prior, Probability: 1.12%\n",
      "Instance 5:\n",
      "  Correct Class: recommend\n",
      "  Class: recommend, Probability: 95.21%\n",
      "  Class: priority, Probability: 1.69%\n",
      "  Class: not_recom, Probability: 1.06%\n",
      "  Class: very_recom, Probability: 0.85%\n",
      "  Class: spec_prior, Probability: 1.19%\n",
      "Accuracy on the training set: 0.9315\n",
      "First few predicted class names and their probabilities (in percentages) for training data:\n",
      "Instance 1:\n",
      "  Correct Class: priority\n",
      "  Class: recommend, Probability: 1.45%\n",
      "  Class: priority, Probability: 87.91%\n",
      "  Class: not_recom, Probability: 1.82%\n",
      "  Class: very_recom, Probability: 6.01%\n",
      "  Class: spec_prior, Probability: 2.81%\n",
      "Instance 2:\n",
      "  Correct Class: very_recom\n",
      "  Class: recommend, Probability: 1.46%\n",
      "  Class: priority, Probability: 19.54%\n",
      "  Class: not_recom, Probability: 1.55%\n",
      "  Class: very_recom, Probability: 76.75%\n",
      "  Class: spec_prior, Probability: 0.70%\n",
      "Instance 3:\n",
      "  Correct Class: recommend\n",
      "  Class: recommend, Probability: 95.34%\n",
      "  Class: priority, Probability: 1.34%\n",
      "  Class: not_recom, Probability: 1.06%\n",
      "  Class: very_recom, Probability: 0.94%\n",
      "  Class: spec_prior, Probability: 1.32%\n",
      "Instance 4:\n",
      "  Correct Class: priority\n",
      "  Class: recommend, Probability: 1.20%\n",
      "  Class: priority, Probability: 81.00%\n",
      "  Class: not_recom, Probability: 1.35%\n",
      "  Class: very_recom, Probability: 13.77%\n",
      "  Class: spec_prior, Probability: 2.69%\n",
      "Instance 5:\n",
      "  Correct Class: priority\n",
      "  Class: recommend, Probability: 1.29%\n",
      "  Class: priority, Probability: 9.10%\n",
      "  Class: not_recom, Probability: 1.16%\n",
      "  Class: very_recom, Probability: 86.92%\n",
      "  Class: spec_prior, Probability: 1.52%\n"
     ]
    }
   ],
   "source": [
    "# Switch to evaluation mode\n",
    "smaller_model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_prob = []  # List to store probabilities\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = smaller_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_percentages = [prob * 100 for prob in y_prob]\n",
    "\n",
    "# Map the predicted class indices to their corresponding class names\n",
    "class_names = unique_targets  # Use the unique target classes from the dataset\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages):\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_percentages[i][class_index]:.2f}%\")\n",
    "\n",
    "# Switch to evaluation mode\n",
    "smaller_model.eval()\n",
    "y_pred_train = []\n",
    "y_true_train = []\n",
    "y_prob_train = []  # List to store probabilities\n",
    "\n",
    "# Test the model on training data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logits, probabilities = smaller_model(inputs)  # Now get both logits and probabilities\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        y_pred_train.extend(predicted.cpu().numpy())\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_prob_train.extend(probabilities.cpu().numpy())  # Store probabilities\n",
    "\n",
    "# Calculate accuracy on training data\n",
    "accuracy_train = accuracy_score(y_true_train, y_pred_train)\n",
    "print(f'Accuracy on the training set: {accuracy_train:.4f}')\n",
    "\n",
    "# Convert probabilities to percentages\n",
    "y_prob_train_percentages = [prob * 100 for prob in y_prob_train]\n",
    "\n",
    "# Map the predicted class indices to their corresponding class names\n",
    "class_names = unique_targets  # Use the unique target classes from the dataset\n",
    "\n",
    "# Display the first few predicted class names along with their probabilities and the correct class\n",
    "print(\"First few predicted class names and their probabilities (in percentages) for training data:\")\n",
    "for i in range(5):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Correct Class: {class_names[y_true_train[i]]}\")\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        print(f\"  Class: {class_name}, Probability: {y_prob_train_percentages[i][class_index]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability for the correct class on the test data: 0.8227\n",
      "Average probability for the correct class on the training data: 0.8257\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract probabilities for the correct class on the test data\n",
    "correct_class_probs_test = [y_prob[i][y_true[i]] for i in range(len(y_true))]\n",
    "average_prob_correct_class_test = np.mean(correct_class_probs_test)\n",
    "print(f'Average probability for the correct class on the test data: {average_prob_correct_class_test:.4f}')\n",
    "\n",
    "# Extract probabilities for the correct class on the training data\n",
    "correct_class_probs_train = [y_prob_train[i][y_true_train[i]] for i in range(len(y_true_train))]\n",
    "average_prob_correct_class_train = np.mean(correct_class_probs_train)\n",
    "print(f'Average probability for the correct class on the training data: {average_prob_correct_class_train:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see even on a smaller model there is no siginificant difference in the probability the model outputs for a correct class.\n",
    "Therefore I will move on to different datasets / problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
