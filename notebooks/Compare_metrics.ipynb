{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_metrics(mamba_dir='mamba_checkpoints', cnn_dir='cnn_checkpoints'):\n",
    "    \"\"\"Load metrics from both model JSON files using correct paths.\"\"\"\n",
    "    try:\n",
    "        with open(f'{mamba_dir}/training_metrics.json', 'r') as f:\n",
    "            mamba_metrics = json.load(f)\n",
    "        with open(f'{cnn_dir}/training_metrics.json', 'r') as f:\n",
    "            cnn_metrics = json.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading metrics: {e}\")\n",
    "        print(\"Please ensure the metrics files exist in the correct directories:\")\n",
    "        print(f\"- {mamba_dir}/training_metrics.json\")\n",
    "        print(f\"- {cnn_dir}/training_metrics.json\")\n",
    "        raise\n",
    "        \n",
    "    return mamba_metrics, cnn_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the models\n",
    "Comparing the metrics that were tracked during training in the traininc_metrics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(mamba_dir='mamba_checkpoints', \n",
    "                          cnn_dir='cnn_checkpoints',\n",
    "                          save_path=None):\n",
    "    \"\"\"Plot comparison of model metrics from training.\"\"\"\n",
    "    \n",
    "    # Load metrics\n",
    "    mamba_metrics, cnn_metrics = load_model_metrics(mamba_dir, cnn_dir)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Calculate epochs - each checkpoint is taken every 100 epochs\n",
    "    checkpoint_freq = 100\n",
    "    mamba_epochs = list(range(checkpoint_freq, len(mamba_metrics['train_accuracies']) * checkpoint_freq + 1, checkpoint_freq))\n",
    "    cnn_epochs = list(range(checkpoint_freq, len(cnn_metrics['train_accuracies']) * checkpoint_freq + 1, checkpoint_freq))\n",
    "    \n",
    "    final_mamba_epoch = mamba_epochs[-1] if mamba_epochs else 0\n",
    "    final_cnn_epoch = cnn_epochs[-1] if cnn_epochs else 0\n",
    "    \n",
    "    print(f\"Plotting metrics - MAMBA up to epoch {final_mamba_epoch}, CNN up to epoch {final_cnn_epoch}\")\n",
    "    \n",
    "    # Adjust fontsize for better readability\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    # 1. Accuracy Comparison\n",
    "    ax1.plot(mamba_epochs, mamba_metrics['train_accuracies'], 'b-', label='MAMBA Train')\n",
    "    ax1.plot(mamba_epochs, mamba_metrics['test_accuracies'], 'b--', label='MAMBA Test')\n",
    "    ax1.plot(cnn_epochs, cnn_metrics['train_accuracies'], 'r-', label='CNN Train')\n",
    "    ax1.plot(cnn_epochs, cnn_metrics['test_accuracies'], 'r--', label='CNN Test')\n",
    "    ax1.set_title('Accuracy Comparison')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    # Format x-axis to show actual epoch numbers\n",
    "    ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x)}'))\n",
    "    \n",
    "    # 2. Loss Comparison\n",
    "    ax2.plot(mamba_epochs, mamba_metrics['train_losses'], 'b-', label='MAMBA Train')\n",
    "    ax2.plot(mamba_epochs, mamba_metrics['test_losses'], 'b--', label='MAMBA Test')\n",
    "    ax2.plot(cnn_epochs, cnn_metrics['train_losses'], 'r-', label='CNN Train')\n",
    "    ax2.plot(cnn_epochs, cnn_metrics['test_losses'], 'r--', label='CNN Test')\n",
    "    ax2.set_title('Loss Comparison')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x)}'))\n",
    "    \n",
    "    # 3. Average Confidence\n",
    "    ax3.plot(mamba_epochs, mamba_metrics['train_confidences'], 'b-', label='MAMBA Train')\n",
    "    ax3.plot(mamba_epochs, mamba_metrics['test_confidences'], 'b--', label='MAMBA Test')\n",
    "    ax3.plot(cnn_epochs, cnn_metrics['train_confidences'], 'r-', label='CNN Train')\n",
    "    ax3.plot(cnn_epochs, cnn_metrics['test_confidences'], 'r--', label='CNN Test')\n",
    "    ax3.set_title('Average Confidence')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Confidence')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    ax3.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x)}'))\n",
    "    \n",
    "    # 4. Overfitting Analysis (Train-Test Accuracy Gap)\n",
    "    mamba_gap = [train - test for train, test in \n",
    "                 zip(mamba_metrics['train_accuracies'], mamba_metrics['test_accuracies'])]\n",
    "    cnn_gap = [train - test for train, test in \n",
    "               zip(cnn_metrics['train_accuracies'], cnn_metrics['test_accuracies'])]\n",
    "    \n",
    "    ax4.plot(mamba_epochs, mamba_gap, 'b-', label='MAMBA')\n",
    "    ax4.plot(cnn_epochs, cnn_gap, 'r-', label='CNN')\n",
    "    ax4.set_title('Overfitting Analysis (Train-Test Accuracy Gap)')\n",
    "    ax4.set_xlabel('Epochs')\n",
    "    ax4.set_ylabel('Accuracy Gap (%)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "    ax4.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x)}'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    # Print summary statistics for final checkpoints\n",
    "    print(\"\\nFinal Checkpoint Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if len(mamba_metrics['train_accuracies']) > 0:\n",
    "        print(f\"\\nMAMBA Final Checkpoint (Epoch {final_mamba_epoch}):\")\n",
    "        print(f\"Accuracies - Train: {mamba_metrics['train_accuracies'][-1]:.2f}%, Test: {mamba_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "        print(f\"Overfitting Gap: {mamba_gap[-1]:.2f}%\")\n",
    "        print(f\"Confidence - Train: {mamba_metrics['train_confidences'][-1]:.4f}, Test: {mamba_metrics['test_confidences'][-1]:.4f}\")\n",
    "    \n",
    "    if len(cnn_metrics['train_accuracies']) > 0:\n",
    "        print(f\"\\nCNN Final Checkpoint (Epoch {final_cnn_epoch}):\")\n",
    "        print(f\"Accuracies - Train: {cnn_metrics['train_accuracies'][-1]:.2f}%, Test: {cnn_metrics['test_accuracies'][-1]:.2f}%\")\n",
    "        print(f\"Overfitting Gap: {cnn_gap[-1]:.2f}%\")\n",
    "        print(f\"Confidence - Train: {cnn_metrics['train_confidences'][-1]:.4f}, Test: {cnn_metrics['test_confidences'][-1]:.4f}\")\n",
    "    \n",
    "    return fig, (mamba_metrics, cnn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (mamba_metrics, cnn_metrics) = plot_metrics_comparison(\n",
    "    mamba_dir='mamba_checkpoints',\n",
    "    cnn_dir='cnn_checkpoints',\n",
    "    save_path='model_comparison.png'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
